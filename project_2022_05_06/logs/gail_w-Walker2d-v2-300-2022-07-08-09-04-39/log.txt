2022-07-08 09:04:39.357454 - utils/flags.py:257 - log_dir = logs/gail_w-Walker2d-v2-300-2022-07-08-09-04-39
2022-07-08 09:04:56.283948 - gail/utils/replay_buffer.py:86 - Load dataset from /content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/Walker2d-v2
2022-07-08 09:05:15.893072 - gail/main.py:80 - Expert Reward 5150.674112
2022-07-08 09:05:16.793233 - gail/main.py:84 - Original dataset size 3000
2022-07-08 09:05:16.884054 - gail/main.py:86 - Subsampled dataset size 3000
2022-07-08 09:05:16.890327 - gail/main.py:87 - np random: 864 random : 786
2022-07-08 09:05:16.920975 - gail/main.py:91 - Sampled obs: 0.0531, acs: 0.2269
2022-07-08 09:05:19.488087 - lunzi/nn/flat_param.py:17 - Enabling flattening... ['GaussianMLPPolicy_1/log_std:0', 'GaussianMLPPolicy_1/Variable:0', 'GaussianMLPPolicy_1/Variable_1:0', 'GaussianMLPPolicy_1/Variable_2:0', 'GaussianMLPPolicy_1/Variable_3:0', 'GaussianMLPPolicy_1/Variable_4:0', 'GaussianMLPPolicy_1/Variable_5:0']
2022-07-08 09:05:43.788636 - gail/discriminator/discriminator.py:30 - Use predefined normalization.
2022-07-08 09:05:43.811307 - gail/discriminator/discriminator.py:33 - Normalizer loc:[[ 1.2194959e+00  2.4445076e-01 -7.8132987e-02 -2.6673764e-01
   1.8222688e-01 -9.5077172e-02 -3.3649772e-01  5.3370733e-02
   4.1614923e+00  4.1431887e-03  3.8142569e-02 -2.6013174e-03
  -1.0202496e-02  5.6982285e-01  2.9836079e-02 -1.5763690e-01
   1.7689442e-02]] 
 scale:[[0.06687175 0.23681822 0.23042987 0.33821535 0.664349   0.20301929
  0.42807332 0.7138035  0.986894   0.65049744 2.0363257  2.3816926
  3.7250905  6.026913   2.0511289  4.406521   6.1475325 ]]
2022-07-08 09:05:54.617531 - gail/discriminator/discriminator.py:67 - Discriminator uses Wasserstein distance.
2022-07-08 09:05:54.624253 - gail/discriminator/discriminator.py:68 - [<tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable:0' shape=(23, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_1:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_2:0' shape=(100, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_3:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_4:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_5:0' shape=(1,) dtype=float32_ref>]
2022-07-08 09:05:54.627855 - gail/discriminator/discriminator.py:69 - Use gradient penalty regularization (coef = 10.000000)
2022-07-08 09:05:57.007221 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions_mean]
2022-07-08 09:06:17.511036 - gail/main.py:142 - [Evaluate] iter = 0 episode={ returns = 70.4326 lengths = 87 } discounted_episode={ returns = 68.4323 lengths = 86 } 
2022-07-08 09:06:17.516316 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions]
2022-07-08 09:06:41.216856 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [fake_states fake_actions] => [scaled_neural_reward]
2022-07-08 09:06:42.010200 - lunzi/nn/module.py:71 - [MLPVFunction] is making TensorFlow callables, key = [states] => [values]
2022-07-08 09:06:43.398107 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [] => [sync_old]
2022-07-08 09:06:44.033047 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss flat_grad dist_std mean_kl dist_mean]
2022-07-08 09:06:48.027482 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states tangents actions] => [hessian_vec_prod]
2022-07-08 09:06:54.775392 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [] => [get_flat]
2022-07-08 09:06:55.412949 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [feed_flat] => [set_flat]
2022-07-08 09:06:56.095973 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss mean_kl]
2022-07-08 09:06:57.348073 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [train_vf vf_loss]
2022-07-08 09:06:59.227864 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [vf_loss]
2022-07-08 09:06:59.885217 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states actions_] => [mse_loss]
2022-07-08 09:07:00.511055 - gail/main.py:174 - [TRPO] iter = 1000 dist_mean = 0.0001 dist_std = 1.0000 vf_loss = 0.2010 grad_norm = 0.3681 nat_grad_norm = 0.5166 cg_residual = 0.0000 step_size = 0.3969 reward = -0.0000 fps = 15 mse_loss = 0.4353 
2022-07-08 09:07:17.436549 - gail/main.py:174 - [TRPO] iter = 2000 dist_mean = -0.0136 dist_std = 1.0050 vf_loss = 0.2367 grad_norm = 0.4137 nat_grad_norm = 0.5294 cg_residual = 0.0000 step_size = 0.3510 reward = 0.0000 fps = 12 mse_loss = 0.4500 
2022-07-08 09:07:34.820114 - gail/main.py:174 - [TRPO] iter = 3000 dist_mean = -0.0347 dist_std = 1.0003 vf_loss = 0.2748 grad_norm = 0.4021 nat_grad_norm = 0.5523 cg_residual = 0.0000 step_size = 0.3446 reward = 0.0000 fps = 10 mse_loss = 0.4777 
2022-07-08 09:07:52.862106 - gail/main.py:174 - [TRPO] iter = 4000 dist_mean = -0.0571 dist_std = 0.9992 vf_loss = 0.3172 grad_norm = 0.4847 nat_grad_norm = 0.5268 cg_residual = 0.0000 step_size = 0.3402 reward = -0.0000 fps = 8 mse_loss = 0.5019 
2022-07-08 09:08:11.055855 - gail/main.py:174 - [TRPO] iter = 5000 dist_mean = -0.0731 dist_std = 0.9978 vf_loss = 0.2435 grad_norm = 0.4364 nat_grad_norm = 0.4939 cg_residual = 0.0001 step_size = 0.3658 reward = 0.0000 fps = 7 mse_loss = 0.5351 
2022-07-08 09:08:11.065866 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [true_states true_actions fake_states fake_actions true_masks] => [train loss true_logits fake_logits true_prob fake_prob grad_norm grad_penalty regularization]
2022-07-08 09:08:17.123326 - gail/main.py:201 - [Discriminator] iter = 5000 loss = 1.0945 grad_norm = 12.5178 grad_penalty = 1.4682 regularization = 0.0000 true_logits = 0.0199 fake_logits = -0.3538 true_prob = 0.5051 fake_prob = 0.4135 
2022-07-08 09:08:21.079028 - gail/main.py:142 - [Evaluate] iter = 5000 episode={ returns = -0.5671 lengths = 14 } discounted_episode={ returns = -0.5516 lengths = 14 } 
2022-07-08 09:08:40.217817 - gail/main.py:174 - [TRPO] iter = 6000 dist_mean = -0.0906 dist_std = 0.9921 vf_loss = 0.2433 grad_norm = 0.4037 nat_grad_norm = 0.5582 cg_residual = 0.0001 step_size = 0.3564 reward = -0.0000 fps = 43 mse_loss = 0.5707 
2022-07-08 09:08:58.584952 - gail/main.py:174 - [TRPO] iter = 7000 dist_mean = -0.0971 dist_std = 0.9881 vf_loss = 0.2158 grad_norm = 0.5261 nat_grad_norm = 0.5300 cg_residual = 0.0001 step_size = 0.3186 reward = -0.0000 fps = 24 mse_loss = 0.6026 
2022-07-08 09:09:23.561084 - gail/main.py:174 - [TRPO] iter = 8000 dist_mean = -0.0925 dist_std = 0.9873 vf_loss = 0.1910 grad_norm = 0.5658 nat_grad_norm = 0.4657 cg_residual = 0.0001 step_size = 0.3512 reward = 0.0000 fps = 15 mse_loss = 0.5898 
2022-07-08 09:09:47.893840 - gail/main.py:174 - [TRPO] iter = 9000 dist_mean = -0.0862 dist_std = 0.9851 vf_loss = 0.1513 grad_norm = 0.5103 nat_grad_norm = 0.5467 cg_residual = 0.0002 step_size = 0.3423 reward = -0.0000 fps = 11 mse_loss = 0.5874 
2022-07-08 09:10:11.223674 - gail/main.py:174 - [TRPO] iter = 10000 dist_mean = -0.0759 dist_std = 0.9834 vf_loss = 0.1626 grad_norm = 0.4182 nat_grad_norm = 0.4988 cg_residual = 0.0002 step_size = 0.3925 reward = 0.0000 fps = 8 mse_loss = 0.5784 
2022-07-08 09:10:12.239294 - gail/main.py:201 - [Discriminator] iter = 10000 loss = -0.0570 grad_norm = 10.2298 grad_penalty = 0.7629 regularization = 0.0000 true_logits = 0.0693 fake_logits = -0.7506 true_prob = 0.5175 fake_prob = 0.3231 
2022-07-08 09:10:20.202388 - gail/main.py:142 - [Evaluate] iter = 10000 episode={ returns = -6.1994 lengths = 19 } discounted_episode={ returns = -5.9969 lengths = 19 } 
2022-07-08 09:10:49.166454 - gail/main.py:174 - [TRPO] iter = 11000 dist_mean = -0.0770 dist_std = 0.9791 vf_loss = 0.1842 grad_norm = 0.5616 nat_grad_norm = 0.4952 cg_residual = 0.0003 step_size = 0.3470 reward = 0.0000 fps = 27 mse_loss = 0.5736 
2022-07-08 09:11:24.658223 - gail/main.py:174 - [TRPO] iter = 12000 dist_mean = -0.0512 dist_std = 0.9734 vf_loss = 0.1647 grad_norm = 0.6783 nat_grad_norm = 0.5140 cg_residual = 0.0003 step_size = 0.3139 reward = -0.0000 fps = 13 mse_loss = 0.5450 
2022-07-08 09:11:52.921607 - gail/main.py:174 - [TRPO] iter = 13000 dist_mean = -0.0205 dist_std = 0.9743 vf_loss = 0.2575 grad_norm = 0.5948 nat_grad_norm = 0.4629 cg_residual = 0.0004 step_size = 0.3750 reward = 0.0000 fps = 9 mse_loss = 0.5528 
2022-07-08 09:12:22.166549 - gail/main.py:174 - [TRPO] iter = 14000 dist_mean = 0.0182 dist_std = 0.9704 vf_loss = 0.2109 grad_norm = 0.6776 nat_grad_norm = 0.4841 cg_residual = 0.0005 step_size = 0.3499 reward = -0.0000 fps = 7 mse_loss = 0.5004 
2022-07-08 09:12:54.332857 - gail/main.py:174 - [TRPO] iter = 15000 dist_mean = 0.0482 dist_std = 0.9685 vf_loss = 0.1603 grad_norm = 0.4470 nat_grad_norm = 0.5387 cg_residual = 0.0008 step_size = 0.4014 reward = 0.0000 fps = 6 mse_loss = 0.5178 
2022-07-08 09:12:55.242774 - gail/main.py:201 - [Discriminator] iter = 15000 loss = -0.7841 grad_norm = 6.8347 grad_penalty = 0.4714 regularization = 0.0000 true_logits = 0.1413 fake_logits = -1.1141 true_prob = 0.5358 fake_prob = 0.2544 
2022-07-08 09:13:25.205175 - gail/main.py:142 - [Evaluate] iter = 15000 episode={ returns = -19.3789 lengths = 72 } discounted_episode={ returns = -18.2623 lengths = 72 } 
2022-07-08 09:13:55.824772 - gail/main.py:174 - [TRPO] iter = 16000 dist_mean = 0.0549 dist_std = 0.9703 vf_loss = 0.1552 grad_norm = 0.5514 nat_grad_norm = 0.5207 cg_residual = 0.0004 step_size = 0.3732 reward = -0.0000 fps = 16 mse_loss = 0.5612 
2022-07-08 09:14:25.398708 - gail/main.py:174 - [TRPO] iter = 17000 dist_mean = 0.0922 dist_std = 0.9710 vf_loss = 0.1164 grad_norm = 0.4994 nat_grad_norm = 0.4943 cg_residual = 0.0006 step_size = 0.4093 reward = 0.0000 fps = 11 mse_loss = 0.5737 
2022-07-08 09:14:57.087557 - gail/main.py:174 - [TRPO] iter = 18000 dist_mean = 0.1107 dist_std = 0.9665 vf_loss = 0.1664 grad_norm = 0.4362 nat_grad_norm = 0.4632 cg_residual = 0.0004 step_size = 0.4384 reward = -0.0000 fps = 8 mse_loss = 0.5612 
2022-07-08 09:15:27.114996 - gail/main.py:174 - [TRPO] iter = 19000 dist_mean = 0.1319 dist_std = 0.9634 vf_loss = 0.0944 grad_norm = 0.5515 nat_grad_norm = 0.6018 cg_residual = 0.0010 step_size = 0.3501 reward = 0.0000 fps = 6 mse_loss = 0.6418 
2022-07-08 09:15:57.122515 - gail/main.py:174 - [TRPO] iter = 20000 dist_mean = 0.1498 dist_std = 0.9619 vf_loss = 0.1741 grad_norm = 0.5041 nat_grad_norm = 0.4881 cg_residual = 0.0006 step_size = 0.4128 reward = 0.0000 fps = 5 mse_loss = 0.7263 
2022-07-08 09:15:58.013190 - gail/main.py:201 - [Discriminator] iter = 20000 loss = -1.2548 grad_norm = 6.7575 grad_penalty = 0.4616 regularization = 0.0000 true_logits = 0.1910 fake_logits = -1.5253 true_prob = 0.5483 fake_prob = 0.1903 
2022-07-08 09:16:18.965624 - gail/main.py:142 - [Evaluate] iter = 20000 episode={ returns = -8.0259 lengths = 55 } discounted_episode={ returns = -6.8165 lengths = 49 } 
2022-07-08 09:16:48.120727 - gail/main.py:174 - [TRPO] iter = 21000 dist_mean = 0.1712 dist_std = 0.9609 vf_loss = 0.1330 grad_norm = 0.4769 nat_grad_norm = 0.4800 cg_residual = 0.0006 step_size = 0.4453 reward = -0.0000 fps = 19 mse_loss = 0.7436 
2022-07-08 09:17:17.234445 - gail/main.py:174 - [TRPO] iter = 22000 dist_mean = 0.1970 dist_std = 0.9619 vf_loss = 0.1244 grad_norm = 0.6710 nat_grad_norm = 0.5022 cg_residual = 0.0008 step_size = 0.3608 reward = -0.0000 fps = 12 mse_loss = 0.8306 
2022-07-08 09:17:46.592267 - gail/main.py:174 - [TRPO] iter = 23000 dist_mean = 0.2170 dist_std = 0.9576 vf_loss = 0.2515 grad_norm = 0.7846 nat_grad_norm = 0.4655 cg_residual = 0.0008 step_size = 0.3362 reward = 0.0000 fps = 9 mse_loss = 0.8819 
2022-07-08 09:18:15.737466 - gail/main.py:174 - [TRPO] iter = 24000 dist_mean = 0.2446 dist_std = 0.9506 vf_loss = 0.1608 grad_norm = 0.6016 nat_grad_norm = 0.5013 cg_residual = 0.0008 step_size = 0.3591 reward = -0.0000 fps = 7 mse_loss = 1.0401 
2022-07-08 09:18:44.595347 - gail/main.py:174 - [TRPO] iter = 25000 dist_mean = 0.3028 dist_std = 0.9521 vf_loss = 0.3759 grad_norm = 0.4558 nat_grad_norm = 0.3943 cg_residual = 0.0006 step_size = 0.5044 reward = 0.0000 fps = 6 mse_loss = 0.9699 
2022-07-08 09:18:45.433698 - gail/main.py:201 - [Discriminator] iter = 25000 loss = -1.7051 grad_norm = 6.1630 grad_penalty = 0.3433 regularization = 0.0000 true_logits = 0.1819 fake_logits = -1.8665 true_prob = 0.5469 fake_prob = 0.1477 
2022-07-08 09:19:29.226699 - gail/main.py:142 - [Evaluate] iter = 25000 episode={ returns = 28.3291 lengths = 105 } discounted_episode={ returns = 28.2561 lengths = 104 } 
2022-07-08 09:19:58.304362 - gail/main.py:174 - [TRPO] iter = 26000 dist_mean = 0.3239 dist_std = 0.9516 vf_loss = 0.5443 grad_norm = 0.6160 nat_grad_norm = 0.4184 cg_residual = 0.0005 step_size = 0.4124 reward = 0.0000 fps = 13 mse_loss = 1.0390 
2022-07-08 09:20:27.907420 - gail/main.py:174 - [TRPO] iter = 27000 dist_mean = 0.4078 dist_std = 0.9513 vf_loss = 0.7579 grad_norm = 0.3325 nat_grad_norm = 0.4940 cg_residual = 0.0038 step_size = 0.4848 reward = 0.0000 fps = 9 mse_loss = 0.9761 
2022-07-08 09:20:56.484865 - gail/main.py:174 - [TRPO] iter = 28000 dist_mean = 0.3372 dist_std = 0.9504 vf_loss = 0.5919 grad_norm = 0.5256 nat_grad_norm = 0.4937 cg_residual = 0.0009 step_size = 0.4122 reward = -0.0000 fps = 7 mse_loss = 1.1166 
2022-07-08 09:21:24.322099 - gail/main.py:174 - [TRPO] iter = 29000 dist_mean = 0.3266 dist_std = 0.9441 vf_loss = 0.4702 grad_norm = 0.5705 nat_grad_norm = 0.4617 cg_residual = 0.0007 step_size = 0.4143 reward = -0.0000 fps = 6 mse_loss = 1.1905 
2022-07-08 09:21:58.124417 - gail/main.py:174 - [TRPO] iter = 30000 dist_mean = 0.5308 dist_std = 0.9373 vf_loss = 0.8212 grad_norm = 0.6709 nat_grad_norm = 0.4318 cg_residual = 0.0014 step_size = 0.4728 reward = -0.0000 fps = 5 mse_loss = 1.2254 
2022-07-08 09:21:59.590924 - gail/main.py:201 - [Discriminator] iter = 30000 loss = -1.8010 grad_norm = 5.8737 grad_penalty = 0.3305 regularization = 0.0000 true_logits = 0.2371 fake_logits = -1.8944 true_prob = 0.5604 fake_prob = 0.1507 
2022-07-08 09:22:55.642078 - gail/main.py:142 - [Evaluate] iter = 30000 episode={ returns = 254.7125 lengths = 138 } discounted_episode={ returns = 233.9096 lengths = 138 } 
2022-07-08 09:23:24.646547 - gail/main.py:174 - [TRPO] iter = 31000 dist_mean = 0.5480 dist_std = 0.9413 vf_loss = 1.0898 grad_norm = 0.5074 nat_grad_norm = 0.4945 cg_residual = 0.0035 step_size = 0.3845 reward = 0.0000 fps = 11 mse_loss = 1.2904 
2022-07-08 09:23:53.380194 - gail/main.py:174 - [TRPO] iter = 32000 dist_mean = 0.6210 dist_std = 0.9377 vf_loss = 1.0347 grad_norm = 0.4522 nat_grad_norm = 0.3818 cg_residual = 0.0019 step_size = 0.5525 reward = 0.0000 fps = 8 mse_loss = 1.3196 
2022-07-08 09:24:21.667624 - gail/main.py:174 - [TRPO] iter = 33000 dist_mean = 0.6403 dist_std = 0.9369 vf_loss = 0.9889 grad_norm = 0.5022 nat_grad_norm = 0.4454 cg_residual = 0.0068 step_size = 0.4332 reward = 0.0000 fps = 7 mse_loss = 1.4259 
2022-07-08 09:24:48.428191 - gail/main.py:174 - [TRPO] iter = 34000 dist_mean = 0.4351 dist_std = 0.9352 vf_loss = 1.3910 grad_norm = 0.4856 nat_grad_norm = 0.4196 cg_residual = 0.0033 step_size = 0.4525 reward = 0.0000 fps = 5 mse_loss = 1.5087 
2022-07-08 09:25:14.903244 - gail/main.py:174 - [TRPO] iter = 35000 dist_mean = 0.5969 dist_std = 0.9328 vf_loss = 1.6288 grad_norm = 0.3255 nat_grad_norm = 0.4548 cg_residual = 0.0018 step_size = 0.4563 reward = 0.0000 fps = 5 mse_loss = 1.5279 
2022-07-08 09:25:15.640177 - gail/main.py:201 - [Discriminator] iter = 35000 loss = -2.1083 grad_norm = 5.9056 grad_penalty = 0.2593 regularization = 0.0000 true_logits = 0.1977 fake_logits = -2.1699 true_prob = 0.5531 fake_prob = 0.1165 
2022-07-08 09:26:08.965407 - gail/main.py:142 - [Evaluate] iter = 35000 episode={ returns = 253.2754 lengths = 136 } discounted_episode={ returns = 230.3078 lengths = 134 } 
2022-07-08 09:26:38.277281 - gail/main.py:174 - [TRPO] iter = 36000 dist_mean = 0.6404 dist_std = 0.9230 vf_loss = 1.5652 grad_norm = 0.3823 nat_grad_norm = 0.4967 cg_residual = 0.0022 step_size = 0.4330 reward = -0.0000 fps = 12 mse_loss = 1.5583 
2022-07-08 09:27:05.758596 - gail/main.py:174 - [TRPO] iter = 37000 dist_mean = 0.5410 dist_std = 0.9294 vf_loss = 0.8179 grad_norm = 0.3289 nat_grad_norm = 0.4535 cg_residual = 0.0050 step_size = 0.5081 reward = -0.0000 fps = 9 mse_loss = 1.6333 
2022-07-08 09:27:32.637863 - gail/main.py:174 - [TRPO] iter = 38000 dist_mean = 0.4945 dist_std = 0.9286 vf_loss = 1.4161 grad_norm = 0.3238 nat_grad_norm = 0.4787 cg_residual = 0.0029 step_size = 0.4756 reward = 0.0000 fps = 7 mse_loss = 1.8016 
2022-07-08 09:27:59.418182 - gail/main.py:174 - [TRPO] iter = 39000 dist_mean = 0.4799 dist_std = 0.9238 vf_loss = 1.4031 grad_norm = 0.4510 nat_grad_norm = 0.5028 cg_residual = 0.0043 step_size = 0.3893 reward = 0.0000 fps = 6 mse_loss = 1.6507 
2022-07-08 09:28:25.934626 - gail/main.py:174 - [TRPO] iter = 40000 dist_mean = 0.5300 dist_std = 0.9142 vf_loss = 1.1524 grad_norm = 0.3842 nat_grad_norm = 0.4908 cg_residual = 0.0048 step_size = 0.4533 reward = 0.0000 fps = 5 mse_loss = 1.6172 
2022-07-08 09:28:26.688514 - gail/main.py:201 - [Discriminator] iter = 40000 loss = -2.6377 grad_norm = 6.1191 grad_penalty = 0.2952 regularization = 0.0000 true_logits = 0.2303 fake_logits = -2.7026 true_prob = 0.5598 fake_prob = 0.0739 
2022-07-08 09:29:18.904395 - gail/main.py:142 - [Evaluate] iter = 40000 episode={ returns = 250.4188 lengths = 134 } discounted_episode={ returns = 230.2201 lengths = 133 } 
2022-07-08 09:29:46.180736 - gail/main.py:174 - [TRPO] iter = 41000 dist_mean = 0.5501 dist_std = 0.9031 vf_loss = 1.0504 grad_norm = 0.3716 nat_grad_norm = 0.4948 cg_residual = 0.0041 step_size = 0.4379 reward = 0.0000 fps = 12 mse_loss = 1.7126 
2022-07-08 09:30:13.370817 - gail/main.py:174 - [TRPO] iter = 42000 dist_mean = 0.5619 dist_std = 0.9066 vf_loss = 0.9332 grad_norm = 0.3522 nat_grad_norm = 0.3779 cg_residual = 0.0018 step_size = 0.5135 reward = 0.0000 fps = 9 mse_loss = 1.6973 
2022-07-08 09:30:39.305867 - gail/main.py:174 - [TRPO] iter = 43000 dist_mean = 0.5328 dist_std = 0.9081 vf_loss = 1.3794 grad_norm = 0.4198 nat_grad_norm = 0.4382 cg_residual = 0.0045 step_size = 0.4462 reward = -0.0000 fps = 7 mse_loss = 1.6167 
2022-07-08 09:31:06.577610 - gail/main.py:174 - [TRPO] iter = 44000 dist_mean = 0.6144 dist_std = 0.9029 vf_loss = 0.6040 grad_norm = 0.3186 nat_grad_norm = 0.4723 cg_residual = 0.0035 step_size = 0.4437 reward = 0.0000 fps = 6 mse_loss = 1.6931 
2022-07-08 09:31:33.706517 - gail/main.py:174 - [TRPO] iter = 45000 dist_mean = 0.5694 dist_std = 0.8995 vf_loss = 0.7297 grad_norm = 0.5190 nat_grad_norm = 0.5932 cg_residual = 0.0071 step_size = 0.3601 reward = 0.0000 fps = 5 mse_loss = 1.7629 
2022-07-08 09:31:34.460765 - gail/main.py:201 - [Discriminator] iter = 45000 loss = -3.0449 grad_norm = 6.4021 grad_penalty = 0.3321 regularization = 0.0000 true_logits = 0.2484 fake_logits = -3.1287 true_prob = 0.5649 fake_prob = 0.0485 
2022-07-08 09:32:28.609880 - gail/main.py:142 - [Evaluate] iter = 45000 episode={ returns = 243.1046 lengths = 129 } discounted_episode={ returns = 226.9982 lengths = 130 } 
2022-07-08 09:32:55.058226 - gail/main.py:174 - [TRPO] iter = 46000 dist_mean = 0.5559 dist_std = 0.8880 vf_loss = 0.7622 grad_norm = 0.3655 nat_grad_norm = 0.4507 cg_residual = 0.0035 step_size = 0.4432 reward = 0.0000 fps = 12 mse_loss = 1.8356 
2022-07-08 09:33:21.621491 - gail/main.py:174 - [TRPO] iter = 47000 dist_mean = 0.5518 dist_std = 0.8836 vf_loss = 0.7803 grad_norm = 0.4393 nat_grad_norm = 0.5134 cg_residual = 0.0044 step_size = 0.3927 reward = 0.0000 fps = 9 mse_loss = 1.7566 
2022-07-08 09:33:48.621163 - gail/main.py:174 - [TRPO] iter = 48000 dist_mean = 0.5760 dist_std = 0.8867 vf_loss = 0.5026 grad_norm = 0.4339 nat_grad_norm = 0.5158 cg_residual = 0.0060 step_size = 0.3907 reward = 0.0000 fps = 7 mse_loss = 1.8524 
2022-07-08 09:34:14.684302 - gail/main.py:174 - [TRPO] iter = 49000 dist_mean = 0.5442 dist_std = 0.8823 vf_loss = 0.4593 grad_norm = 0.4196 nat_grad_norm = 0.4615 cg_residual = 0.0056 step_size = 0.4170 reward = 0.0000 fps = 6 mse_loss = 1.7358 
2022-07-08 09:34:40.598096 - gail/main.py:174 - [TRPO] iter = 50000 dist_mean = 0.5534 dist_std = 0.8813 vf_loss = 0.5494 grad_norm = 0.4639 nat_grad_norm = 0.4865 cg_residual = 0.0072 step_size = 0.4009 reward = -0.0000 fps = 5 mse_loss = 1.9055 
2022-07-08 09:34:41.437409 - gail/main.py:201 - [Discriminator] iter = 50000 loss = -3.3762 grad_norm = 7.5605 grad_penalty = 0.3834 regularization = 0.0000 true_logits = 0.1957 fake_logits = -3.5640 true_prob = 0.5548 fake_prob = 0.0328 
2022-07-08 09:35:20.631420 - gail/main.py:142 - [Evaluate] iter = 50000 episode={ returns = 214.2635 lengths = 109 } discounted_episode={ returns = 197.7473 lengths = 107 } 
2022-07-08 09:35:47.191421 - gail/main.py:174 - [TRPO] iter = 51000 dist_mean = 0.5347 dist_std = 0.8773 vf_loss = 0.5880 grad_norm = 0.4004 nat_grad_norm = 0.4834 cg_residual = 0.0131 step_size = 0.4360 reward = 0.0000 fps = 15 mse_loss = 1.8307 
2022-07-08 09:36:12.738570 - gail/main.py:174 - [TRPO] iter = 52000 dist_mean = 0.5781 dist_std = 0.8766 vf_loss = 0.3203 grad_norm = 0.4989 nat_grad_norm = 0.5311 cg_residual = 0.0050 step_size = 0.3869 reward = 0.0000 fps = 10 mse_loss = 1.9857 
2022-07-08 09:36:39.315580 - gail/main.py:174 - [TRPO] iter = 53000 dist_mean = 0.6064 dist_std = 0.8758 vf_loss = 0.1776 grad_norm = 0.6175 nat_grad_norm = 0.4202 cg_residual = 0.0046 step_size = 0.4468 reward = -0.0000 fps = 8 mse_loss = 1.9704 
2022-07-08 09:37:06.491342 - gail/main.py:174 - [TRPO] iter = 54000 dist_mean = 0.5639 dist_std = 0.8794 vf_loss = 0.2977 grad_norm = 0.4808 nat_grad_norm = 0.4494 cg_residual = 0.0053 step_size = 0.4360 reward = 0.0000 fps = 6 mse_loss = 1.9946 
2022-07-08 09:37:32.557301 - gail/main.py:174 - [TRPO] iter = 55000 dist_mean = 0.5606 dist_std = 0.8761 vf_loss = 0.2280 grad_norm = 0.5505 nat_grad_norm = 0.4431 cg_residual = 0.0064 step_size = 0.4339 reward = 0.0000 fps = 5 mse_loss = 2.0029 
2022-07-08 09:37:33.302429 - gail/main.py:201 - [Discriminator] iter = 55000 loss = -3.5174 grad_norm = 6.2295 grad_penalty = 0.4267 regularization = 0.0000 true_logits = 0.1773 fake_logits = -3.7668 true_prob = 0.5570 fake_prob = 0.0257 
2022-07-08 09:38:06.704821 - gail/main.py:142 - [Evaluate] iter = 55000 episode={ returns = 189.1634 lengths = 95 } discounted_episode={ returns = 180.5252 lengths = 96 } 
2022-07-08 09:38:32.774635 - gail/main.py:174 - [TRPO] iter = 56000 dist_mean = 0.5516 dist_std = 0.8751 vf_loss = 0.3763 grad_norm = 0.4721 nat_grad_norm = 0.4363 cg_residual = 0.0056 step_size = 0.4483 reward = 0.0000 fps = 16 mse_loss = 2.2809 
2022-07-08 09:38:59.264450 - gail/main.py:174 - [TRPO] iter = 57000 dist_mean = 0.5161 dist_std = 0.8778 vf_loss = 0.3892 grad_norm = 0.4546 nat_grad_norm = 0.4558 cg_residual = 0.0099 step_size = 0.4028 reward = -0.0000 fps = 11 mse_loss = 2.2288 
2022-07-08 09:39:25.074510 - gail/main.py:174 - [TRPO] iter = 58000 dist_mean = 0.5011 dist_std = 0.8746 vf_loss = 0.6423 grad_norm = 0.4399 nat_grad_norm = 0.4828 cg_residual = 0.0052 step_size = 0.4073 reward = -0.0000 fps = 8 mse_loss = 2.1972 
2022-07-08 09:39:51.755190 - gail/main.py:174 - [TRPO] iter = 59000 dist_mean = 0.5294 dist_std = 0.8745 vf_loss = 0.4081 grad_norm = 0.5146 nat_grad_norm = 0.5840 cg_residual = 0.0175 step_size = 0.3786 reward = 0.0000 fps = 7 mse_loss = 1.9832 
2022-07-08 09:40:18.665364 - gail/main.py:174 - [TRPO] iter = 60000 dist_mean = 0.5209 dist_std = 0.8710 vf_loss = 0.2710 grad_norm = 0.5567 nat_grad_norm = 0.4683 cg_residual = 0.0070 step_size = 0.3987 reward = 0.0000 fps = 6 mse_loss = 2.1380 
2022-07-08 09:40:19.469336 - gail/main.py:201 - [Discriminator] iter = 60000 loss = -3.6183 grad_norm = 4.4114 grad_penalty = 0.4245 regularization = 0.0000 true_logits = 0.1736 fake_logits = -3.8692 true_prob = 0.5523 fake_prob = 0.0240 
2022-07-08 09:40:53.588498 - gail/main.py:142 - [Evaluate] iter = 60000 episode={ returns = 186.4059 lengths = 95 } discounted_episode={ returns = 177.9951 lengths = 95 } 
2022-07-08 09:41:19.248717 - gail/main.py:174 - [TRPO] iter = 61000 dist_mean = 0.4991 dist_std = 0.8672 vf_loss = 0.3182 grad_norm = 0.5930 nat_grad_norm = 0.5445 cg_residual = 0.0169 step_size = 0.3575 reward = -0.0000 fps = 16 mse_loss = 2.2574 
2022-07-08 09:41:45.531584 - gail/main.py:174 - [TRPO] iter = 62000 dist_mean = 0.4997 dist_std = 0.8642 vf_loss = 0.3056 grad_norm = 0.6281 nat_grad_norm = 0.4128 cg_residual = 0.0071 step_size = 0.4062 reward = -0.0000 fps = 11 mse_loss = 2.2662 
2022-07-08 09:42:17.231409 - gail/main.py:174 - [TRPO] iter = 63000 dist_mean = 0.4993 dist_std = 0.8655 vf_loss = 0.3041 grad_norm = 0.3921 nat_grad_norm = 0.5189 cg_residual = 0.0107 step_size = 0.4171 reward = 0.0000 fps = 8 mse_loss = 2.2965 
2022-07-08 09:42:42.920771 - gail/main.py:174 - [TRPO] iter = 64000 dist_mean = 0.4660 dist_std = 0.8646 vf_loss = 0.4630 grad_norm = 0.6048 nat_grad_norm = 0.4817 cg_residual = 0.0097 step_size = 0.3946 reward = -0.0000 fps = 6 mse_loss = 2.3835 
2022-07-08 09:43:07.860752 - gail/main.py:174 - [TRPO] iter = 65000 dist_mean = 0.4608 dist_std = 0.8624 vf_loss = 0.2944 grad_norm = 0.5998 nat_grad_norm = 0.5032 cg_residual = 0.0147 step_size = 0.3900 reward = -0.0000 fps = 5 mse_loss = 2.5106 
2022-07-08 09:43:08.687784 - gail/main.py:201 - [Discriminator] iter = 65000 loss = -3.7844 grad_norm = 5.3798 grad_penalty = 0.4376 regularization = 0.0000 true_logits = 0.2135 fake_logits = -4.0086 true_prob = 0.5638 fake_prob = 0.0202 
2022-07-08 09:43:33.377089 - gail/main.py:142 - [Evaluate] iter = 65000 episode={ returns = 141.8145 lengths = 75 } discounted_episode={ returns = 138.4724 lengths = 76 } 
2022-07-08 09:43:58.945715 - gail/main.py:174 - [TRPO] iter = 66000 dist_mean = 0.4513 dist_std = 0.8625 vf_loss = 0.2475 grad_norm = 0.5307 nat_grad_norm = 0.5368 cg_residual = 0.0161 step_size = 0.3795 reward = -0.0000 fps = 19 mse_loss = 2.6168 
2022-07-08 09:44:24.701602 - gail/main.py:174 - [TRPO] iter = 67000 dist_mean = 0.4581 dist_std = 0.8634 vf_loss = 0.2635 grad_norm = 0.6299 nat_grad_norm = 0.5561 cg_residual = 0.0142 step_size = 0.3597 reward = 0.0000 fps = 13 mse_loss = 2.4911 
2022-07-08 09:44:50.170104 - gail/main.py:174 - [TRPO] iter = 68000 dist_mean = 0.4496 dist_std = 0.8630 vf_loss = 0.3004 grad_norm = 0.5716 nat_grad_norm = 0.5711 cg_residual = 0.0182 step_size = 0.3833 reward = 0.0000 fps = 9 mse_loss = 2.6944 
2022-07-08 09:45:15.008027 - gail/main.py:174 - [TRPO] iter = 69000 dist_mean = 0.4067 dist_std = 0.8636 vf_loss = 0.3055 grad_norm = 0.5505 nat_grad_norm = 0.5253 cg_residual = 0.0134 step_size = 0.3984 reward = -0.0000 fps = 7 mse_loss = 2.8342 
2022-07-08 09:45:40.553826 - gail/main.py:174 - [TRPO] iter = 70000 dist_mean = 0.4242 dist_std = 0.8574 vf_loss = 0.1783 grad_norm = 0.6409 nat_grad_norm = 0.6867 cg_residual = 0.0231 step_size = 0.2938 reward = 0.0000 fps = 6 mse_loss = 2.8935 
2022-07-08 09:45:41.275833 - gail/main.py:201 - [Discriminator] iter = 70000 loss = -3.8944 grad_norm = 5.8287 grad_penalty = 0.4184 regularization = 0.0000 true_logits = 0.1967 fake_logits = -4.1161 true_prob = 0.5581 fake_prob = 0.0181 
2022-07-08 09:46:03.132071 - gail/main.py:142 - [Evaluate] iter = 70000 episode={ returns = 123.7740 lengths = 67 } discounted_episode={ returns = 119.1177 lengths = 67 } 
2022-07-08 09:46:28.478570 - gail/main.py:174 - [TRPO] iter = 71000 dist_mean = 0.4051 dist_std = 0.8578 vf_loss = 0.3408 grad_norm = 0.5553 nat_grad_norm = 0.6601 cg_residual = 0.0083 step_size = 0.3629 reward = -0.0000 fps = 21 mse_loss = 2.9463 
2022-07-08 09:46:53.442647 - gail/main.py:174 - [TRPO] iter = 72000 dist_mean = 0.3953 dist_std = 0.8628 vf_loss = 0.3040 grad_norm = 0.4352 nat_grad_norm = 0.6024 cg_residual = 0.0167 step_size = 0.3812 reward = 0.0000 fps = 13 mse_loss = 2.9702 
2022-07-08 09:47:18.596588 - gail/main.py:174 - [TRPO] iter = 73000 dist_mean = 0.4461 dist_std = 0.8647 vf_loss = 0.2882 grad_norm = 0.6088 nat_grad_norm = 0.6544 cg_residual = 0.0185 step_size = 0.3480 reward = 0.0000 fps = 10 mse_loss = 2.6759 
2022-07-08 09:47:44.026830 - gail/main.py:174 - [TRPO] iter = 74000 dist_mean = 0.4195 dist_std = 0.8656 vf_loss = 0.2562 grad_norm = 0.5627 nat_grad_norm = 0.6314 cg_residual = 0.0269 step_size = 0.3383 reward = 0.0000 fps = 8 mse_loss = 2.7704 
2022-07-08 09:48:09.427841 - gail/main.py:174 - [TRPO] iter = 75000 dist_mean = 0.4187 dist_std = 0.8649 vf_loss = 0.2774 grad_norm = 0.4309 nat_grad_norm = 0.6733 cg_residual = 0.0184 step_size = 0.3755 reward = 0.0000 fps = 6 mse_loss = 2.8410 
2022-07-08 09:48:10.182528 - gail/main.py:201 - [Discriminator] iter = 75000 loss = -3.9919 grad_norm = 3.7920 grad_penalty = 0.3829 regularization = 0.0000 true_logits = 0.1629 fake_logits = -4.2118 true_prob = 0.5498 fake_prob = 0.0173 
2022-07-08 09:48:34.618433 - gail/main.py:142 - [Evaluate] iter = 75000 episode={ returns = 136.8061 lengths = 72 } discounted_episode={ returns = 135.3354 lengths = 74 } 
2022-07-08 09:49:00.423743 - gail/main.py:174 - [TRPO] iter = 76000 dist_mean = 0.3823 dist_std = 0.8675 vf_loss = 0.3474 grad_norm = 0.4586 nat_grad_norm = 0.5148 cg_residual = 0.0130 step_size = 0.4147 reward = 0.0000 fps = 19 mse_loss = 2.7629 
2022-07-08 09:49:26.358449 - gail/main.py:174 - [TRPO] iter = 77000 dist_mean = 0.3996 dist_std = 0.8676 vf_loss = 0.3020 grad_norm = 0.6971 nat_grad_norm = 0.7249 cg_residual = 0.0170 step_size = 0.3138 reward = -0.0000 fps = 13 mse_loss = 2.7879 
2022-07-08 09:49:54.669056 - gail/main.py:174 - [TRPO] iter = 78000 dist_mean = 0.3885 dist_std = 0.8654 vf_loss = 0.3387 grad_norm = 0.5437 nat_grad_norm = 0.5695 cg_residual = 0.0152 step_size = 0.3784 reward = -0.0000 fps = 9 mse_loss = 2.5379 
2022-07-08 09:50:21.976294 - gail/main.py:174 - [TRPO] iter = 79000 dist_mean = 0.3959 dist_std = 0.8639 vf_loss = 0.3204 grad_norm = 0.6549 nat_grad_norm = 0.5985 cg_residual = 0.0195 step_size = 0.3496 reward = -0.0000 fps = 7 mse_loss = 2.6074 
2022-07-08 09:50:46.904568 - gail/main.py:174 - [TRPO] iter = 80000 dist_mean = 0.3778 dist_std = 0.8605 vf_loss = 0.3614 grad_norm = 0.5861 nat_grad_norm = 0.6286 cg_residual = 0.0210 step_size = 0.3452 reward = 0.0000 fps = 6 mse_loss = 2.6136 
2022-07-08 09:50:47.649022 - gail/main.py:201 - [Discriminator] iter = 80000 loss = -4.0601 grad_norm = 3.4636 grad_penalty = 0.4341 regularization = 0.0000 true_logits = 0.1737 fake_logits = -4.3205 true_prob = 0.5527 fake_prob = 0.0155 
2022-07-08 09:51:14.217525 - gail/main.py:142 - [Evaluate] iter = 80000 episode={ returns = 165.0998 lengths = 82 } discounted_episode={ returns = 153.8501 lengths = 81 } 
2022-07-08 09:51:38.804547 - gail/main.py:174 - [TRPO] iter = 81000 dist_mean = 0.3943 dist_std = 0.8616 vf_loss = 0.3204 grad_norm = 0.5836 nat_grad_norm = 0.5104 cg_residual = 0.0143 step_size = 0.3796 reward = -0.0000 fps = 19 mse_loss = 2.8213 
2022-07-08 09:52:11.551063 - gail/main.py:174 - [TRPO] iter = 82000 dist_mean = 0.3649 dist_std = 0.8602 vf_loss = 0.2410 grad_norm = 0.5690 nat_grad_norm = 0.5801 cg_residual = 0.0206 step_size = 0.3519 reward = 0.0000 fps = 11 mse_loss = 2.6465 
2022-07-08 09:52:36.870497 - gail/main.py:174 - [TRPO] iter = 83000 dist_mean = 0.3682 dist_std = 0.8570 vf_loss = 0.2366 grad_norm = 0.4920 nat_grad_norm = 0.5763 cg_residual = 0.0172 step_size = 0.3676 reward = -0.0000 fps = 9 mse_loss = 3.0122 
2022-07-08 09:53:02.391354 - gail/main.py:174 - [TRPO] iter = 84000 dist_mean = 0.3536 dist_std = 0.8544 vf_loss = 0.3800 grad_norm = 0.5665 nat_grad_norm = 0.6331 cg_residual = 0.0207 step_size = 0.3610 reward = -0.0000 fps = 7 mse_loss = 2.6551 
2022-07-08 09:53:29.191586 - gail/main.py:174 - [TRPO] iter = 85000 dist_mean = 0.3861 dist_std = 0.8550 vf_loss = 0.2803 grad_norm = 0.6356 nat_grad_norm = 0.6278 cg_residual = 0.0243 step_size = 0.3327 reward = -0.0000 fps = 6 mse_loss = 2.5486 
2022-07-08 09:53:30.063332 - gail/main.py:201 - [Discriminator] iter = 85000 loss = -4.0153 grad_norm = 3.6276 grad_penalty = 0.3792 regularization = 0.0000 true_logits = 0.0864 fake_logits = -4.3081 true_prob = 0.5398 fake_prob = 0.0159 
2022-07-08 09:54:00.283082 - gail/main.py:142 - [Evaluate] iter = 85000 episode={ returns = 171.0220 lengths = 84 } discounted_episode={ returns = 161.7468 lengths = 83 } 
2022-07-08 09:54:27.469152 - gail/main.py:174 - [TRPO] iter = 86000 dist_mean = 0.3957 dist_std = 0.8504 vf_loss = 0.4676 grad_norm = 0.4496 nat_grad_norm = 0.6331 cg_residual = 0.0156 step_size = 0.3888 reward = -0.0000 fps = 17 mse_loss = 2.5307 
2022-07-08 09:54:53.022138 - gail/main.py:174 - [TRPO] iter = 87000 dist_mean = 0.3721 dist_std = 0.8471 vf_loss = 0.4274 grad_norm = 0.6691 nat_grad_norm = 0.5247 cg_residual = 0.0202 step_size = 0.3745 reward = 0.0000 fps = 12 mse_loss = 2.6618 
2022-07-08 09:55:18.172638 - gail/main.py:174 - [TRPO] iter = 88000 dist_mean = 0.3682 dist_std = 0.8450 vf_loss = 0.2797 grad_norm = 0.6061 nat_grad_norm = 0.5385 cg_residual = 0.0261 step_size = 0.3800 reward = -0.0000 fps = 9 mse_loss = 3.0665 
2022-07-08 09:55:41.730885 - gail/main.py:174 - [TRPO] iter = 89000 dist_mean = 0.3651 dist_std = 0.8443 vf_loss = 0.3348 grad_norm = 0.5241 nat_grad_norm = 0.5050 cg_residual = 0.0195 step_size = 0.4318 reward = 0.0000 fps = 7 mse_loss = 3.1133 
2022-07-08 09:56:06.115609 - gail/main.py:174 - [TRPO] iter = 90000 dist_mean = 0.3631 dist_std = 0.8404 vf_loss = 0.3198 grad_norm = 0.5399 nat_grad_norm = 0.5427 cg_residual = 0.0204 step_size = 0.3867 reward = -0.0000 fps = 6 mse_loss = 2.8650 
2022-07-08 09:56:06.839136 - gail/main.py:201 - [Discriminator] iter = 90000 loss = -4.1728 grad_norm = 3.6897 grad_penalty = 0.3930 regularization = 0.0000 true_logits = 0.0721 fake_logits = -4.4937 true_prob = 0.5366 fake_prob = 0.0133 
2022-07-08 09:56:34.445099 - gail/main.py:142 - [Evaluate] iter = 90000 episode={ returns = 175.8957 lengths = 85 } discounted_episode={ returns = 165.2300 lengths = 84 } 
2022-07-08 09:56:58.668441 - gail/main.py:174 - [TRPO] iter = 91000 dist_mean = 0.3674 dist_std = 0.8416 vf_loss = 0.3754 grad_norm = 0.5339 nat_grad_norm = 0.5424 cg_residual = 0.0122 step_size = 0.3652 reward = 0.0000 fps = 19 mse_loss = 2.8435 
2022-07-08 09:57:22.395903 - gail/main.py:174 - [TRPO] iter = 92000 dist_mean = 0.3613 dist_std = 0.8354 vf_loss = 0.3730 grad_norm = 0.4825 nat_grad_norm = 0.5527 cg_residual = 0.0261 step_size = 0.3782 reward = 0.0000 fps = 13 mse_loss = 3.1695 
2022-07-08 09:57:46.710642 - gail/main.py:174 - [TRPO] iter = 93000 dist_mean = 0.3627 dist_std = 0.8358 vf_loss = 0.2190 grad_norm = 0.4718 nat_grad_norm = 0.6300 cg_residual = 0.0193 step_size = 0.3581 reward = -0.0000 fps = 10 mse_loss = 3.2938 
2022-07-08 09:58:11.077056 - gail/main.py:174 - [TRPO] iter = 94000 dist_mean = 0.3697 dist_std = 0.8340 vf_loss = 0.3431 grad_norm = 0.5761 nat_grad_norm = 0.6295 cg_residual = 0.0173 step_size = 0.3514 reward = -0.0000 fps = 8 mse_loss = 3.5817 
2022-07-08 09:58:37.528308 - gail/main.py:174 - [TRPO] iter = 95000 dist_mean = 0.3507 dist_std = 0.8327 vf_loss = 0.3317 grad_norm = 0.5692 nat_grad_norm = 0.5461 cg_residual = 0.0101 step_size = 0.3892 reward = -0.0000 fps = 6 mse_loss = 3.4560 
2022-07-08 09:58:38.310834 - gail/main.py:201 - [Discriminator] iter = 95000 loss = -4.1630 grad_norm = 3.3876 grad_penalty = 0.4383 regularization = 0.0000 true_logits = 0.0837 fake_logits = -4.5175 true_prob = 0.5405 fake_prob = 0.0129 
2022-07-08 09:59:05.791530 - gail/main.py:142 - [Evaluate] iter = 95000 episode={ returns = 163.7360 lengths = 82 } discounted_episode={ returns = 153.3929 lengths = 81 } 
2022-07-08 09:59:31.916911 - gail/main.py:174 - [TRPO] iter = 96000 dist_mean = 0.3828 dist_std = 0.8345 vf_loss = 0.2852 grad_norm = 0.5771 nat_grad_norm = 0.5837 cg_residual = 0.0258 step_size = 0.3515 reward = -0.0000 fps = 18 mse_loss = 3.4387 
2022-07-08 09:59:59.065491 - gail/main.py:174 - [TRPO] iter = 97000 dist_mean = 0.3885 dist_std = 0.8365 vf_loss = 0.2468 grad_norm = 0.5762 nat_grad_norm = 0.5267 cg_residual = 0.0186 step_size = 0.3594 reward = -0.0000 fps = 12 mse_loss = 3.2878 
2022-07-08 10:00:25.735805 - gail/main.py:174 - [TRPO] iter = 98000 dist_mean = 0.3666 dist_std = 0.8332 vf_loss = 0.1999 grad_norm = 0.6019 nat_grad_norm = 0.6281 cg_residual = 0.0310 step_size = 0.3549 reward = 0.0000 fps = 9 mse_loss = 3.1300 
2022-07-08 10:00:52.669308 - gail/main.py:174 - [TRPO] iter = 99000 dist_mean = 0.3836 dist_std = 0.8313 vf_loss = 0.2585 grad_norm = 0.5014 nat_grad_norm = 0.6031 cg_residual = 0.0279 step_size = 0.3597 reward = 0.0000 fps = 7 mse_loss = 3.1972 
2022-07-08 10:01:18.987360 - gail/main.py:174 - [TRPO] iter = 100000 dist_mean = 0.3851 dist_std = 0.8291 vf_loss = 0.2069 grad_norm = 0.4602 nat_grad_norm = 0.5958 cg_residual = 0.0262 step_size = 0.3783 reward = 0.0000 fps = 6 mse_loss = 3.1580 
2022-07-08 10:01:19.733650 - gail/main.py:201 - [Discriminator] iter = 100000 loss = -4.2180 grad_norm = 3.4774 grad_penalty = 0.4027 regularization = 0.0000 true_logits = 0.0412 fake_logits = -4.5795 true_prob = 0.5310 fake_prob = 0.0123 
2022-07-08 10:02:00.279087 - gail/main.py:142 - [Evaluate] iter = 100000 episode={ returns = 203.7957 lengths = 98 } discounted_episode={ returns = 193.5462 lengths = 99 } 
2022-07-08 10:02:33.629398 - gail/main.py:174 - [TRPO] iter = 101000 dist_mean = 0.3897 dist_std = 0.8274 vf_loss = 0.2235 grad_norm = 0.6096 nat_grad_norm = 0.5604 cg_residual = 0.0157 step_size = 0.3707 reward = -0.0000 fps = 13 mse_loss = 3.0515 
2022-07-08 10:03:02.754282 - gail/main.py:174 - [TRPO] iter = 102000 dist_mean = 0.3697 dist_std = 0.8242 vf_loss = 0.2162 grad_norm = 0.6242 nat_grad_norm = 0.5187 cg_residual = 0.0200 step_size = 0.3563 reward = 0.0000 fps = 9 mse_loss = 3.0187 
2022-07-08 10:03:30.693861 - gail/main.py:174 - [TRPO] iter = 103000 dist_mean = 0.3866 dist_std = 0.8247 vf_loss = 0.3550 grad_norm = 0.6392 nat_grad_norm = 0.6393 cg_residual = 0.0253 step_size = 0.3266 reward = 0.0000 fps = 7 mse_loss = 3.0201 
2022-07-08 10:04:00.704836 - gail/main.py:174 - [TRPO] iter = 104000 dist_mean = 0.3824 dist_std = 0.8203 vf_loss = 0.2669 grad_norm = 0.5069 nat_grad_norm = 0.5231 cg_residual = 0.0185 step_size = 0.3995 reward = 0.0000 fps = 6 mse_loss = 2.8890 
2022-07-08 10:04:29.189858 - gail/main.py:174 - [TRPO] iter = 105000 dist_mean = 0.3749 dist_std = 0.8186 vf_loss = 0.2051 grad_norm = 0.6377 nat_grad_norm = 0.4761 cg_residual = 0.0184 step_size = 0.3803 reward = -0.0000 fps = 5 mse_loss = 2.7526 
2022-07-08 10:04:29.985447 - gail/main.py:201 - [Discriminator] iter = 105000 loss = -4.2203 grad_norm = 3.0321 grad_penalty = 0.4167 regularization = 0.0000 true_logits = 0.0536 fake_logits = -4.5834 true_prob = 0.5284 fake_prob = 0.0119 
2022-07-08 10:05:11.611302 - gail/main.py:142 - [Evaluate] iter = 105000 episode={ returns = 217.5452 lengths = 104 } discounted_episode={ returns = 207.8049 lengths = 106 } 
2022-07-08 10:05:41.360443 - gail/main.py:174 - [TRPO] iter = 106000 dist_mean = 0.3839 dist_std = 0.8153 vf_loss = 0.1963 grad_norm = 0.6097 nat_grad_norm = 0.5466 cg_residual = 0.0162 step_size = 0.3576 reward = 0.0000 fps = 14 mse_loss = 3.0136 
2022-07-08 10:06:10.270118 - gail/main.py:174 - [TRPO] iter = 107000 dist_mean = 0.3762 dist_std = 0.8112 vf_loss = 0.2606 grad_norm = 0.5647 nat_grad_norm = 0.4821 cg_residual = 0.0154 step_size = 0.3664 reward = 0.0000 fps = 9 mse_loss = 3.0520 
2022-07-08 10:06:39.406094 - gail/main.py:174 - [TRPO] iter = 108000 dist_mean = 0.3723 dist_std = 0.8070 vf_loss = 0.1971 grad_norm = 0.6167 nat_grad_norm = 0.5104 cg_residual = 0.0270 step_size = 0.3639 reward = -0.0000 fps = 7 mse_loss = 2.8441 
2022-07-08 10:07:09.033718 - gail/main.py:174 - [TRPO] iter = 109000 dist_mean = 0.3832 dist_std = 0.8076 vf_loss = 0.2157 grad_norm = 0.6007 nat_grad_norm = 0.4751 cg_residual = 0.0171 step_size = 0.3884 reward = 0.0000 fps = 6 mse_loss = 2.6979 
2022-07-08 10:07:37.597268 - gail/main.py:174 - [TRPO] iter = 110000 dist_mean = 0.3747 dist_std = 0.8044 vf_loss = 0.1642 grad_norm = 0.5643 nat_grad_norm = 0.4738 cg_residual = 0.0225 step_size = 0.3923 reward = 0.0000 fps = 5 mse_loss = 2.7511 
2022-07-08 10:07:38.416143 - gail/main.py:201 - [Discriminator] iter = 110000 loss = -4.2562 grad_norm = 3.0385 grad_penalty = 0.4175 regularization = 0.0000 true_logits = 0.1123 fake_logits = -4.5614 true_prob = 0.5420 fake_prob = 0.0124 
2022-07-08 10:08:17.990732 - gail/main.py:142 - [Evaluate] iter = 110000 episode={ returns = 213.4301 lengths = 101 } discounted_episode={ returns = 203.5067 lengths = 103 } 
2022-07-08 10:08:47.345157 - gail/main.py:174 - [TRPO] iter = 111000 dist_mean = 0.3971 dist_std = 0.7994 vf_loss = 0.2384 grad_norm = 0.5755 nat_grad_norm = 0.5060 cg_residual = 0.0313 step_size = 0.3602 reward = 0.0000 fps = 14 mse_loss = 2.9280 
2022-07-08 10:09:16.973304 - gail/main.py:174 - [TRPO] iter = 112000 dist_mean = 0.4060 dist_std = 0.8010 vf_loss = 0.2287 grad_norm = 0.5319 nat_grad_norm = 0.5322 cg_residual = 0.0267 step_size = 0.3897 reward = 0.0000 fps = 10 mse_loss = 2.9872 
2022-07-08 10:09:46.598349 - gail/main.py:174 - [TRPO] iter = 113000 dist_mean = 0.3932 dist_std = 0.7951 vf_loss = 0.1682 grad_norm = 0.3902 nat_grad_norm = 0.4213 cg_residual = 0.0153 step_size = 0.4647 reward = 0.0000 fps = 7 mse_loss = 3.0019 
2022-07-08 10:10:16.237047 - gail/main.py:174 - [TRPO] iter = 114000 dist_mean = 0.3878 dist_std = 0.7922 vf_loss = 0.1897 grad_norm = 0.5890 nat_grad_norm = 0.5406 cg_residual = 0.0342 step_size = 0.3726 reward = -0.0000 fps = 6 mse_loss = 3.0248 
2022-07-08 10:10:44.535971 - gail/main.py:174 - [TRPO] iter = 115000 dist_mean = 0.3856 dist_std = 0.7894 vf_loss = 0.2516 grad_norm = 0.6271 nat_grad_norm = 0.5653 cg_residual = 0.0362 step_size = 0.3659 reward = 0.0000 fps = 5 mse_loss = 3.1340 
2022-07-08 10:10:45.299583 - gail/main.py:201 - [Discriminator] iter = 115000 loss = -4.2242 grad_norm = 2.6463 grad_penalty = 0.4013 regularization = 0.0000 true_logits = 0.1034 fake_logits = -4.5222 true_prob = 0.5453 fake_prob = 0.0130 
2022-07-08 10:11:30.256380 - gail/main.py:142 - [Evaluate] iter = 115000 episode={ returns = 237.6382 lengths = 112 } discounted_episode={ returns = 223.7269 lengths = 113 } 
2022-07-08 10:12:06.755112 - gail/main.py:174 - [TRPO] iter = 116000 dist_mean = 0.3947 dist_std = 0.7897 vf_loss = 0.2501 grad_norm = 0.5286 nat_grad_norm = 0.4760 cg_residual = 0.0205 step_size = 0.3844 reward = -0.0000 fps = 12 mse_loss = 3.1660 
2022-07-08 10:12:35.742251 - gail/main.py:174 - [TRPO] iter = 117000 dist_mean = 0.3863 dist_std = 0.7834 vf_loss = 0.1981 grad_norm = 0.5628 nat_grad_norm = 0.5122 cg_residual = 0.0313 step_size = 0.3844 reward = 0.0000 fps = 9 mse_loss = 3.1780 
2022-07-08 10:13:04.053706 - gail/main.py:174 - [TRPO] iter = 118000 dist_mean = 0.3844 dist_std = 0.7821 vf_loss = 0.2198 grad_norm = 0.6464 nat_grad_norm = 0.4857 cg_residual = 0.0214 step_size = 0.4081 reward = 0.0000 fps = 7 mse_loss = 3.0939 
2022-07-08 10:13:32.480636 - gail/main.py:174 - [TRPO] iter = 119000 dist_mean = 0.3975 dist_std = 0.7772 vf_loss = 0.2008 grad_norm = 0.6614 nat_grad_norm = 0.4507 cg_residual = 0.0285 step_size = 0.3635 reward = -0.0000 fps = 5 mse_loss = 3.1450 
2022-07-08 10:14:00.166468 - gail/main.py:174 - [TRPO] iter = 120000 dist_mean = 0.3874 dist_std = 0.7757 vf_loss = 0.2024 grad_norm = 0.5508 nat_grad_norm = 0.5169 cg_residual = 0.0199 step_size = 0.3902 reward = -0.0000 fps = 5 mse_loss = 3.2894 
2022-07-08 10:14:01.009371 - gail/main.py:201 - [Discriminator] iter = 120000 loss = -4.2696 grad_norm = 3.4501 grad_penalty = 0.4135 regularization = 0.0000 true_logits = 0.1453 fake_logits = -4.5378 true_prob = 0.5500 fake_prob = 0.0125 
2022-07-08 10:14:44.201927 - gail/main.py:142 - [Evaluate] iter = 120000 episode={ returns = 231.4398 lengths = 109 } discounted_episode={ returns = 220.2825 lengths = 110 } 
2022-07-08 10:15:13.681123 - gail/main.py:174 - [TRPO] iter = 121000 dist_mean = 0.4101 dist_std = 0.7753 vf_loss = 0.2555 grad_norm = 0.6486 nat_grad_norm = 0.4826 cg_residual = 0.0174 step_size = 0.3756 reward = -0.0000 fps = 13 mse_loss = 3.0848 
2022-07-08 10:15:42.702326 - gail/main.py:174 - [TRPO] iter = 122000 dist_mean = 0.4107 dist_std = 0.7696 vf_loss = 0.2040 grad_norm = 0.6191 nat_grad_norm = 0.4967 cg_residual = 0.0288 step_size = 0.3769 reward = 0.0000 fps = 9 mse_loss = 3.3645 
2022-07-08 10:16:11.315147 - gail/main.py:174 - [TRPO] iter = 123000 dist_mean = 0.4114 dist_std = 0.7667 vf_loss = 0.2871 grad_norm = 0.5715 nat_grad_norm = 0.5005 cg_residual = 0.0216 step_size = 0.3608 reward = -0.0000 fps = 7 mse_loss = 3.2863 
2022-07-08 10:16:39.174415 - gail/main.py:174 - [TRPO] iter = 124000 dist_mean = 0.3958 dist_std = 0.7641 vf_loss = 0.1801 grad_norm = 0.6470 nat_grad_norm = 0.4740 cg_residual = 0.0248 step_size = 0.4050 reward = -0.0000 fps = 6 mse_loss = 3.3426 
2022-07-08 10:17:07.870241 - gail/main.py:174 - [TRPO] iter = 125000 dist_mean = 0.3935 dist_std = 0.7576 vf_loss = 0.2563 grad_norm = 0.6306 nat_grad_norm = 0.4622 cg_residual = 0.0326 step_size = 0.3658 reward = -0.0000 fps = 5 mse_loss = 3.5860 
2022-07-08 10:17:08.661047 - gail/main.py:201 - [Discriminator] iter = 125000 loss = -4.1877 grad_norm = 2.9040 grad_penalty = 0.3872 regularization = 0.0000 true_logits = 0.1209 fake_logits = -4.4540 true_prob = 0.5433 fake_prob = 0.0138 
2022-07-08 10:17:51.838913 - gail/main.py:142 - [Evaluate] iter = 125000 episode={ returns = 241.8627 lengths = 113 } discounted_episode={ returns = 225.9562 lengths = 113 } 
2022-07-08 10:18:19.665545 - gail/main.py:174 - [TRPO] iter = 126000 dist_mean = 0.3932 dist_std = 0.7566 vf_loss = 0.2366 grad_norm = 0.5980 nat_grad_norm = 0.4798 cg_residual = 0.0258 step_size = 0.4076 reward = -0.0000 fps = 14 mse_loss = 3.5301 
2022-07-08 10:18:46.960282 - gail/main.py:174 - [TRPO] iter = 127000 dist_mean = 0.3850 dist_std = 0.7566 vf_loss = 0.2691 grad_norm = 0.4998 nat_grad_norm = 0.4630 cg_residual = 0.0267 step_size = 0.3797 reward = -0.0000 fps = 10 mse_loss = 3.2707 
2022-07-08 10:19:13.070705 - gail/main.py:174 - [TRPO] iter = 128000 dist_mean = 0.3885 dist_std = 0.7558 vf_loss = 0.2319 grad_norm = 0.7492 nat_grad_norm = 0.4310 cg_residual = 0.0259 step_size = 0.3991 reward = -0.0000 fps = 8 mse_loss = 3.4096 
2022-07-08 10:19:42.160870 - gail/main.py:174 - [TRPO] iter = 129000 dist_mean = 0.3960 dist_std = 0.7509 vf_loss = 0.2192 grad_norm = 0.6852 nat_grad_norm = 0.4717 cg_residual = 0.0213 step_size = 0.3877 reward = -0.0000 fps = 6 mse_loss = 3.1741 
2022-07-08 10:20:10.803711 - gail/main.py:174 - [TRPO] iter = 130000 dist_mean = 0.4065 dist_std = 0.7517 vf_loss = 0.4089 grad_norm = 0.4807 nat_grad_norm = 0.5813 cg_residual = 0.0590 step_size = 0.4073 reward = -0.0000 fps = 5 mse_loss = 3.5435 
2022-07-08 10:20:11.659917 - gail/main.py:201 - [Discriminator] iter = 130000 loss = -4.1987 grad_norm = 3.5065 grad_penalty = 0.3920 regularization = 0.0000 true_logits = 0.1274 fake_logits = -4.4634 true_prob = 0.5482 fake_prob = 0.0142 
2022-07-08 10:20:55.092981 - gail/main.py:142 - [Evaluate] iter = 130000 episode={ returns = 254.1378 lengths = 116 } discounted_episode={ returns = 226.3574 lengths = 110 } 
2022-07-08 10:21:23.598442 - gail/main.py:174 - [TRPO] iter = 131000 dist_mean = 0.4024 dist_std = 0.7503 vf_loss = 0.3485 grad_norm = 0.5962 nat_grad_norm = 0.4686 cg_residual = 0.0503 step_size = 0.4011 reward = 0.0000 fps = 13 mse_loss = 3.3403 
2022-07-08 10:21:57.705480 - gail/main.py:174 - [TRPO] iter = 132000 dist_mean = 0.3855 dist_std = 0.7480 vf_loss = 0.2211 grad_norm = 0.7497 nat_grad_norm = 0.5594 cg_residual = 0.0569 step_size = 0.3417 reward = -0.0000 fps = 9 mse_loss = 3.6713 
2022-07-08 10:22:24.328100 - gail/main.py:174 - [TRPO] iter = 133000 dist_mean = 0.3869 dist_std = 0.7459 vf_loss = 0.2190 grad_norm = 0.7524 nat_grad_norm = 0.4758 cg_residual = 0.0647 step_size = 0.3865 reward = 0.0000 fps = 7 mse_loss = 3.5379 
2022-07-08 10:22:52.635564 - gail/main.py:174 - [TRPO] iter = 134000 dist_mean = 0.4166 dist_std = 0.7423 vf_loss = 0.3473 grad_norm = 0.6990 nat_grad_norm = 0.5194 cg_residual = 0.0363 step_size = 0.3618 reward = 0.0000 fps = 6 mse_loss = 3.7105 
2022-07-08 10:23:21.053286 - gail/main.py:174 - [TRPO] iter = 135000 dist_mean = 0.3920 dist_std = 0.7391 vf_loss = 0.3888 grad_norm = 0.8435 nat_grad_norm = 0.5646 cg_residual = 0.0386 step_size = 0.3161 reward = 0.0000 fps = 5 mse_loss = 3.1896 
2022-07-08 10:23:21.869433 - gail/main.py:201 - [Discriminator] iter = 135000 loss = -4.2781 grad_norm = 3.5165 grad_penalty = 0.3906 regularization = 0.0000 true_logits = 0.1379 fake_logits = -4.5307 true_prob = 0.5501 fake_prob = 0.0129 
2022-07-08 10:24:05.749057 - gail/main.py:142 - [Evaluate] iter = 135000 episode={ returns = 247.2162 lengths = 114 } discounted_episode={ returns = 239.0235 lengths = 118 } 
2022-07-08 10:24:35.214699 - gail/main.py:174 - [TRPO] iter = 136000 dist_mean = 0.4041 dist_std = 0.7383 vf_loss = 0.3407 grad_norm = 0.7769 nat_grad_norm = 0.5934 cg_residual = 0.0618 step_size = 0.3264 reward = 0.0000 fps = 13 mse_loss = 3.5093 
2022-07-08 10:25:04.050766 - gail/main.py:174 - [TRPO] iter = 137000 dist_mean = 0.4036 dist_std = 0.7361 vf_loss = 0.2958 grad_norm = 0.6507 nat_grad_norm = 0.4903 cg_residual = 0.0385 step_size = 0.3789 reward = 0.0000 fps = 9 mse_loss = 3.4832 
2022-07-08 10:25:32.220752 - gail/main.py:174 - [TRPO] iter = 138000 dist_mean = 0.3978 dist_std = 0.7346 vf_loss = 0.3234 grad_norm = 0.6149 nat_grad_norm = 0.5273 cg_residual = 0.0545 step_size = 0.3544 reward = -0.0000 fps = 7 mse_loss = 3.4946 
2022-07-08 10:26:00.634405 - gail/main.py:174 - [TRPO] iter = 139000 dist_mean = 0.3900 dist_std = 0.7294 vf_loss = 0.2751 grad_norm = 0.7663 nat_grad_norm = 0.4320 cg_residual = 0.0405 step_size = 0.3714 reward = -0.0000 fps = 6 mse_loss = 3.4100 
2022-07-08 10:26:30.938650 - gail/main.py:174 - [TRPO] iter = 140000 dist_mean = 0.4102 dist_std = 0.7311 vf_loss = 0.4347 grad_norm = 0.8092 nat_grad_norm = 0.5406 cg_residual = 0.0415 step_size = 0.3170 reward = -0.0000 fps = 5 mse_loss = 3.3797 
2022-07-08 10:26:31.767503 - gail/main.py:201 - [Discriminator] iter = 140000 loss = -4.3168 grad_norm = 2.8953 grad_penalty = 0.4125 regularization = 0.0000 true_logits = 0.1486 fake_logits = -4.5807 true_prob = 0.5481 fake_prob = 0.0125 
2022-07-08 10:27:18.002594 - gail/main.py:142 - [Evaluate] iter = 140000 episode={ returns = 265.0081 lengths = 119 } discounted_episode={ returns = 240.5729 lengths = 116 } 
2022-07-08 10:27:47.184329 - gail/main.py:174 - [TRPO] iter = 141000 dist_mean = 0.3942 dist_std = 0.7324 vf_loss = 0.5038 grad_norm = 0.7941 nat_grad_norm = 0.5449 cg_residual = 0.0577 step_size = 0.3236 reward = -0.0000 fps = 13 mse_loss = 3.1873 
2022-07-08 10:28:16.720956 - gail/main.py:174 - [TRPO] iter = 142000 dist_mean = 0.3999 dist_std = 0.7302 vf_loss = 0.3353 grad_norm = 0.8008 nat_grad_norm = 0.4720 cg_residual = 0.0662 step_size = 0.3828 reward = -0.0000 fps = 9 mse_loss = 3.3712 
2022-07-08 10:28:46.051968 - gail/main.py:174 - [TRPO] iter = 143000 dist_mean = 0.4027 dist_std = 0.7258 vf_loss = 0.3406 grad_norm = 0.6720 nat_grad_norm = 0.4392 cg_residual = 0.0633 step_size = 0.3773 reward = -0.0000 fps = 7 mse_loss = 3.4605 
2022-07-08 10:29:15.655201 - gail/main.py:174 - [TRPO] iter = 144000 dist_mean = 0.3800 dist_std = 0.7228 vf_loss = 0.3535 grad_norm = 0.8026 nat_grad_norm = 0.4971 cg_residual = 0.0581 step_size = 0.3387 reward = 0.0000 fps = 6 mse_loss = 3.4743 
2022-07-08 10:29:44.788535 - gail/main.py:174 - [TRPO] iter = 145000 dist_mean = 0.4002 dist_std = 0.7207 vf_loss = 0.3699 grad_norm = 0.6419 nat_grad_norm = 0.5187 cg_residual = 0.0369 step_size = 0.3678 reward = 0.0000 fps = 5 mse_loss = 3.5405 
2022-07-08 10:29:45.724396 - gail/main.py:201 - [Discriminator] iter = 145000 loss = -4.2023 grad_norm = 3.5261 grad_penalty = 0.3986 regularization = 0.0000 true_logits = 0.1619 fake_logits = -4.4390 true_prob = 0.5539 fake_prob = 0.0145 
2022-07-08 10:30:32.125097 - gail/main.py:142 - [Evaluate] iter = 145000 episode={ returns = 258.3394 lengths = 115 } discounted_episode={ returns = 232.9598 lengths = 112 } 
2022-07-08 10:31:02.566850 - gail/main.py:174 - [TRPO] iter = 146000 dist_mean = 0.3993 dist_std = 0.7199 vf_loss = 0.2587 grad_norm = 0.6647 nat_grad_norm = 0.4691 cg_residual = 0.0722 step_size = 0.3751 reward = -0.0000 fps = 13 mse_loss = 3.6716 
2022-07-08 10:31:39.105641 - gail/main.py:174 - [TRPO] iter = 147000 dist_mean = 0.4148 dist_std = 0.7189 vf_loss = 0.3424 grad_norm = 0.6625 nat_grad_norm = 0.4796 cg_residual = 0.0599 step_size = 0.3921 reward = -0.0000 fps = 8 mse_loss = 3.7590 
