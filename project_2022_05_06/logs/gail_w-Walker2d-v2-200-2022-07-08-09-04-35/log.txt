2022-07-08 09:04:35.754056 - utils/flags.py:257 - log_dir = logs/gail_w-Walker2d-v2-200-2022-07-08-09-04-35
2022-07-08 09:04:53.918543 - gail/utils/replay_buffer.py:86 - Load dataset from /content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/Walker2d-v2
2022-07-08 09:05:11.668703 - gail/main.py:80 - Expert Reward 5150.674112
2022-07-08 09:05:12.592322 - gail/main.py:84 - Original dataset size 3000
2022-07-08 09:05:12.742402 - gail/main.py:86 - Subsampled dataset size 3000
2022-07-08 09:05:12.753373 - gail/main.py:87 - np random: 864 random : 786
2022-07-08 09:05:12.770841 - gail/main.py:91 - Sampled obs: 0.0531, acs: 0.2269
2022-07-08 09:05:15.950770 - lunzi/nn/flat_param.py:17 - Enabling flattening... ['GaussianMLPPolicy_1/log_std:0', 'GaussianMLPPolicy_1/Variable:0', 'GaussianMLPPolicy_1/Variable_1:0', 'GaussianMLPPolicy_1/Variable_2:0', 'GaussianMLPPolicy_1/Variable_3:0', 'GaussianMLPPolicy_1/Variable_4:0', 'GaussianMLPPolicy_1/Variable_5:0']
2022-07-08 09:05:40.223054 - gail/discriminator/discriminator.py:30 - Use predefined normalization.
2022-07-08 09:05:40.247095 - gail/discriminator/discriminator.py:33 - Normalizer loc:[[ 1.2194959e+00  2.4445076e-01 -7.8132987e-02 -2.6673764e-01
   1.8222688e-01 -9.5077172e-02 -3.3649772e-01  5.3370733e-02
   4.1614923e+00  4.1431887e-03  3.8142569e-02 -2.6013174e-03
  -1.0202496e-02  5.6982285e-01  2.9836079e-02 -1.5763690e-01
   1.7689442e-02]] 
 scale:[[0.06687175 0.23681822 0.23042987 0.33821535 0.664349   0.20301929
  0.42807332 0.7138035  0.986894   0.65049744 2.0363257  2.3816926
  3.7250905  6.026913   2.0511289  4.406521   6.1475325 ]]
2022-07-08 09:05:51.459181 - gail/discriminator/discriminator.py:67 - Discriminator uses Wasserstein distance.
2022-07-08 09:05:51.463145 - gail/discriminator/discriminator.py:68 - [<tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable:0' shape=(23, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_1:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_2:0' shape=(100, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_3:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_4:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_5:0' shape=(1,) dtype=float32_ref>]
2022-07-08 09:05:51.465233 - gail/discriminator/discriminator.py:69 - Use gradient penalty regularization (coef = 10.000000)
2022-07-08 09:05:53.921214 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions_mean]
2022-07-08 09:06:28.937845 - gail/main.py:142 - [Evaluate] iter = 0 episode={ returns = 113.4058 lengths = 135 } discounted_episode={ returns = 135.8490 lengths = 156 } 
2022-07-08 09:06:28.971048 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions]
2022-07-08 09:06:53.835799 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [fake_states fake_actions] => [scaled_neural_reward]
2022-07-08 09:06:54.568572 - lunzi/nn/module.py:71 - [MLPVFunction] is making TensorFlow callables, key = [states] => [values]
2022-07-08 09:06:55.823787 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [] => [sync_old]
2022-07-08 09:06:56.435851 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss flat_grad dist_std mean_kl dist_mean]
2022-07-08 09:07:00.520157 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states tangents actions] => [hessian_vec_prod]
2022-07-08 09:07:07.414734 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [] => [get_flat]
2022-07-08 09:07:08.072208 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [feed_flat] => [set_flat]
2022-07-08 09:07:08.817818 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss mean_kl]
2022-07-08 09:07:10.051191 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [train_vf vf_loss]
2022-07-08 09:07:12.002728 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [vf_loss]
2022-07-08 09:07:12.667359 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states actions_] => [mse_loss]
2022-07-08 09:07:13.340239 - gail/main.py:174 - [TRPO] iter = 1000 dist_mean = 0.0000 dist_std = 1.0000 vf_loss = 0.2127 grad_norm = 0.3176 nat_grad_norm = 0.3986 cg_residual = 0.0000 step_size = 0.5038 reward = 0.0000 fps = 12 mse_loss = 0.4380 
2022-07-08 09:07:30.333138 - gail/main.py:174 - [TRPO] iter = 2000 dist_mean = -0.0148 dist_std = 1.0010 vf_loss = 0.3019 grad_norm = 0.5345 nat_grad_norm = 0.4888 cg_residual = 0.0000 step_size = 0.3417 reward = -0.0000 fps = 10 mse_loss = 0.4759 
2022-07-08 09:07:48.441283 - gail/main.py:174 - [TRPO] iter = 3000 dist_mean = -0.0348 dist_std = 1.0057 vf_loss = 0.1878 grad_norm = 0.5650 nat_grad_norm = 0.4909 cg_residual = 0.0000 step_size = 0.3328 reward = -0.0000 fps = 8 mse_loss = 0.5481 
2022-07-08 09:08:06.300820 - gail/main.py:174 - [TRPO] iter = 4000 dist_mean = -0.0429 dist_std = 1.0060 vf_loss = 0.1870 grad_norm = 0.4669 nat_grad_norm = 0.5075 cg_residual = 0.0000 step_size = 0.3714 reward = 0.0000 fps = 7 mse_loss = 0.6285 
2022-07-08 09:08:25.354952 - gail/main.py:174 - [TRPO] iter = 5000 dist_mean = -0.0506 dist_std = 1.0078 vf_loss = 0.1613 grad_norm = 0.5281 nat_grad_norm = 0.4704 cg_residual = 0.0001 step_size = 0.3728 reward = -0.0000 fps = 6 mse_loss = 0.6875 
2022-07-08 09:08:25.356932 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [true_states true_actions fake_states fake_actions true_masks] => [train loss true_logits fake_logits true_prob fake_prob grad_norm grad_penalty regularization]
2022-07-08 09:08:30.840714 - gail/main.py:201 - [Discriminator] iter = 5000 loss = 1.4121 grad_norm = 9.8276 grad_penalty = 1.0169 regularization = 0.0000 true_logits = 0.3014 fake_logits = 0.6966 true_prob = 0.5739 fake_prob = 0.6630 
2022-07-08 09:08:37.263130 - gail/main.py:142 - [Evaluate] iter = 5000 episode={ returns = 0.5522 lengths = 26 } discounted_episode={ returns = 0.2826 lengths = 26 } 
2022-07-08 09:08:54.969053 - gail/main.py:174 - [TRPO] iter = 6000 dist_mean = -0.0595 dist_std = 1.0085 vf_loss = 0.3100 grad_norm = 0.4358 nat_grad_norm = 0.4758 cg_residual = 0.0001 step_size = 0.4007 reward = 0.0000 fps = 41 mse_loss = 0.6795 
2022-07-08 09:09:18.227226 - gail/main.py:174 - [TRPO] iter = 7000 dist_mean = -0.0571 dist_std = 1.0104 vf_loss = 0.4002 grad_norm = 0.4682 nat_grad_norm = 0.4719 cg_residual = 0.0001 step_size = 0.3677 reward = -0.0000 fps = 21 mse_loss = 0.6821 
2022-07-08 09:09:43.742771 - gail/main.py:174 - [TRPO] iter = 8000 dist_mean = -0.0578 dist_std = 1.0098 vf_loss = 0.3454 grad_norm = 0.4779 nat_grad_norm = 0.4845 cg_residual = 0.0002 step_size = 0.3702 reward = 0.0000 fps = 13 mse_loss = 0.6815 
2022-07-08 09:10:07.897279 - gail/main.py:174 - [TRPO] iter = 9000 dist_mean = -0.0432 dist_std = 1.0079 vf_loss = 0.3692 grad_norm = 0.4299 nat_grad_norm = 0.4747 cg_residual = 0.0002 step_size = 0.4066 reward = 0.0000 fps = 10 mse_loss = 0.6830 
2022-07-08 09:10:37.270349 - gail/main.py:174 - [TRPO] iter = 10000 dist_mean = -0.0477 dist_std = 1.0020 vf_loss = 0.3104 grad_norm = 0.5218 nat_grad_norm = 0.4771 cg_residual = 0.0001 step_size = 0.3738 reward = -0.0000 fps = 7 mse_loss = 0.7671 
2022-07-08 09:10:38.082093 - gail/main.py:201 - [Discriminator] iter = 10000 loss = 0.7476 grad_norm = 7.4934 grad_penalty = 0.6376 regularization = 0.0000 true_logits = 0.3380 fake_logits = 0.4480 true_prob = 0.5823 fake_prob = 0.6059 
2022-07-08 09:10:48.058420 - gail/main.py:142 - [Evaluate] iter = 10000 episode={ returns = -0.5345 lengths = 24 } discounted_episode={ returns = -0.7446 lengths = 24 } 
2022-07-08 09:11:23.202851 - gail/main.py:174 - [TRPO] iter = 11000 dist_mean = -0.0247 dist_std = 1.0060 vf_loss = 0.6413 grad_norm = 0.4589 nat_grad_norm = 0.5094 cg_residual = 0.0001 step_size = 0.3757 reward = -0.0000 fps = 22 mse_loss = 0.7846 
2022-07-08 09:11:51.198867 - gail/main.py:174 - [TRPO] iter = 12000 dist_mean = -0.0201 dist_std = 1.0071 vf_loss = 0.6034 grad_norm = 0.4672 nat_grad_norm = 0.5023 cg_residual = 0.0002 step_size = 0.3789 reward = -0.0000 fps = 13 mse_loss = 0.8191 
2022-07-08 09:12:20.933441 - gail/main.py:174 - [TRPO] iter = 13000 dist_mean = -0.0128 dist_std = 1.0010 vf_loss = 0.4867 grad_norm = 0.4440 nat_grad_norm = 0.5169 cg_residual = 0.0001 step_size = 0.3828 reward = -0.0000 fps = 9 mse_loss = 0.8075 
2022-07-08 09:12:53.382214 - gail/main.py:174 - [TRPO] iter = 14000 dist_mean = 0.0193 dist_std = 0.9993 vf_loss = 0.5215 grad_norm = 0.4364 nat_grad_norm = 0.5285 cg_residual = 0.0002 step_size = 0.4072 reward = 0.0000 fps = 7 mse_loss = 0.7674 
2022-07-08 09:13:23.252641 - gail/main.py:174 - [TRPO] iter = 15000 dist_mean = 0.0343 dist_std = 0.9990 vf_loss = 0.4964 grad_norm = 0.3391 nat_grad_norm = 0.6331 cg_residual = 0.0004 step_size = 0.3883 reward = 0.0000 fps = 6 mse_loss = 0.7814 
2022-07-08 09:13:24.139666 - gail/main.py:201 - [Discriminator] iter = 15000 loss = 0.4732 grad_norm = 6.6826 grad_penalty = 0.5586 regularization = 0.0000 true_logits = 0.3724 fake_logits = 0.2870 true_prob = 0.5901 fake_prob = 0.5679 
2022-07-08 09:13:45.927675 - gail/main.py:142 - [Evaluate] iter = 15000 episode={ returns = -6.3059 lengths = 53 } discounted_episode={ returns = -6.1579 lengths = 53 } 
2022-07-08 09:14:15.455914 - gail/main.py:174 - [TRPO] iter = 16000 dist_mean = 0.0207 dist_std = 0.9953 vf_loss = 0.5158 grad_norm = 0.4636 nat_grad_norm = 0.6354 cg_residual = 0.0003 step_size = 0.3420 reward = 0.0000 fps = 19 mse_loss = 0.8440 
2022-07-08 09:14:46.448438 - gail/main.py:174 - [TRPO] iter = 17000 dist_mean = 0.0293 dist_std = 0.9932 vf_loss = 0.4927 grad_norm = 0.4326 nat_grad_norm = 0.5414 cg_residual = 0.0003 step_size = 0.4098 reward = 0.0000 fps = 12 mse_loss = 0.8344 
2022-07-08 09:15:17.304408 - gail/main.py:174 - [TRPO] iter = 18000 dist_mean = 0.0529 dist_std = 0.9890 vf_loss = 0.5382 grad_norm = 0.4347 nat_grad_norm = 0.5970 cg_residual = 0.0004 step_size = 0.3815 reward = -0.0000 fps = 8 mse_loss = 0.8124 
2022-07-08 09:15:47.113328 - gail/main.py:174 - [TRPO] iter = 19000 dist_mean = 0.0611 dist_std = 0.9899 vf_loss = 0.3538 grad_norm = 0.3922 nat_grad_norm = 0.5491 cg_residual = 0.0003 step_size = 0.4062 reward = -0.0000 fps = 6 mse_loss = 0.8226 
2022-07-08 09:16:17.339750 - gail/main.py:174 - [TRPO] iter = 20000 dist_mean = 0.0638 dist_std = 0.9921 vf_loss = 0.3143 grad_norm = 0.4093 nat_grad_norm = 0.6924 cg_residual = 0.0009 step_size = 0.3587 reward = 0.0000 fps = 5 mse_loss = 0.8542 
2022-07-08 09:16:18.196037 - gail/main.py:201 - [Discriminator] iter = 20000 loss = -0.0558 grad_norm = 6.1154 grad_penalty = 0.4229 regularization = 0.0000 true_logits = 0.4090 fake_logits = -0.0698 true_prob = 0.5979 fake_prob = 0.4851 
2022-07-08 09:16:35.840191 - gail/main.py:142 - [Evaluate] iter = 20000 episode={ returns = -4.0447 lengths = 45 } discounted_episode={ returns = -4.3344 lengths = 46 } 
2022-07-08 09:17:06.334975 - gail/main.py:174 - [TRPO] iter = 21000 dist_mean = 0.0607 dist_std = 0.9961 vf_loss = 0.5308 grad_norm = 0.3524 nat_grad_norm = 0.5610 cg_residual = 0.0004 step_size = 0.4229 reward = 0.0000 fps = 20 mse_loss = 0.8047 
2022-07-08 09:17:35.804312 - gail/main.py:174 - [TRPO] iter = 22000 dist_mean = 0.0775 dist_std = 0.9976 vf_loss = 0.3949 grad_norm = 0.3932 nat_grad_norm = 0.8176 cg_residual = 0.0008 step_size = 0.3378 reward = -0.0000 fps = 12 mse_loss = 0.8143 
2022-07-08 09:18:04.912513 - gail/main.py:174 - [TRPO] iter = 23000 dist_mean = 0.0764 dist_std = 0.9965 vf_loss = 0.3121 grad_norm = 0.4510 nat_grad_norm = 0.6182 cg_residual = 0.0007 step_size = 0.3569 reward = -0.0000 fps = 9 mse_loss = 0.7806 
2022-07-08 09:18:32.740786 - gail/main.py:174 - [TRPO] iter = 24000 dist_mean = 0.0641 dist_std = 1.0050 vf_loss = 0.2684 grad_norm = 0.3781 nat_grad_norm = 0.5631 cg_residual = 0.0005 step_size = 0.4362 reward = 0.0000 fps = 7 mse_loss = 0.7748 
2022-07-08 09:19:02.192872 - gail/main.py:174 - [TRPO] iter = 25000 dist_mean = 0.0575 dist_std = 1.0038 vf_loss = 0.3993 grad_norm = 0.4084 nat_grad_norm = 0.5325 cg_residual = 0.0003 step_size = 0.4159 reward = 0.0000 fps = 6 mse_loss = 0.7954 
2022-07-08 09:19:02.970728 - gail/main.py:201 - [Discriminator] iter = 25000 loss = -0.6814 grad_norm = 6.0971 grad_penalty = 0.4262 regularization = 0.0000 true_logits = 0.4051 fake_logits = -0.7025 true_prob = 0.5973 fake_prob = 0.3458 
2022-07-08 09:19:20.877072 - gail/main.py:142 - [Evaluate] iter = 25000 episode={ returns = -4.1848 lengths = 46 } discounted_episode={ returns = -4.0676 lengths = 46 } 
2022-07-08 09:19:50.075231 - gail/main.py:174 - [TRPO] iter = 26000 dist_mean = 0.0708 dist_std = 0.9952 vf_loss = 0.2992 grad_norm = 0.3350 nat_grad_norm = 0.5684 cg_residual = 0.0006 step_size = 0.4386 reward = -0.0000 fps = 21 mse_loss = 0.8095 
2022-07-08 09:20:19.068986 - gail/main.py:174 - [TRPO] iter = 27000 dist_mean = 0.0808 dist_std = 0.9876 vf_loss = 0.6416 grad_norm = 0.5263 nat_grad_norm = 0.6126 cg_residual = 0.0004 step_size = 0.3557 reward = 0.0000 fps = 13 mse_loss = 0.8210 
2022-07-08 09:20:47.756698 - gail/main.py:174 - [TRPO] iter = 28000 dist_mean = 0.0750 dist_std = 0.9846 vf_loss = 0.4300 grad_norm = 0.4662 nat_grad_norm = 0.5532 cg_residual = 0.0005 step_size = 0.3959 reward = 0.0000 fps = 9 mse_loss = 0.8766 
2022-07-08 09:21:15.054182 - gail/main.py:174 - [TRPO] iter = 29000 dist_mean = 0.0496 dist_std = 0.9820 vf_loss = 0.2634 grad_norm = 0.3508 nat_grad_norm = 0.5377 cg_residual = 0.0004 step_size = 0.4324 reward = -0.0000 fps = 7 mse_loss = 0.8969 
2022-07-08 09:21:43.224504 - gail/main.py:174 - [TRPO] iter = 30000 dist_mean = 0.0837 dist_std = 0.9816 vf_loss = 0.6232 grad_norm = 0.4092 nat_grad_norm = 0.5376 cg_residual = 0.0006 step_size = 0.4242 reward = -0.0000 fps = 6 mse_loss = 0.9464 
2022-07-08 09:21:44.052075 - gail/main.py:201 - [Discriminator] iter = 30000 loss = -1.2253 grad_norm = 6.1716 grad_penalty = 0.3971 regularization = 0.0000 true_logits = 0.4149 fake_logits = -1.2074 true_prob = 0.5997 fake_prob = 0.2488 
2022-07-08 09:22:07.276870 - gail/main.py:142 - [Evaluate] iter = 30000 episode={ returns = -5.3251 lengths = 43 } discounted_episode={ returns = -4.8255 lengths = 43 } 
2022-07-08 09:22:36.386476 - gail/main.py:174 - [TRPO] iter = 31000 dist_mean = 0.0808 dist_std = 0.9832 vf_loss = 0.4438 grad_norm = 0.4137 nat_grad_norm = 0.6252 cg_residual = 0.0008 step_size = 0.3814 reward = -0.0000 fps = 19 mse_loss = 1.0183 
2022-07-08 09:23:04.761651 - gail/main.py:174 - [TRPO] iter = 32000 dist_mean = 0.0765 dist_std = 0.9853 vf_loss = 1.2024 grad_norm = 0.4307 nat_grad_norm = 0.6430 cg_residual = 0.0011 step_size = 0.3620 reward = -0.0000 fps = 12 mse_loss = 0.9503 
2022-07-08 09:23:33.801670 - gail/main.py:174 - [TRPO] iter = 33000 dist_mean = 0.0851 dist_std = 0.9777 vf_loss = 0.3926 grad_norm = 0.3696 nat_grad_norm = 0.5513 cg_residual = 0.0006 step_size = 0.4381 reward = -0.0000 fps = 9 mse_loss = 0.9473 
2022-07-08 09:24:02.368588 - gail/main.py:174 - [TRPO] iter = 34000 dist_mean = 0.0902 dist_std = 0.9760 vf_loss = 0.7552 grad_norm = 0.4280 nat_grad_norm = 0.5816 cg_residual = 0.0012 step_size = 0.3750 reward = 0.0000 fps = 7 mse_loss = 1.0080 
2022-07-08 09:24:29.492720 - gail/main.py:174 - [TRPO] iter = 35000 dist_mean = 0.0716 dist_std = 0.9777 vf_loss = 0.5485 grad_norm = 0.3812 nat_grad_norm = 0.6887 cg_residual = 0.0008 step_size = 0.3734 reward = -0.0000 fps = 6 mse_loss = 0.9314 
2022-07-08 09:24:30.294138 - gail/main.py:201 - [Discriminator] iter = 35000 loss = -1.9746 grad_norm = 6.8495 grad_penalty = 0.4146 regularization = 0.0000 true_logits = 0.4951 fake_logits = -1.8941 true_prob = 0.6183 fake_prob = 0.1569 
2022-07-08 09:24:56.919203 - gail/main.py:142 - [Evaluate] iter = 35000 episode={ returns = 1.0737 lengths = 75 } discounted_episode={ returns = 0.4192 lengths = 76 } 
2022-07-08 09:25:24.385299 - gail/main.py:174 - [TRPO] iter = 36000 dist_mean = 0.0841 dist_std = 0.9776 vf_loss = 0.5749 grad_norm = 0.4295 nat_grad_norm = 0.5811 cg_residual = 0.0007 step_size = 0.3955 reward = -0.0000 fps = 18 mse_loss = 0.9696 
2022-07-08 09:25:52.267901 - gail/main.py:174 - [TRPO] iter = 37000 dist_mean = 0.0804 dist_std = 0.9828 vf_loss = 0.4274 grad_norm = 0.3948 nat_grad_norm = 0.5591 cg_residual = 0.0012 step_size = 0.4050 reward = -0.0000 fps = 12 mse_loss = 1.0068 
2022-07-08 09:26:21.357417 - gail/main.py:174 - [TRPO] iter = 38000 dist_mean = 0.1322 dist_std = 0.9731 vf_loss = 0.9000 grad_norm = 0.3682 nat_grad_norm = 0.5828 cg_residual = 0.0012 step_size = 0.4010 reward = -0.0000 fps = 9 mse_loss = 0.9440 
2022-07-08 09:26:49.595459 - gail/main.py:174 - [TRPO] iter = 39000 dist_mean = 0.1124 dist_std = 0.9731 vf_loss = 0.7508 grad_norm = 0.3424 nat_grad_norm = 0.6417 cg_residual = 0.0012 step_size = 0.4292 reward = -0.0000 fps = 7 mse_loss = 0.9167 
2022-07-08 09:27:16.632462 - gail/main.py:174 - [TRPO] iter = 40000 dist_mean = 0.0933 dist_std = 0.9726 vf_loss = 0.4449 grad_norm = 0.4652 nat_grad_norm = 0.6734 cg_residual = 0.0010 step_size = 0.3512 reward = -0.0000 fps = 6 mse_loss = 0.8471 
2022-07-08 09:27:17.353955 - gail/main.py:201 - [Discriminator] iter = 40000 loss = -2.5910 grad_norm = 7.0262 grad_penalty = 0.4180 regularization = 0.0000 true_logits = 0.5172 fake_logits = -2.4918 true_prob = 0.6239 fake_prob = 0.1018 
2022-07-08 09:27:57.526102 - gail/main.py:142 - [Evaluate] iter = 40000 episode={ returns = 13.6172 lengths = 103 } discounted_episode={ returns = 10.1206 lengths = 116 } 
2022-07-08 09:28:23.463837 - gail/main.py:174 - [TRPO] iter = 41000 dist_mean = 0.0945 dist_std = 0.9731 vf_loss = 0.3788 grad_norm = 0.3621 nat_grad_norm = 0.5873 cg_residual = 0.0005 step_size = 0.4280 reward = 0.0000 fps = 15 mse_loss = 0.8816 
2022-07-08 09:28:51.110293 - gail/main.py:174 - [TRPO] iter = 42000 dist_mean = 0.1059 dist_std = 0.9704 vf_loss = 0.6891 grad_norm = 0.4261 nat_grad_norm = 0.5634 cg_residual = 0.0013 step_size = 0.3911 reward = 0.0000 fps = 10 mse_loss = 0.8912 
2022-07-08 09:29:18.589341 - gail/main.py:174 - [TRPO] iter = 43000 dist_mean = 0.1876 dist_std = 0.9687 vf_loss = 0.4119 grad_norm = 0.4199 nat_grad_norm = 0.5722 cg_residual = 0.0041 step_size = 0.4090 reward = 0.0000 fps = 8 mse_loss = 0.8855 
2022-07-08 09:29:45.758969 - gail/main.py:174 - [TRPO] iter = 44000 dist_mean = 0.1457 dist_std = 0.9708 vf_loss = 0.6276 grad_norm = 0.4092 nat_grad_norm = 0.6689 cg_residual = 0.0035 step_size = 0.3674 reward = 0.0000 fps = 6 mse_loss = 0.8967 
2022-07-08 09:30:12.641374 - gail/main.py:174 - [TRPO] iter = 45000 dist_mean = 0.2067 dist_std = 0.9714 vf_loss = 0.2892 grad_norm = 0.5024 nat_grad_norm = 0.6049 cg_residual = 0.0069 step_size = 0.3422 reward = -0.0000 fps = 5 mse_loss = 0.8880 
2022-07-08 09:30:13.503184 - gail/main.py:201 - [Discriminator] iter = 45000 loss = -2.9693 grad_norm = 6.8531 grad_penalty = 0.4071 regularization = 0.0000 true_logits = 0.5964 fake_logits = -2.7800 true_prob = 0.6416 fake_prob = 0.0801 
2022-07-08 09:30:57.927009 - gail/main.py:142 - [Evaluate] iter = 45000 episode={ returns = 209.4827 lengths = 120 } discounted_episode={ returns = 194.7423 lengths = 122 } 
2022-07-08 09:31:25.475152 - gail/main.py:174 - [TRPO] iter = 46000 dist_mean = 0.1635 dist_std = 0.9728 vf_loss = 0.8451 grad_norm = 0.5590 nat_grad_norm = 0.5555 cg_residual = 0.0027 step_size = 0.3676 reward = -0.0000 fps = 13 mse_loss = 0.8987 
2022-07-08 09:31:52.862927 - gail/main.py:174 - [TRPO] iter = 47000 dist_mean = 0.1606 dist_std = 0.9724 vf_loss = 0.5208 grad_norm = 0.4000 nat_grad_norm = 0.6898 cg_residual = 0.0033 step_size = 0.3670 reward = 0.0000 fps = 10 mse_loss = 0.8770 
2022-07-08 09:32:26.054732 - gail/main.py:174 - [TRPO] iter = 48000 dist_mean = 0.1537 dist_std = 0.9699 vf_loss = 0.4611 grad_norm = 0.4212 nat_grad_norm = 0.5260 cg_residual = 0.0029 step_size = 0.4189 reward = 0.0000 fps = 7 mse_loss = 0.9336 
2022-07-08 09:32:52.238386 - gail/main.py:174 - [TRPO] iter = 49000 dist_mean = 0.1795 dist_std = 0.9704 vf_loss = 0.3398 grad_norm = 0.4876 nat_grad_norm = 0.5540 cg_residual = 0.0050 step_size = 0.3738 reward = 0.0000 fps = 6 mse_loss = 0.9282 
2022-07-08 09:33:19.273196 - gail/main.py:174 - [TRPO] iter = 50000 dist_mean = 0.1712 dist_std = 0.9718 vf_loss = 0.3391 grad_norm = 0.5145 nat_grad_norm = 0.6019 cg_residual = 0.0042 step_size = 0.3648 reward = 0.0000 fps = 5 mse_loss = 0.8871 
2022-07-08 09:33:20.041378 - gail/main.py:201 - [Discriminator] iter = 50000 loss = -3.3414 grad_norm = 7.1683 grad_penalty = 0.4685 regularization = 0.0000 true_logits = 0.5944 fake_logits = -3.2155 true_prob = 0.6391 fake_prob = 0.0590 
2022-07-08 09:34:31.990717 - gail/main.py:142 - [Evaluate] iter = 50000 episode={ returns = 301.5934 lengths = 205 } discounted_episode={ returns = 261.6042 lengths = 201 } 
2022-07-08 09:34:58.367049 - gail/main.py:174 - [TRPO] iter = 51000 dist_mean = 0.1593 dist_std = 0.9666 vf_loss = 0.3671 grad_norm = 0.4422 nat_grad_norm = 0.6572 cg_residual = 0.0031 step_size = 0.3754 reward = 0.0000 fps = 10 mse_loss = 0.9385 
2022-07-08 09:35:24.009579 - gail/main.py:174 - [TRPO] iter = 52000 dist_mean = 0.2076 dist_std = 0.9737 vf_loss = 0.3936 grad_norm = 0.4636 nat_grad_norm = 0.6035 cg_residual = 0.0039 step_size = 0.3853 reward = 0.0000 fps = 8 mse_loss = 0.9047 
2022-07-08 09:35:50.416671 - gail/main.py:174 - [TRPO] iter = 53000 dist_mean = 0.1928 dist_std = 0.9747 vf_loss = 0.2557 grad_norm = 0.4766 nat_grad_norm = 0.5652 cg_residual = 0.0040 step_size = 0.3880 reward = -0.0000 fps = 6 mse_loss = 0.8822 
2022-07-08 09:36:16.745479 - gail/main.py:174 - [TRPO] iter = 54000 dist_mean = 0.1745 dist_std = 0.9800 vf_loss = 0.4214 grad_norm = 0.4095 nat_grad_norm = 0.6188 cg_residual = 0.0025 step_size = 0.4011 reward = 0.0000 fps = 5 mse_loss = 0.9565 
2022-07-08 09:36:42.632136 - gail/main.py:174 - [TRPO] iter = 55000 dist_mean = 0.1637 dist_std = 0.9765 vf_loss = 0.4069 grad_norm = 0.4406 nat_grad_norm = 0.5961 cg_residual = 0.0020 step_size = 0.4192 reward = -0.0000 fps = 4 mse_loss = 0.8732 
2022-07-08 09:36:43.473546 - gail/main.py:201 - [Discriminator] iter = 55000 loss = -3.8434 grad_norm = 7.2907 grad_penalty = 0.4936 regularization = 0.0000 true_logits = 0.6312 fake_logits = -3.7058 true_prob = 0.6472 fake_prob = 0.0401 
2022-07-08 09:40:37.426806 - gail/main.py:142 - [Evaluate] iter = 55000 episode={ returns = 633.8180 lengths = 583 } discounted_episode={ returns = 493.0809 lengths = 696 } 
2022-07-08 09:41:02.599490 - gail/main.py:174 - [TRPO] iter = 56000 dist_mean = 0.2100 dist_std = 0.9743 vf_loss = 0.3729 grad_norm = 0.4018 nat_grad_norm = 0.6548 cg_residual = 0.0026 step_size = 0.3804 reward = 0.0000 fps = 3 mse_loss = 0.9255 
2022-07-08 09:41:28.885737 - gail/main.py:174 - [TRPO] iter = 57000 dist_mean = 0.1941 dist_std = 0.9739 vf_loss = 0.6683 grad_norm = 0.4424 nat_grad_norm = 0.6956 cg_residual = 0.0058 step_size = 0.3824 reward = -0.0000 fps = 3 mse_loss = 0.9132 
2022-07-08 09:41:54.960747 - gail/main.py:174 - [TRPO] iter = 58000 dist_mean = 0.1752 dist_std = 0.9751 vf_loss = 0.3971 grad_norm = 0.4321 nat_grad_norm = 0.5468 cg_residual = 0.0022 step_size = 0.4521 reward = -0.0000 fps = 3 mse_loss = 0.9371 
2022-07-08 09:42:26.962431 - gail/main.py:174 - [TRPO] iter = 59000 dist_mean = 0.2000 dist_std = 0.9722 vf_loss = 0.4621 grad_norm = 0.4532 nat_grad_norm = 0.6619 cg_residual = 0.0046 step_size = 0.3718 reward = 0.0000 fps = 2 mse_loss = 0.8718 
2022-07-08 09:42:52.329779 - gail/main.py:174 - [TRPO] iter = 60000 dist_mean = 0.2191 dist_std = 0.9682 vf_loss = 0.4348 grad_norm = 0.3654 nat_grad_norm = 0.5664 cg_residual = 0.0030 step_size = 0.4346 reward = -0.0000 fps = 2 mse_loss = 0.9349 
2022-07-08 09:42:53.064153 - gail/main.py:201 - [Discriminator] iter = 60000 loss = -4.0519 grad_norm = 6.5735 grad_penalty = 0.5249 regularization = 0.0000 true_logits = 0.5748 fake_logits = -4.0020 true_prob = 0.6346 fake_prob = 0.0343 
2022-07-08 09:48:44.796763 - gail/main.py:142 - [Evaluate] iter = 60000 episode={ returns = 979.3089 lengths = 1000 } discounted_episode={ returns = 610.1385 lengths = 1000 } 
2022-07-08 09:49:09.885038 - gail/main.py:174 - [TRPO] iter = 61000 dist_mean = 0.1761 dist_std = 0.9647 vf_loss = 0.5222 grad_norm = 0.5043 nat_grad_norm = 0.6761 cg_residual = 0.0035 step_size = 0.3571 reward = -0.0000 fps = 2 mse_loss = 0.9076 
2022-07-08 09:49:35.500641 - gail/main.py:174 - [TRPO] iter = 62000 dist_mean = 0.2361 dist_std = 0.9679 vf_loss = 0.5194 grad_norm = 0.5148 nat_grad_norm = 0.6267 cg_residual = 0.0051 step_size = 0.3622 reward = -0.0000 fps = 2 mse_loss = 0.8254 
2022-07-08 09:50:03.868320 - gail/main.py:174 - [TRPO] iter = 63000 dist_mean = 0.2137 dist_std = 0.9624 vf_loss = 0.4210 grad_norm = 0.5061 nat_grad_norm = 0.6520 cg_residual = 0.0065 step_size = 0.3649 reward = 0.0000 fps = 2 mse_loss = 0.8544 
2022-07-08 09:50:31.148216 - gail/main.py:174 - [TRPO] iter = 64000 dist_mean = 0.2148 dist_std = 0.9613 vf_loss = 0.3843 grad_norm = 0.5410 nat_grad_norm = 0.6142 cg_residual = 0.0064 step_size = 0.3455 reward = -0.0000 fps = 2 mse_loss = 0.7693 
2022-07-08 09:50:55.536661 - gail/main.py:174 - [TRPO] iter = 65000 dist_mean = 0.2037 dist_std = 0.9628 vf_loss = 0.2969 grad_norm = 0.4342 nat_grad_norm = 0.6171 cg_residual = 0.0048 step_size = 0.4027 reward = 0.0000 fps = 2 mse_loss = 0.8961 
2022-07-08 09:50:56.243552 - gail/main.py:201 - [Discriminator] iter = 65000 loss = -4.0902 grad_norm = 9.3328 grad_penalty = 0.5762 regularization = 0.0000 true_logits = 0.4626 fake_logits = -4.2038 true_prob = 0.6126 fake_prob = 0.0317 
2022-07-08 09:53:28.374871 - gail/main.py:142 - [Evaluate] iter = 65000 episode={ returns = 525.1393 lengths = 441 } discounted_episode={ returns = 373.0434 lengths = 379 } 
2022-07-08 09:53:55.004385 - gail/main.py:174 - [TRPO] iter = 66000 dist_mean = 0.2132 dist_std = 0.9567 vf_loss = 0.3874 grad_norm = 0.4576 nat_grad_norm = 0.5785 cg_residual = 0.0056 step_size = 0.3939 reward = 0.0000 fps = 5 mse_loss = 0.8303 
2022-07-08 09:54:21.880214 - gail/main.py:174 - [TRPO] iter = 67000 dist_mean = 0.1999 dist_std = 0.9551 vf_loss = 0.2728 grad_norm = 0.4993 nat_grad_norm = 0.6640 cg_residual = 0.0055 step_size = 0.3528 reward = 0.0000 fps = 4 mse_loss = 0.7501 
2022-07-08 09:54:47.760462 - gail/main.py:174 - [TRPO] iter = 68000 dist_mean = 0.1889 dist_std = 0.9544 vf_loss = 0.2162 grad_norm = 0.4592 nat_grad_norm = 0.7452 cg_residual = 0.0044 step_size = 0.3546 reward = -0.0000 fps = 4 mse_loss = 0.6964 
2022-07-08 09:55:13.274102 - gail/main.py:174 - [TRPO] iter = 69000 dist_mean = 0.1716 dist_std = 0.9499 vf_loss = 0.3977 grad_norm = 0.4715 nat_grad_norm = 0.5468 cg_residual = 0.0059 step_size = 0.3992 reward = 0.0000 fps = 3 mse_loss = 0.6751 
2022-07-08 09:55:37.410056 - gail/main.py:174 - [TRPO] iter = 70000 dist_mean = 0.2031 dist_std = 0.9488 vf_loss = 0.3125 grad_norm = 0.4906 nat_grad_norm = 0.6069 cg_residual = 0.0056 step_size = 0.3886 reward = 0.0000 fps = 3 mse_loss = 0.7365 
2022-07-08 09:55:38.130367 - gail/main.py:201 - [Discriminator] iter = 70000 loss = -4.4120 grad_norm = 6.7568 grad_penalty = 0.5008 regularization = 0.0000 true_logits = 0.3385 fake_logits = -4.5744 true_prob = 0.5859 fake_prob = 0.0239 
2022-07-08 09:56:34.591478 - gail/main.py:142 - [Evaluate] iter = 70000 episode={ returns = 250.9397 lengths = 163 } discounted_episode={ returns = 243.8782 lengths = 182 } 
2022-07-08 09:56:58.895709 - gail/main.py:174 - [TRPO] iter = 71000 dist_mean = 0.1927 dist_std = 0.9423 vf_loss = 0.2177 grad_norm = 0.4573 nat_grad_norm = 0.6651 cg_residual = 0.0070 step_size = 0.3751 reward = -0.0000 fps = 12 mse_loss = 0.7699 
2022-07-08 09:57:22.883842 - gail/main.py:174 - [TRPO] iter = 72000 dist_mean = 0.1739 dist_std = 0.9455 vf_loss = 0.4405 grad_norm = 0.4642 nat_grad_norm = 0.6649 cg_residual = 0.0047 step_size = 0.3534 reward = -0.0000 fps = 9 mse_loss = 0.7685 
2022-07-08 09:57:47.183561 - gail/main.py:174 - [TRPO] iter = 73000 dist_mean = 0.1555 dist_std = 0.9472 vf_loss = 0.4347 grad_norm = 0.4727 nat_grad_norm = 0.6425 cg_residual = 0.0043 step_size = 0.3737 reward = 0.0000 fps = 7 mse_loss = 0.7094 
2022-07-08 09:58:12.014386 - gail/main.py:174 - [TRPO] iter = 74000 dist_mean = 0.1991 dist_std = 0.9428 vf_loss = 0.4380 grad_norm = 0.4853 nat_grad_norm = 0.6254 cg_residual = 0.0064 step_size = 0.3615 reward = 0.0000 fps = 6 mse_loss = 0.8192 
2022-07-08 09:58:37.984767 - gail/main.py:174 - [TRPO] iter = 75000 dist_mean = 0.1780 dist_std = 0.9450 vf_loss = 0.2840 grad_norm = 0.5192 nat_grad_norm = 0.6730 cg_residual = 0.0072 step_size = 0.3445 reward = 0.0000 fps = 5 mse_loss = 0.8180 
2022-07-08 09:58:38.753972 - gail/main.py:201 - [Discriminator] iter = 75000 loss = -4.8045 grad_norm = 5.3498 grad_penalty = 0.4941 regularization = 0.0000 true_logits = 0.2715 fake_logits = -5.0271 true_prob = 0.5718 fake_prob = 0.0175 
2022-07-08 10:01:29.395228 - gail/main.py:142 - [Evaluate] iter = 75000 episode={ returns = 553.9214 lengths = 449 } discounted_episode={ returns = 417.8958 lengths = 477 } 
2022-07-08 10:02:00.138176 - gail/main.py:174 - [TRPO] iter = 76000 dist_mean = 0.1979 dist_std = 0.9407 vf_loss = 0.2521 grad_norm = 0.5584 nat_grad_norm = 0.6514 cg_residual = 0.0062 step_size = 0.3342 reward = 0.0000 fps = 4 mse_loss = 0.8586 
2022-07-08 10:02:33.226383 - gail/main.py:174 - [TRPO] iter = 77000 dist_mean = 0.1781 dist_std = 0.9406 vf_loss = 0.3151 grad_norm = 0.5862 nat_grad_norm = 0.6565 cg_residual = 0.0082 step_size = 0.3398 reward = -0.0000 fps = 4 mse_loss = 0.7765 
2022-07-08 10:03:01.908831 - gail/main.py:174 - [TRPO] iter = 78000 dist_mean = 0.2102 dist_std = 0.9366 vf_loss = 0.1778 grad_norm = 0.4645 nat_grad_norm = 0.6504 cg_residual = 0.0064 step_size = 0.3522 reward = -0.0000 fps = 3 mse_loss = 0.8000 
2022-07-08 10:03:30.851739 - gail/main.py:174 - [TRPO] iter = 79000 dist_mean = 0.2086 dist_std = 0.9337 vf_loss = 0.2037 grad_norm = 0.5338 nat_grad_norm = 0.6206 cg_residual = 0.0099 step_size = 0.3810 reward = -0.0000 fps = 3 mse_loss = 0.8078 
2022-07-08 10:04:00.385608 - gail/main.py:174 - [TRPO] iter = 80000 dist_mean = 0.2211 dist_std = 0.9318 vf_loss = 0.2600 grad_norm = 0.4715 nat_grad_norm = 0.6631 cg_residual = 0.0042 step_size = 0.3966 reward = -0.0000 fps = 3 mse_loss = 0.7453 
2022-07-08 10:04:01.265765 - gail/main.py:201 - [Discriminator] iter = 80000 loss = -4.8310 grad_norm = 6.7165 grad_penalty = 0.5316 regularization = 0.0000 true_logits = 0.0964 fake_logits = -5.2662 true_prob = 0.5397 fake_prob = 0.0148 
2022-07-08 10:06:45.616780 - gail/main.py:142 - [Evaluate] iter = 80000 episode={ returns = 435.2746 lengths = 320 } discounted_episode={ returns = 424.8347 lengths = 501 } 
2022-07-08 10:07:14.944170 - gail/main.py:174 - [TRPO] iter = 81000 dist_mean = 0.2519 dist_std = 0.9349 vf_loss = 0.4089 grad_norm = 0.5868 nat_grad_norm = 0.6152 cg_residual = 0.0149 step_size = 0.3375 reward = -0.0000 fps = 5 mse_loss = 0.7763 
2022-07-08 10:07:42.707835 - gail/main.py:174 - [TRPO] iter = 82000 dist_mean = 0.2213 dist_std = 0.9356 vf_loss = 0.3020 grad_norm = 0.5476 nat_grad_norm = 0.6586 cg_residual = 0.0088 step_size = 0.3461 reward = -0.0000 fps = 4 mse_loss = 0.7142 
2022-07-08 10:08:10.186846 - gail/main.py:174 - [TRPO] iter = 83000 dist_mean = 0.2233 dist_std = 0.9302 vf_loss = 0.2157 grad_norm = 0.4910 nat_grad_norm = 0.6464 cg_residual = 0.0107 step_size = 0.3708 reward = -0.0000 fps = 4 mse_loss = 0.8093 
2022-07-08 10:08:39.046277 - gail/main.py:174 - [TRPO] iter = 84000 dist_mean = 0.2102 dist_std = 0.9230 vf_loss = 0.3436 grad_norm = 0.4723 nat_grad_norm = 0.6516 cg_residual = 0.0054 step_size = 0.3710 reward = 0.0000 fps = 3 mse_loss = 0.7759 
2022-07-08 10:09:08.534180 - gail/main.py:174 - [TRPO] iter = 85000 dist_mean = 0.2200 dist_std = 0.9186 vf_loss = 0.5861 grad_norm = 0.3913 nat_grad_norm = 0.5731 cg_residual = 0.0049 step_size = 0.4188 reward = 0.0000 fps = 3 mse_loss = 0.7747 
2022-07-08 10:09:09.341370 - gail/main.py:201 - [Discriminator] iter = 85000 loss = -5.0019 grad_norm = 6.3925 grad_penalty = 0.5884 regularization = 0.0000 true_logits = 0.0131 fake_logits = -5.5773 true_prob = 0.5265 fake_prob = 0.0115 
2022-07-08 10:16:05.310577 - gail/main.py:142 - [Evaluate] iter = 85000 episode={ returns = 964.6305 lengths = 1000 } discounted_episode={ returns = 597.3228 lengths = 1000 } 
2022-07-08 10:16:33.278363 - gail/main.py:174 - [TRPO] iter = 86000 dist_mean = 0.2342 dist_std = 0.9125 vf_loss = 0.3197 grad_norm = 0.5238 nat_grad_norm = 0.6119 cg_residual = 0.0120 step_size = 0.3649 reward = -0.0000 fps = 2 mse_loss = 0.7758 
2022-07-08 10:17:01.095984 - gail/main.py:174 - [TRPO] iter = 87000 dist_mean = 0.2565 dist_std = 0.9114 vf_loss = 0.3986 grad_norm = 0.4685 nat_grad_norm = 0.5137 cg_residual = 0.0128 step_size = 0.4172 reward = -0.0000 fps = 2 mse_loss = 0.7191 
2022-07-08 10:17:28.844567 - gail/main.py:174 - [TRPO] iter = 88000 dist_mean = 0.2835 dist_std = 0.9108 vf_loss = 0.2307 grad_norm = 0.4718 nat_grad_norm = 0.6354 cg_residual = 0.0147 step_size = 0.3674 reward = -0.0000 fps = 2 mse_loss = 0.7032 
2022-07-08 10:17:57.223138 - gail/main.py:174 - [TRPO] iter = 89000 dist_mean = 0.2432 dist_std = 0.9109 vf_loss = 0.3259 grad_norm = 0.4987 nat_grad_norm = 0.6099 cg_residual = 0.0136 step_size = 0.3722 reward = -0.0000 fps = 1 mse_loss = 0.7979 
2022-07-08 10:18:24.234994 - gail/main.py:174 - [TRPO] iter = 90000 dist_mean = 0.2771 dist_std = 0.9056 vf_loss = 0.3093 grad_norm = 0.6256 nat_grad_norm = 0.5325 cg_residual = 0.0102 step_size = 0.4057 reward = 0.0000 fps = 1 mse_loss = 0.7606 
2022-07-08 10:18:25.022813 - gail/main.py:201 - [Discriminator] iter = 90000 loss = -4.5799 grad_norm = 6.0707 grad_penalty = 0.5947 regularization = 0.0000 true_logits = -0.0861 fake_logits = -5.2606 true_prob = 0.5046 fake_prob = 0.0144 
2022-07-08 10:25:05.630668 - gail/main.py:142 - [Evaluate] iter = 90000 episode={ returns = 966.9305 lengths = 1000 } discounted_episode={ returns = 599.5776 lengths = 1000 } 
2022-07-08 10:25:33.548113 - gail/main.py:174 - [TRPO] iter = 91000 dist_mean = 0.2862 dist_std = 0.9036 vf_loss = 0.3160 grad_norm = 0.6423 nat_grad_norm = 0.6385 cg_residual = 0.0153 step_size = 0.3517 reward = -0.0000 fps = 2 mse_loss = 0.7581 
2022-07-08 10:26:01.796337 - gail/main.py:174 - [TRPO] iter = 92000 dist_mean = 0.2754 dist_std = 0.8988 vf_loss = 0.1974 grad_norm = 0.4964 nat_grad_norm = 0.5876 cg_residual = 0.0189 step_size = 0.3819 reward = -0.0000 fps = 2 mse_loss = 0.7735 
2022-07-08 10:26:31.737260 - gail/main.py:174 - [TRPO] iter = 93000 dist_mean = 0.2496 dist_std = 0.8997 vf_loss = 0.3022 grad_norm = 0.5306 nat_grad_norm = 0.5297 cg_residual = 0.0111 step_size = 0.3620 reward = -0.0000 fps = 2 mse_loss = 0.7471 
2022-07-08 10:27:00.234467 - gail/main.py:174 - [TRPO] iter = 94000 dist_mean = 0.2204 dist_std = 0.8936 vf_loss = 0.2345 grad_norm = 0.5520 nat_grad_norm = 0.5854 cg_residual = 0.0109 step_size = 0.3672 reward = -0.0000 fps = 1 mse_loss = 0.6961 
2022-07-08 10:27:30.139076 - gail/main.py:174 - [TRPO] iter = 95000 dist_mean = 0.2774 dist_std = 0.8916 vf_loss = 0.2108 grad_norm = 0.6066 nat_grad_norm = 0.5889 cg_residual = 0.0197 step_size = 0.3550 reward = -0.0000 fps = 1 mse_loss = 0.7321 
2022-07-08 10:27:31.007730 - gail/main.py:201 - [Discriminator] iter = 95000 loss = -4.8817 grad_norm = 7.3253 grad_penalty = 0.5574 regularization = 0.0000 true_logits = -0.1981 fake_logits = -5.6373 true_prob = 0.4836 fake_prob = 0.0109 
