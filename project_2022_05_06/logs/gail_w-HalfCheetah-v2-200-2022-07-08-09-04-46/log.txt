2022-07-08 09:04:47.216607 - utils/flags.py:257 - log_dir = logs/gail_w-HalfCheetah-v2-200-2022-07-08-09-04-46
2022-07-08 09:05:04.272605 - gail/utils/replay_buffer.py:86 - Load dataset from /content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/HalfCheetah-v2
2022-07-08 09:05:25.268239 - gail/main.py:80 - Expert Reward 6594.485577
2022-07-08 09:05:26.192057 - gail/main.py:84 - Original dataset size 3000
2022-07-08 09:05:26.289850 - gail/main.py:86 - Subsampled dataset size 3000
2022-07-08 09:05:26.294738 - gail/main.py:87 - np random: 864 random : 786
2022-07-08 09:05:26.305056 - gail/main.py:91 - Sampled obs: 0.6422, acs: 0.2570
2022-07-08 09:05:29.093017 - lunzi/nn/flat_param.py:17 - Enabling flattening... ['GaussianMLPPolicy_1/log_std:0', 'GaussianMLPPolicy_1/Variable:0', 'GaussianMLPPolicy_1/Variable_1:0', 'GaussianMLPPolicy_1/Variable_2:0', 'GaussianMLPPolicy_1/Variable_3:0', 'GaussianMLPPolicy_1/Variable_4:0', 'GaussianMLPPolicy_1/Variable_5:0']
2022-07-08 09:05:52.243056 - gail/discriminator/discriminator.py:30 - Use predefined normalization.
2022-07-08 09:05:52.253046 - gail/discriminator/discriminator.py:33 - Normalizer loc:[[-1.0330443e-01 -9.3026832e-03  2.9791275e-01  8.1040427e-02
   1.7489190e-01  4.0859219e-01 -3.0232381e-02 -9.5055901e-02
   7.1118622e+00  5.5424552e-03 -1.8441556e-01  3.1117105e-01
  -2.7393317e-01  2.6658252e-01  4.0665963e-01 -2.8198040e-01
   2.5822452e-01]] 
 scale:[[ 0.03102476  0.07177044  0.37663096  0.5276757   0.46577376  0.12057849
   0.35446092  0.3551628   1.290817    0.4731574   1.4939526  12.314076
  14.637607   14.150712    3.6817648  11.124868    9.962843  ]]
2022-07-08 09:06:02.420252 - gail/discriminator/discriminator.py:67 - Discriminator uses Wasserstein distance.
2022-07-08 09:06:02.423244 - gail/discriminator/discriminator.py:68 - [<tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable:0' shape=(23, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_1:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_2:0' shape=(100, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_3:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_4:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_5:0' shape=(1,) dtype=float32_ref>]
2022-07-08 09:06:02.426041 - gail/discriminator/discriminator.py:69 - Use gradient penalty regularization (coef = 10.000000)
2022-07-08 09:06:04.769382 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions_mean]
2022-07-08 09:08:41.713256 - gail/main.py:142 - [Evaluate] iter = 0 episode={ returns = -0.6418 lengths = 1000 } discounted_episode={ returns = 0.1100 lengths = 1000 } 
2022-07-08 09:08:41.720355 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions]
2022-07-08 09:09:00.665959 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [fake_states fake_actions] => [scaled_neural_reward]
2022-07-08 09:09:01.465697 - lunzi/nn/module.py:71 - [MLPVFunction] is making TensorFlow callables, key = [states] => [values]
2022-07-08 09:09:02.963067 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [] => [sync_old]
2022-07-08 09:09:03.702592 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss flat_grad dist_std mean_kl dist_mean]
2022-07-08 09:09:08.734983 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states tangents actions] => [hessian_vec_prod]
2022-07-08 09:09:18.129043 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [] => [get_flat]
2022-07-08 09:09:19.018731 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [feed_flat] => [set_flat]
2022-07-08 09:09:20.036535 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss mean_kl]
2022-07-08 09:09:21.648145 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [train_vf vf_loss]
2022-07-08 09:09:24.386996 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [vf_loss]
2022-07-08 09:09:25.312619 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states actions_] => [mse_loss]
2022-07-08 09:09:26.163252 - gail/main.py:174 - [TRPO] iter = 1000 dist_mean = -0.0000 dist_std = 1.0000 vf_loss = 0.4141 grad_norm = 0.2861 nat_grad_norm = 0.5188 cg_residual = 0.0000 step_size = 0.4310 reward = -0.0000 fps = 4 mse_loss = 0.9358 
2022-07-08 09:09:44.373335 - gail/main.py:174 - [TRPO] iter = 2000 dist_mean = 0.0087 dist_std = 1.0053 vf_loss = 1.6372 grad_norm = 0.3517 nat_grad_norm = 0.5130 cg_residual = 0.0000 step_size = 0.3746 reward = -0.0000 fps = 4 mse_loss = 0.9822 
2022-07-08 09:10:00.211179 - gail/main.py:174 - [TRPO] iter = 3000 dist_mean = 0.0030 dist_std = 1.0018 vf_loss = 2.1472 grad_norm = 0.3032 nat_grad_norm = 0.4694 cg_residual = 0.0000 step_size = 0.4389 reward = 0.0000 fps = 4 mse_loss = 0.9519 
2022-07-08 09:10:20.186552 - gail/main.py:174 - [TRPO] iter = 4000 dist_mean = -0.0095 dist_std = 1.0049 vf_loss = 2.1587 grad_norm = 0.3233 nat_grad_norm = 0.5197 cg_residual = 0.0000 step_size = 0.3919 reward = -0.0000 fps = 3 mse_loss = 0.9790 
2022-07-08 09:10:42.259878 - gail/main.py:174 - [TRPO] iter = 5000 dist_mean = -0.0356 dist_std = 1.0034 vf_loss = 2.8862 grad_norm = 0.3388 nat_grad_norm = 0.5097 cg_residual = 0.0000 step_size = 0.4144 reward = 0.0000 fps = 3 mse_loss = 1.0049 
2022-07-08 09:10:42.262289 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [true_states true_actions fake_states fake_actions true_masks] => [train loss true_logits fake_logits true_prob fake_prob grad_norm grad_penalty regularization]
2022-07-08 09:10:50.787849 - gail/main.py:201 - [Discriminator] iter = 5000 loss = 3.2285 grad_norm = 59.2099 grad_penalty = 3.0606 regularization = 0.0000 true_logits = 0.3494 fake_logits = 0.5172 true_prob = 0.5860 fake_prob = 0.6230 
2022-07-08 09:15:34.644399 - gail/main.py:142 - [Evaluate] iter = 5000 episode={ returns = -4.6817 lengths = 1000 } discounted_episode={ returns = -3.0303 lengths = 1000 } 
2022-07-08 09:15:58.422417 - gail/main.py:174 - [TRPO] iter = 6000 dist_mean = -0.0240 dist_std = 1.0106 vf_loss = 1.3900 grad_norm = 0.2931 nat_grad_norm = 0.6099 cg_residual = 0.0000 step_size = 0.3823 reward = 0.0000 fps = 3 mse_loss = 0.9997 
2022-07-08 09:16:21.602722 - gail/main.py:174 - [TRPO] iter = 7000 dist_mean = -0.0243 dist_std = 1.0100 vf_loss = 2.5572 grad_norm = 0.3050 nat_grad_norm = 0.6084 cg_residual = 0.0000 step_size = 0.3777 reward = -0.0000 fps = 3 mse_loss = 0.9796 
2022-07-08 09:16:44.621704 - gail/main.py:174 - [TRPO] iter = 8000 dist_mean = -0.0264 dist_std = 1.0076 vf_loss = 3.1389 grad_norm = 0.3330 nat_grad_norm = 0.6053 cg_residual = 0.0000 step_size = 0.3607 reward = 0.0000 fps = 2 mse_loss = 0.9699 
2022-07-08 09:17:08.051431 - gail/main.py:174 - [TRPO] iter = 9000 dist_mean = -0.0240 dist_std = 1.0043 vf_loss = 1.0427 grad_norm = 0.2558 nat_grad_norm = 0.5970 cg_residual = 0.0000 step_size = 0.4260 reward = 0.0000 fps = 2 mse_loss = 0.9951 
2022-07-08 09:17:31.085377 - gail/main.py:174 - [TRPO] iter = 10000 dist_mean = -0.0277 dist_std = 1.0014 vf_loss = 1.8329 grad_norm = 0.2910 nat_grad_norm = 0.5550 cg_residual = 0.0000 step_size = 0.4127 reward = -0.0000 fps = 2 mse_loss = 0.9423 
2022-07-08 09:17:31.982382 - gail/main.py:201 - [Discriminator] iter = 10000 loss = 2.6530 grad_norm = 88.6811 grad_penalty = 2.4638 regularization = 0.0000 true_logits = 0.3660 fake_logits = 0.5552 true_prob = 0.5900 fake_prob = 0.6121 
2022-07-08 09:22:08.319764 - gail/main.py:142 - [Evaluate] iter = 10000 episode={ returns = -6.8716 lengths = 1000 } discounted_episode={ returns = -4.4656 lengths = 1000 } 
2022-07-08 09:22:31.155236 - gail/main.py:174 - [TRPO] iter = 11000 dist_mean = -0.0260 dist_std = 1.0015 vf_loss = 0.4262 grad_norm = 0.3406 nat_grad_norm = 0.8236 cg_residual = 0.0000 step_size = 0.3187 reward = -0.0000 fps = 3 mse_loss = 0.9629 
2022-07-08 09:22:54.247594 - gail/main.py:174 - [TRPO] iter = 12000 dist_mean = -0.0410 dist_std = 1.0010 vf_loss = 0.2303 grad_norm = 0.3133 nat_grad_norm = 0.6312 cg_residual = 0.0000 step_size = 0.3747 reward = -0.0000 fps = 3 mse_loss = 1.0848 
2022-07-08 09:23:17.726900 - gail/main.py:174 - [TRPO] iter = 13000 dist_mean = -0.0427 dist_std = 1.0019 vf_loss = 0.2669 grad_norm = 0.2932 nat_grad_norm = 0.5889 cg_residual = 0.0000 step_size = 0.4030 reward = -0.0000 fps = 2 mse_loss = 1.1235 
2022-07-08 09:23:40.559140 - gail/main.py:174 - [TRPO] iter = 14000 dist_mean = -0.0660 dist_std = 0.9986 vf_loss = 1.0967 grad_norm = 0.3189 nat_grad_norm = 0.5316 cg_residual = 0.0000 step_size = 0.4204 reward = 0.0000 fps = 2 mse_loss = 1.1412 
2022-07-08 09:24:02.736541 - gail/main.py:174 - [TRPO] iter = 15000 dist_mean = -0.0320 dist_std = 1.0015 vf_loss = 0.2936 grad_norm = 0.3352 nat_grad_norm = 0.5151 cg_residual = 0.0000 step_size = 0.4181 reward = 0.0000 fps = 2 mse_loss = 1.1984 
2022-07-08 09:24:03.528562 - gail/main.py:201 - [Discriminator] iter = 15000 loss = -0.8407 grad_norm = 51.4457 grad_penalty = 2.0545 regularization = 0.0000 true_logits = 0.3722 fake_logits = -2.5229 true_prob = 0.5914 fake_prob = 0.0960 
2022-07-08 09:28:20.595316 - gail/main.py:142 - [Evaluate] iter = 15000 episode={ returns = -60.8179 lengths = 1000 } discounted_episode={ returns = -40.2974 lengths = 1000 } 
2022-07-08 09:28:42.984150 - gail/main.py:174 - [TRPO] iter = 16000 dist_mean = -0.0864 dist_std = 1.0041 vf_loss = 1.8560 grad_norm = 0.3647 nat_grad_norm = 0.6201 cg_residual = 0.0006 step_size = 0.3678 reward = 0.0000 fps = 3 mse_loss = 1.2406 
2022-07-08 09:29:05.115358 - gail/main.py:174 - [TRPO] iter = 17000 dist_mean = -0.0365 dist_std = 1.0079 vf_loss = 0.6551 grad_norm = 0.3346 nat_grad_norm = 0.6941 cg_residual = 0.0000 step_size = 0.3407 reward = -0.0000 fps = 3 mse_loss = 1.1828 
2022-07-08 09:29:26.869314 - gail/main.py:174 - [TRPO] iter = 18000 dist_mean = -0.0559 dist_std = 1.0092 vf_loss = 1.8232 grad_norm = 0.3336 nat_grad_norm = 0.6110 cg_residual = 0.0000 step_size = 0.3796 reward = -0.0000 fps = 3 mse_loss = 1.1187 
2022-07-08 09:29:47.608139 - gail/main.py:174 - [TRPO] iter = 19000 dist_mean = -0.0289 dist_std = 1.0100 vf_loss = 1.4518 grad_norm = 0.3555 nat_grad_norm = 0.8006 cg_residual = 0.0000 step_size = 0.2980 reward = 0.0000 fps = 2 mse_loss = 1.1197 
2022-07-08 09:30:08.932060 - gail/main.py:174 - [TRPO] iter = 20000 dist_mean = -0.0733 dist_std = 1.0085 vf_loss = 3.8328 grad_norm = 0.3101 nat_grad_norm = 0.6035 cg_residual = 0.0000 step_size = 0.4160 reward = 0.0000 fps = 2 mse_loss = 1.1207 
2022-07-08 09:30:09.668609 - gail/main.py:201 - [Discriminator] iter = 20000 loss = -0.5113 grad_norm = 27.3569 grad_penalty = 1.6425 regularization = 0.0000 true_logits = 0.4592 fake_logits = -1.6946 true_prob = 0.6119 fake_prob = 0.2288 
2022-07-08 09:34:28.936241 - gail/main.py:142 - [Evaluate] iter = 20000 episode={ returns = -25.3386 lengths = 1000 } discounted_episode={ returns = -15.5967 lengths = 1000 } 
2022-07-08 09:34:50.878379 - gail/main.py:174 - [TRPO] iter = 21000 dist_mean = -0.0301 dist_std = 1.0128 vf_loss = 2.0585 grad_norm = 0.3148 nat_grad_norm = 0.5895 cg_residual = 0.0000 step_size = 0.3921 reward = -0.0000 fps = 3 mse_loss = 1.0922 
2022-07-08 09:35:11.799258 - gail/main.py:174 - [TRPO] iter = 22000 dist_mean = -0.0699 dist_std = 1.0152 vf_loss = 4.8085 grad_norm = 0.3111 nat_grad_norm = 0.5994 cg_residual = 0.0000 step_size = 0.4035 reward = 0.0000 fps = 3 mse_loss = 1.1281 
2022-07-08 09:35:32.416131 - gail/main.py:174 - [TRPO] iter = 23000 dist_mean = -0.0174 dist_std = 1.0155 vf_loss = 1.1725 grad_norm = 0.3290 nat_grad_norm = 0.7889 cg_residual = 0.0000 step_size = 0.3398 reward = 0.0000 fps = 3 mse_loss = 1.0657 
2022-07-08 09:35:53.213424 - gail/main.py:174 - [TRPO] iter = 24000 dist_mean = -0.0662 dist_std = 1.0166 vf_loss = 3.5571 grad_norm = 0.3203 nat_grad_norm = 0.5250 cg_residual = 0.0000 step_size = 0.4207 reward = 0.0000 fps = 2 mse_loss = 1.0829 
2022-07-08 09:36:13.628028 - gail/main.py:174 - [TRPO] iter = 25000 dist_mean = -0.0794 dist_std = 1.0126 vf_loss = 7.0886 grad_norm = 0.3970 nat_grad_norm = 0.5752 cg_residual = 0.0001 step_size = 0.3858 reward = 0.0000 fps = 2 mse_loss = 1.0767 
2022-07-08 09:36:14.615126 - gail/main.py:201 - [Discriminator] iter = 25000 loss = -0.4800 grad_norm = 20.0757 grad_penalty = 1.2794 regularization = 0.0000 true_logits = 0.5141 fake_logits = -1.2453 true_prob = 0.6246 fake_prob = 0.2872 
2022-07-08 09:40:25.300886 - gail/main.py:142 - [Evaluate] iter = 25000 episode={ returns = -56.8373 lengths = 1000 } discounted_episode={ returns = -35.3050 lengths = 1000 } 
2022-07-08 09:40:45.935227 - gail/main.py:174 - [TRPO] iter = 26000 dist_mean = -0.0369 dist_std = 1.0160 vf_loss = 4.8016 grad_norm = 0.3486 nat_grad_norm = 0.6777 cg_residual = 0.0000 step_size = 0.3624 reward = 0.0000 fps = 3 mse_loss = 1.1096 
2022-07-08 09:41:06.295341 - gail/main.py:174 - [TRPO] iter = 27000 dist_mean = -0.1170 dist_std = 1.0130 vf_loss = 9.8445 grad_norm = 0.3750 nat_grad_norm = 0.5894 cg_residual = 0.0002 step_size = 0.3764 reward = 0.0000 fps = 3 mse_loss = 1.1774 
2022-07-08 09:41:27.273366 - gail/main.py:174 - [TRPO] iter = 28000 dist_mean = -0.1107 dist_std = 1.0162 vf_loss = 3.4319 grad_norm = 0.2922 nat_grad_norm = 0.5508 cg_residual = 0.0004 step_size = 0.4436 reward = 0.0000 fps = 3 mse_loss = 1.0981 
2022-07-08 09:41:47.084394 - gail/main.py:174 - [TRPO] iter = 29000 dist_mean = -0.1238 dist_std = 1.0147 vf_loss = 5.0379 grad_norm = 0.3054 nat_grad_norm = 0.5759 cg_residual = 0.0004 step_size = 0.4317 reward = 0.0000 fps = 3 mse_loss = 1.0942 
2022-07-08 09:42:12.921435 - gail/main.py:174 - [TRPO] iter = 30000 dist_mean = -0.0070 dist_std = 1.0192 vf_loss = 3.3210 grad_norm = 0.3411 nat_grad_norm = 0.6142 cg_residual = 0.0000 step_size = 0.3785 reward = -0.0000 fps = 2 mse_loss = 1.0909 
2022-07-08 09:42:14.043141 - gail/main.py:201 - [Discriminator] iter = 30000 loss = -1.9315 grad_norm = 25.5681 grad_penalty = 1.2411 regularization = 0.0000 true_logits = 0.5777 fake_logits = -2.5949 true_prob = 0.6390 fake_prob = 0.0899 
2022-07-08 09:46:09.906862 - gail/main.py:142 - [Evaluate] iter = 30000 episode={ returns = 17.0251 lengths = 1000 } discounted_episode={ returns = 10.5760 lengths = 1000 } 
2022-07-08 09:46:29.873529 - gail/main.py:174 - [TRPO] iter = 31000 dist_mean = -0.0150 dist_std = 1.0270 vf_loss = 3.1839 grad_norm = 0.3266 nat_grad_norm = 0.5777 cg_residual = 0.0000 step_size = 0.3901 reward = -0.0000 fps = 3 mse_loss = 1.1227 
2022-07-08 09:46:49.944979 - gail/main.py:174 - [TRPO] iter = 32000 dist_mean = -0.0330 dist_std = 1.0269 vf_loss = 5.5782 grad_norm = 0.3140 nat_grad_norm = 0.5685 cg_residual = 0.0000 step_size = 0.4239 reward = -0.0000 fps = 3 mse_loss = 1.0809 
2022-07-08 09:47:10.104965 - gail/main.py:174 - [TRPO] iter = 33000 dist_mean = -0.0992 dist_std = 1.0289 vf_loss = 4.8228 grad_norm = 0.3678 nat_grad_norm = 0.5714 cg_residual = 0.0002 step_size = 0.4058 reward = 0.0000 fps = 3 mse_loss = 1.1236 
2022-07-08 09:47:30.169938 - gail/main.py:174 - [TRPO] iter = 34000 dist_mean = -0.0510 dist_std = 1.0302 vf_loss = 2.0687 grad_norm = 0.3693 nat_grad_norm = 0.6430 cg_residual = 0.0004 step_size = 0.3805 reward = 0.0000 fps = 3 mse_loss = 1.0947 
2022-07-08 09:47:50.878888 - gail/main.py:174 - [TRPO] iter = 35000 dist_mean = -0.0897 dist_std = 1.0241 vf_loss = 3.7127 grad_norm = 0.3298 nat_grad_norm = 0.5750 cg_residual = 0.0001 step_size = 0.4112 reward = 0.0000 fps = 2 mse_loss = 1.1487 
2022-07-08 09:47:51.666944 - gail/main.py:201 - [Discriminator] iter = 35000 loss = 0.0243 grad_norm = 30.2598 grad_penalty = 1.4084 regularization = 0.0000 true_logits = 0.6174 fake_logits = -0.7667 true_prob = 0.6478 fake_prob = 0.3430 
2022-07-08 09:51:53.106575 - gail/main.py:142 - [Evaluate] iter = 35000 episode={ returns = 162.1926 lengths = 1000 } discounted_episode={ returns = 96.1086 lengths = 1000 } 
2022-07-08 09:52:20.576186 - gail/main.py:174 - [TRPO] iter = 36000 dist_mean = -0.0271 dist_std = 1.0272 vf_loss = 2.6562 grad_norm = 0.3226 nat_grad_norm = 0.5643 cg_residual = 0.0000 step_size = 0.4140 reward = 0.0000 fps = 3 mse_loss = 1.1758 
2022-07-08 09:52:40.722506 - gail/main.py:174 - [TRPO] iter = 37000 dist_mean = -0.0524 dist_std = 1.0254 vf_loss = 1.4240 grad_norm = 0.3810 nat_grad_norm = 0.7172 cg_residual = 0.0006 step_size = 0.3378 reward = 0.0000 fps = 3 mse_loss = 1.2621 
2022-07-08 09:53:01.076979 - gail/main.py:174 - [TRPO] iter = 38000 dist_mean = 0.0201 dist_std = 1.0314 vf_loss = 2.1474 grad_norm = 0.3741 nat_grad_norm = 0.7603 cg_residual = 0.0000 step_size = 0.3179 reward = 0.0000 fps = 3 mse_loss = 1.2820 
2022-07-08 09:53:21.237831 - gail/main.py:174 - [TRPO] iter = 39000 dist_mean = 0.0285 dist_std = 1.0295 vf_loss = 0.6608 grad_norm = 0.4024 nat_grad_norm = 0.7745 cg_residual = 0.0001 step_size = 0.3127 reward = -0.0000 fps = 3 mse_loss = 1.2558 
2022-07-08 09:53:43.562959 - gail/main.py:174 - [TRPO] iter = 40000 dist_mean = -0.0507 dist_std = 1.0329 vf_loss = 2.5836 grad_norm = 0.3553 nat_grad_norm = 0.5985 cg_residual = 0.0000 step_size = 0.3932 reward = 0.0000 fps = 2 mse_loss = 1.2950 
2022-07-08 09:53:44.300118 - gail/main.py:201 - [Discriminator] iter = 40000 loss = -1.1525 grad_norm = 21.9718 grad_penalty = 1.3343 regularization = 0.0000 true_logits = 0.6464 fake_logits = -1.8405 true_prob = 0.6537 fake_prob = 0.1989 
2022-07-08 09:57:37.234542 - gail/main.py:142 - [Evaluate] iter = 40000 episode={ returns = 356.5229 lengths = 1000 } discounted_episode={ returns = 223.8941 lengths = 1000 } 
2022-07-08 09:57:56.776825 - gail/main.py:174 - [TRPO] iter = 41000 dist_mean = -0.0245 dist_std = 1.0384 vf_loss = 5.7357 grad_norm = 0.3393 nat_grad_norm = 0.6239 cg_residual = 0.0000 step_size = 0.3883 reward = -0.0000 fps = 3 mse_loss = 1.3906 
2022-07-08 09:58:16.366056 - gail/main.py:174 - [TRPO] iter = 42000 dist_mean = -0.0526 dist_std = 1.0384 vf_loss = 9.1997 grad_norm = 0.3113 nat_grad_norm = 0.5990 cg_residual = 0.0000 step_size = 0.4148 reward = 0.0000 fps = 3 mse_loss = 1.4081 
2022-07-08 09:58:37.305566 - gail/main.py:174 - [TRPO] iter = 43000 dist_mean = -0.0132 dist_std = 1.0334 vf_loss = 4.0203 grad_norm = 0.2786 nat_grad_norm = 0.5468 cg_residual = 0.0000 step_size = 0.4816 reward = 0.0000 fps = 3 mse_loss = 1.4938 
2022-07-08 09:58:58.046932 - gail/main.py:174 - [TRPO] iter = 44000 dist_mean = 0.0170 dist_std = 1.0367 vf_loss = 2.4584 grad_norm = 0.3049 nat_grad_norm = 0.6305 cg_residual = 0.0011 step_size = 0.4001 reward = 0.0000 fps = 3 mse_loss = 1.5395 
2022-07-08 09:59:17.176729 - gail/main.py:174 - [TRPO] iter = 45000 dist_mean = 0.0237 dist_std = 1.0340 vf_loss = 6.1691 grad_norm = 0.3537 nat_grad_norm = 0.6847 cg_residual = 0.0000 step_size = 0.3759 reward = 0.0000 fps = 3 mse_loss = 1.5703 
2022-07-08 09:59:17.947272 - gail/main.py:201 - [Discriminator] iter = 45000 loss = -2.2016 grad_norm = 20.3683 grad_penalty = 1.1765 regularization = 0.0000 true_logits = 0.7290 fake_logits = -2.6492 true_prob = 0.6714 fake_prob = 0.1091 
2022-07-08 10:03:44.408043 - gail/main.py:142 - [Evaluate] iter = 45000 episode={ returns = 306.8668 lengths = 1000 } discounted_episode={ returns = 194.8634 lengths = 1000 } 
2022-07-08 10:04:08.275146 - gail/main.py:174 - [TRPO] iter = 46000 dist_mean = -0.0548 dist_std = 1.0311 vf_loss = 8.3197 grad_norm = 0.3244 nat_grad_norm = 0.5766 cg_residual = 0.0001 step_size = 0.4250 reward = 0.0000 fps = 3 mse_loss = 1.6330 
2022-07-08 10:04:31.044015 - gail/main.py:174 - [TRPO] iter = 47000 dist_mean = -0.0369 dist_std = 1.0385 vf_loss = 2.8471 grad_norm = 0.3710 nat_grad_norm = 0.6975 cg_residual = 0.0011 step_size = 0.3383 reward = 0.0000 fps = 3 mse_loss = 1.7711 
2022-07-08 10:04:53.752404 - gail/main.py:174 - [TRPO] iter = 48000 dist_mean = -0.0529 dist_std = 1.0382 vf_loss = 1.4429 grad_norm = 0.3577 nat_grad_norm = 0.6201 cg_residual = 0.0001 step_size = 0.3957 reward = 0.0000 fps = 2 mse_loss = 1.8224 
2022-07-08 10:05:16.906033 - gail/main.py:174 - [TRPO] iter = 49000 dist_mean = 0.0436 dist_std = 1.0404 vf_loss = 2.1715 grad_norm = 0.3275 nat_grad_norm = 0.6237 cg_residual = 0.0000 step_size = 0.3895 reward = 0.0000 fps = 2 mse_loss = 1.8796 
2022-07-08 10:05:39.951520 - gail/main.py:174 - [TRPO] iter = 50000 dist_mean = 0.0228 dist_std = 1.0384 vf_loss = 3.1236 grad_norm = 0.3255 nat_grad_norm = 0.6122 cg_residual = 0.0000 step_size = 0.4129 reward = 0.0000 fps = 2 mse_loss = 1.7930 
2022-07-08 10:05:40.799818 - gail/main.py:201 - [Discriminator] iter = 50000 loss = -2.5619 grad_norm = 15.2932 grad_penalty = 0.9944 regularization = 0.0000 true_logits = 0.8209 fake_logits = -2.7354 true_prob = 0.6908 fake_prob = 0.1124 
2022-07-08 10:10:15.117502 - gail/main.py:142 - [Evaluate] iter = 50000 episode={ returns = -10.5158 lengths = 1000 } discounted_episode={ returns = -5.4559 lengths = 1000 } 
2022-07-08 10:10:37.336712 - gail/main.py:174 - [TRPO] iter = 51000 dist_mean = -0.0143 dist_std = 1.0385 vf_loss = 5.6458 grad_norm = 0.4316 nat_grad_norm = 0.6731 cg_residual = 0.0001 step_size = 0.3445 reward = 0.0000 fps = 3 mse_loss = 1.8708 
2022-07-08 10:10:59.990916 - gail/main.py:174 - [TRPO] iter = 52000 dist_mean = 0.0335 dist_std = 1.0476 vf_loss = 8.3126 grad_norm = 0.3674 nat_grad_norm = 0.7518 cg_residual = 0.0000 step_size = 0.3380 reward = 0.0000 fps = 3 mse_loss = 1.8890 
2022-07-08 10:11:23.496994 - gail/main.py:174 - [TRPO] iter = 53000 dist_mean = 0.0323 dist_std = 1.0494 vf_loss = 1.6635 grad_norm = 0.3905 nat_grad_norm = 0.6819 cg_residual = 0.0002 step_size = 0.3456 reward = -0.0000 fps = 2 mse_loss = 1.9585 
2022-07-08 10:11:46.472817 - gail/main.py:174 - [TRPO] iter = 54000 dist_mean = 0.0298 dist_std = 1.0503 vf_loss = 1.2810 grad_norm = 0.3399 nat_grad_norm = 0.6250 cg_residual = 0.0003 step_size = 0.3728 reward = -0.0000 fps = 2 mse_loss = 2.1630 
2022-07-08 10:12:15.676164 - gail/main.py:174 - [TRPO] iter = 55000 dist_mean = -0.0300 dist_std = 1.0508 vf_loss = 2.9118 grad_norm = 0.3723 nat_grad_norm = 0.5928 cg_residual = 0.0001 step_size = 0.4060 reward = 0.0000 fps = 2 mse_loss = 2.0450 
2022-07-08 10:12:16.571804 - gail/main.py:201 - [Discriminator] iter = 55000 loss = -1.5895 grad_norm = 18.9950 grad_penalty = 1.1853 regularization = 0.0000 true_logits = 0.9502 fake_logits = -1.8246 true_prob = 0.7162 fake_prob = 0.1836 
2022-07-08 10:16:48.458826 - gail/main.py:142 - [Evaluate] iter = 55000 episode={ returns = -24.9582 lengths = 1000 } discounted_episode={ returns = -16.3155 lengths = 1000 } 
2022-07-08 10:17:10.967996 - gail/main.py:174 - [TRPO] iter = 56000 dist_mean = -0.0658 dist_std = 1.0503 vf_loss = 4.7005 grad_norm = 0.3944 nat_grad_norm = 0.7810 cg_residual = 0.0008 step_size = 0.3530 reward = -0.0000 fps = 3 mse_loss = 2.2479 
2022-07-08 10:17:32.736547 - gail/main.py:174 - [TRPO] iter = 57000 dist_mean = 0.0451 dist_std = 1.0528 vf_loss = 2.7114 grad_norm = 0.3908 nat_grad_norm = 0.7676 cg_residual = 0.0002 step_size = 0.3197 reward = 0.0000 fps = 3 mse_loss = 2.2251 
2022-07-08 10:17:55.566891 - gail/main.py:174 - [TRPO] iter = 58000 dist_mean = 0.0267 dist_std = 1.0566 vf_loss = 1.2706 grad_norm = 0.3707 nat_grad_norm = 0.6691 cg_residual = 0.0000 step_size = 0.3675 reward = 0.0000 fps = 2 mse_loss = 2.2996 
2022-07-08 10:18:17.256503 - gail/main.py:174 - [TRPO] iter = 59000 dist_mean = 0.0341 dist_std = 1.0567 vf_loss = 2.3063 grad_norm = 0.3293 nat_grad_norm = 0.6792 cg_residual = 0.0001 step_size = 0.3888 reward = -0.0000 fps = 2 mse_loss = 2.3944 
2022-07-08 10:18:39.791531 - gail/main.py:174 - [TRPO] iter = 60000 dist_mean = -0.0315 dist_std = 1.0542 vf_loss = 2.2479 grad_norm = 0.4484 nat_grad_norm = 0.7907 cg_residual = 0.0003 step_size = 0.3269 reward = -0.0000 fps = 2 mse_loss = 2.4027 
2022-07-08 10:18:40.556236 - gail/main.py:201 - [Discriminator] iter = 60000 loss = -1.8656 grad_norm = 12.5605 grad_penalty = 0.7667 regularization = 0.0000 true_logits = 1.0443 fake_logits = -1.5881 true_prob = 0.7344 fake_prob = 0.1980 
2022-07-08 10:23:11.679425 - gail/main.py:142 - [Evaluate] iter = 60000 episode={ returns = 51.4031 lengths = 1000 } discounted_episode={ returns = 30.5173 lengths = 1000 } 
2022-07-08 10:23:33.820228 - gail/main.py:174 - [TRPO] iter = 61000 dist_mean = 0.0778 dist_std = 1.0534 vf_loss = 3.5281 grad_norm = 0.3621 nat_grad_norm = 0.7092 cg_residual = 0.0004 step_size = 0.3588 reward = -0.0000 fps = 3 mse_loss = 2.2969 
2022-07-08 10:23:56.015270 - gail/main.py:174 - [TRPO] iter = 62000 dist_mean = -0.0899 dist_std = 1.0584 vf_loss = 4.3099 grad_norm = 0.4426 nat_grad_norm = 0.5649 cg_residual = 0.0005 step_size = 0.3948 reward = -0.0000 fps = 3 mse_loss = 2.3894 
2022-07-08 10:24:18.547142 - gail/main.py:174 - [TRPO] iter = 63000 dist_mean = -0.0873 dist_std = 1.0619 vf_loss = 2.0751 grad_norm = 0.4190 nat_grad_norm = 0.6806 cg_residual = 0.0004 step_size = 0.3459 reward = 0.0000 fps = 2 mse_loss = 2.4504 
2022-07-08 10:24:41.594472 - gail/main.py:174 - [TRPO] iter = 64000 dist_mean = 0.0860 dist_std = 1.0644 vf_loss = 1.5541 grad_norm = 0.3287 nat_grad_norm = 0.7104 cg_residual = 0.0003 step_size = 0.3696 reward = 0.0000 fps = 2 mse_loss = 2.2479 
2022-07-08 10:25:04.218993 - gail/main.py:174 - [TRPO] iter = 65000 dist_mean = -0.0649 dist_std = 1.0632 vf_loss = 2.1692 grad_norm = 0.4576 nat_grad_norm = 0.7017 cg_residual = 0.0005 step_size = 0.3287 reward = -0.0000 fps = 2 mse_loss = 2.2323 
2022-07-08 10:25:04.946863 - gail/main.py:201 - [Discriminator] iter = 65000 loss = -1.7858 grad_norm = 15.3684 grad_penalty = 0.8344 regularization = 0.0000 true_logits = 1.2067 fake_logits = -1.4136 true_prob = 0.7624 fake_prob = 0.2050 
2022-07-08 10:29:46.146057 - gail/main.py:142 - [Evaluate] iter = 65000 episode={ returns = -29.0521 lengths = 1000 } discounted_episode={ returns = -17.7355 lengths = 1000 } 
2022-07-08 10:30:11.121545 - gail/main.py:174 - [TRPO] iter = 66000 dist_mean = 0.0356 dist_std = 1.0623 vf_loss = 0.7525 grad_norm = 0.3175 nat_grad_norm = 0.5700 cg_residual = 0.0000 step_size = 0.4302 reward = 0.0000 fps = 3 mse_loss = 2.4071 
2022-07-08 10:30:35.567846 - gail/main.py:174 - [TRPO] iter = 67000 dist_mean = 0.0367 dist_std = 1.0622 vf_loss = 0.6332 grad_norm = 0.3388 nat_grad_norm = 0.6170 cg_residual = 0.0000 step_size = 0.3996 reward = 0.0000 fps = 3 mse_loss = 2.3480 
2022-07-08 10:31:00.401264 - gail/main.py:174 - [TRPO] iter = 68000 dist_mean = -0.0554 dist_std = 1.0667 vf_loss = 2.7488 grad_norm = 0.4356 nat_grad_norm = 0.6004 cg_residual = 0.0005 step_size = 0.3905 reward = -0.0000 fps = 2 mse_loss = 2.4374 
2022-07-08 10:31:25.630746 - gail/main.py:174 - [TRPO] iter = 69000 dist_mean = -0.0108 dist_std = 1.0708 vf_loss = 2.7046 grad_norm = 0.3881 nat_grad_norm = 0.7885 cg_residual = 0.0001 step_size = 0.3349 reward = 0.0000 fps = 2 mse_loss = 2.1560 
2022-07-08 10:31:54.928255 - gail/main.py:174 - [TRPO] iter = 70000 dist_mean = 0.0257 dist_std = 1.0674 vf_loss = 1.2304 grad_norm = 0.3260 nat_grad_norm = 0.5956 cg_residual = 0.0000 step_size = 0.4431 reward = -0.0000 fps = 2 mse_loss = 2.1993 
2022-07-08 10:31:55.761692 - gail/main.py:201 - [Discriminator] iter = 70000 loss = -3.0960 grad_norm = 16.5851 grad_penalty = 0.8603 regularization = 0.0000 true_logits = 1.3636 fake_logits = -2.5927 true_prob = 0.7879 fake_prob = 0.1059 
