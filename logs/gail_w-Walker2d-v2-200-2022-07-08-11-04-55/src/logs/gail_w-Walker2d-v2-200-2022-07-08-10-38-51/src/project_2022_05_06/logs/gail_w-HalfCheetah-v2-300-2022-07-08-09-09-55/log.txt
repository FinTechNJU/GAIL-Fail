2022-07-08 09:09:56.443579 - utils/flags.py:257 - log_dir = logs/gail_w-HalfCheetah-v2-300-2022-07-08-09-09-55
2022-07-08 09:10:36.022888 - gail/utils/replay_buffer.py:86 - Load dataset from /content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/HalfCheetah-v2
2022-07-08 09:11:09.761165 - gail/main.py:80 - Expert Reward 6594.485577
2022-07-08 09:11:12.067059 - gail/main.py:84 - Original dataset size 3000
2022-07-08 09:11:12.305402 - gail/main.py:86 - Subsampled dataset size 3000
2022-07-08 09:11:12.363279 - gail/main.py:87 - np random: 864 random : 786
2022-07-08 09:11:12.368435 - gail/main.py:91 - Sampled obs: 0.6422, acs: 0.2570
2022-07-08 09:11:17.749517 - lunzi/nn/flat_param.py:17 - Enabling flattening... ['GaussianMLPPolicy_1/log_std:0', 'GaussianMLPPolicy_1/Variable:0', 'GaussianMLPPolicy_1/Variable_1:0', 'GaussianMLPPolicy_1/Variable_2:0', 'GaussianMLPPolicy_1/Variable_3:0', 'GaussianMLPPolicy_1/Variable_4:0', 'GaussianMLPPolicy_1/Variable_5:0']
2022-07-08 09:11:54.367110 - gail/discriminator/discriminator.py:30 - Use predefined normalization.
2022-07-08 09:11:54.396072 - gail/discriminator/discriminator.py:33 - Normalizer loc:[[-1.0330443e-01 -9.3026832e-03  2.9791275e-01  8.1040427e-02
   1.7489190e-01  4.0859219e-01 -3.0232381e-02 -9.5055901e-02
   7.1118622e+00  5.5424552e-03 -1.8441556e-01  3.1117105e-01
  -2.7393317e-01  2.6658252e-01  4.0665963e-01 -2.8198040e-01
   2.5822452e-01]] 
 scale:[[ 0.03102476  0.07177044  0.37663096  0.5276757   0.46577376  0.12057849
   0.35446092  0.3551628   1.290817    0.4731574   1.4939526  12.314076
  14.637607   14.150712    3.6817648  11.124868    9.962843  ]]
2022-07-08 09:12:12.349138 - gail/discriminator/discriminator.py:67 - Discriminator uses Wasserstein distance.
2022-07-08 09:12:12.358143 - gail/discriminator/discriminator.py:68 - [<tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable:0' shape=(23, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_1:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_2:0' shape=(100, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_3:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_4:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_5:0' shape=(1,) dtype=float32_ref>]
2022-07-08 09:12:12.386045 - gail/discriminator/discriminator.py:69 - Use gradient penalty regularization (coef = 10.000000)
2022-07-08 09:12:16.590055 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions_mean]
2022-07-08 09:16:59.762659 - gail/main.py:142 - [Evaluate] iter = 0 episode={ returns = 0.0497 lengths = 1000 } discounted_episode={ returns = -0.2886 lengths = 1000 } 
2022-07-08 09:16:59.774438 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions]
2022-07-08 09:17:31.853951 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [fake_states fake_actions] => [scaled_neural_reward]
2022-07-08 09:17:32.973150 - lunzi/nn/module.py:71 - [MLPVFunction] is making TensorFlow callables, key = [states] => [values]
2022-07-08 09:17:34.856645 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [] => [sync_old]
2022-07-08 09:17:35.735248 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss flat_grad dist_std mean_kl dist_mean]
2022-07-08 09:17:41.849717 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states tangents actions] => [hessian_vec_prod]
2022-07-08 09:17:52.525453 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [] => [get_flat]
2022-07-08 09:17:53.574649 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [feed_flat] => [set_flat]
2022-07-08 09:17:54.685365 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss mean_kl]
2022-07-08 09:17:56.528581 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [train_vf vf_loss]
2022-07-08 09:17:59.511882 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [vf_loss]
2022-07-08 09:18:00.502077 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states actions_] => [mse_loss]
2022-07-08 09:18:01.559712 - gail/main.py:174 - [TRPO] iter = 1000 dist_mean = -0.0000 dist_std = 1.0000 vf_loss = 0.9310 grad_norm = 0.3044 nat_grad_norm = 0.5189 cg_residual = 0.0000 step_size = 0.4012 reward = -0.0000 fps = 2 mse_loss = 0.9230 
2022-07-08 09:18:23.908507 - gail/main.py:174 - [TRPO] iter = 2000 dist_mean = 0.0076 dist_std = 0.9946 vf_loss = 0.8663 grad_norm = 0.3123 nat_grad_norm = 0.5171 cg_residual = 0.0000 step_size = 0.4079 reward = -0.0000 fps = 2 mse_loss = 0.9096 
2022-07-08 09:18:46.783518 - gail/main.py:174 - [TRPO] iter = 3000 dist_mean = 0.0113 dist_std = 0.9992 vf_loss = 1.8100 grad_norm = 0.2948 nat_grad_norm = 0.5528 cg_residual = 0.0000 step_size = 0.3915 reward = 0.0000 fps = 2 mse_loss = 0.9029 
2022-07-08 09:19:10.221729 - gail/main.py:174 - [TRPO] iter = 4000 dist_mean = 0.0131 dist_std = 1.0011 vf_loss = 2.5428 grad_norm = 0.3180 nat_grad_norm = 0.5054 cg_residual = 0.0000 step_size = 0.4217 reward = 0.0000 fps = 2 mse_loss = 0.9105 
2022-07-08 09:19:33.042593 - gail/main.py:174 - [TRPO] iter = 5000 dist_mean = 0.0048 dist_std = 1.0023 vf_loss = 1.0869 grad_norm = 0.2934 nat_grad_norm = 0.6174 cg_residual = 0.0001 step_size = 0.3895 reward = 0.0000 fps = 2 mse_loss = 0.8791 
2022-07-08 09:19:33.045974 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [true_states true_actions fake_states fake_actions true_masks] => [train loss true_logits fake_logits true_prob fake_prob grad_norm grad_penalty regularization]
2022-07-08 09:19:42.029822 - gail/main.py:201 - [Discriminator] iter = 5000 loss = 1.9551 grad_norm = 80.9477 grad_penalty = 1.7928 regularization = 0.0000 true_logits = -0.0399 fake_logits = 0.1225 true_prob = 0.4902 fake_prob = 0.5273 
2022-07-08 09:24:18.324789 - gail/main.py:142 - [Evaluate] iter = 5000 episode={ returns = -1.7388 lengths = 1000 } discounted_episode={ returns = -0.7453 lengths = 1000 } 
2022-07-08 09:24:38.621125 - gail/main.py:174 - [TRPO] iter = 6000 dist_mean = 0.0025 dist_std = 1.0057 vf_loss = 0.5450 grad_norm = 0.3096 nat_grad_norm = 0.5398 cg_residual = 0.0000 step_size = 0.3837 reward = 0.0000 fps = 3 mse_loss = 0.9021 
2022-07-08 09:25:00.218384 - gail/main.py:174 - [TRPO] iter = 7000 dist_mean = -0.0038 dist_std = 1.0050 vf_loss = 0.1864 grad_norm = 0.2801 nat_grad_norm = 0.4563 cg_residual = 0.0000 step_size = 0.4640 reward = -0.0000 fps = 3 mse_loss = 0.8705 
2022-07-08 09:25:20.594260 - gail/main.py:174 - [TRPO] iter = 8000 dist_mean = 0.0139 dist_std = 1.0151 vf_loss = 0.3025 grad_norm = 0.3201 nat_grad_norm = 0.5450 cg_residual = 0.0000 step_size = 0.3828 reward = -0.0000 fps = 2 mse_loss = 0.8830 
2022-07-08 09:25:41.953827 - gail/main.py:174 - [TRPO] iter = 9000 dist_mean = 0.0264 dist_std = 1.0125 vf_loss = 0.2016 grad_norm = 0.3185 nat_grad_norm = 0.5782 cg_residual = 0.0000 step_size = 0.4086 reward = 0.0000 fps = 2 mse_loss = 0.9440 
2022-07-08 09:26:05.206851 - gail/main.py:174 - [TRPO] iter = 10000 dist_mean = 0.0239 dist_std = 1.0178 vf_loss = 0.5258 grad_norm = 0.2946 nat_grad_norm = 0.5352 cg_residual = 0.0000 step_size = 0.4058 reward = -0.0000 fps = 2 mse_loss = 0.9555 
2022-07-08 09:26:06.090836 - gail/main.py:201 - [Discriminator] iter = 10000 loss = 0.0920 grad_norm = 47.7800 grad_penalty = 1.9564 regularization = 0.0000 true_logits = 0.0672 fake_logits = -1.7972 true_prob = 0.5167 fake_prob = 0.1803 
2022-07-08 09:30:23.445260 - gail/main.py:142 - [Evaluate] iter = 10000 episode={ returns = -14.3423 lengths = 1000 } discounted_episode={ returns = -8.9294 lengths = 1000 } 
2022-07-08 09:30:44.341710 - gail/main.py:174 - [TRPO] iter = 11000 dist_mean = 0.0249 dist_std = 1.0209 vf_loss = 1.9844 grad_norm = 0.3148 nat_grad_norm = 0.5348 cg_residual = 0.0000 step_size = 0.3959 reward = -0.0000 fps = 3 mse_loss = 0.9599 
2022-07-08 09:31:05.817353 - gail/main.py:174 - [TRPO] iter = 12000 dist_mean = 0.0304 dist_std = 1.0232 vf_loss = 2.2726 grad_norm = 0.3083 nat_grad_norm = 0.6730 cg_residual = 0.0000 step_size = 0.3563 reward = 0.0000 fps = 3 mse_loss = 1.0812 
2022-07-08 09:31:26.828671 - gail/main.py:174 - [TRPO] iter = 13000 dist_mean = 0.0014 dist_std = 1.0236 vf_loss = 5.7882 grad_norm = 0.2803 nat_grad_norm = 0.4871 cg_residual = 0.0000 step_size = 0.4550 reward = 0.0000 fps = 3 mse_loss = 0.9914 
2022-07-08 09:31:47.876412 - gail/main.py:174 - [TRPO] iter = 14000 dist_mean = -0.0133 dist_std = 1.0200 vf_loss = 6.8675 grad_norm = 0.3228 nat_grad_norm = 0.5562 cg_residual = 0.0000 step_size = 0.4018 reward = 0.0000 fps = 2 mse_loss = 1.0269 
2022-07-08 09:32:10.738378 - gail/main.py:174 - [TRPO] iter = 15000 dist_mean = 0.0387 dist_std = 1.0142 vf_loss = 4.7836 grad_norm = 0.3106 nat_grad_norm = 0.5628 cg_residual = 0.0000 step_size = 0.3987 reward = 0.0000 fps = 2 mse_loss = 0.9920 
2022-07-08 09:32:12.154931 - gail/main.py:201 - [Discriminator] iter = 15000 loss = -1.2221 grad_norm = 24.5887 grad_penalty = 1.6917 regularization = 0.0000 true_logits = 0.1376 fake_logits = -2.7762 true_prob = 0.5340 fake_prob = 0.1200 
2022-07-08 09:36:26.059049 - gail/main.py:142 - [Evaluate] iter = 15000 episode={ returns = -16.1296 lengths = 1000 } discounted_episode={ returns = -10.2912 lengths = 1000 } 
2022-07-08 09:36:46.372108 - gail/main.py:174 - [TRPO] iter = 16000 dist_mean = 0.0583 dist_std = 1.0140 vf_loss = 2.2366 grad_norm = 0.3028 nat_grad_norm = 0.6005 cg_residual = 0.0000 step_size = 0.3810 reward = 0.0000 fps = 3 mse_loss = 0.9942 
2022-07-08 09:37:06.804800 - gail/main.py:174 - [TRPO] iter = 17000 dist_mean = 0.0640 dist_std = 1.0195 vf_loss = 0.9651 grad_norm = 0.3105 nat_grad_norm = 0.7245 cg_residual = 0.0001 step_size = 0.3580 reward = 0.0000 fps = 3 mse_loss = 1.0556 
2022-07-08 09:37:26.569628 - gail/main.py:174 - [TRPO] iter = 18000 dist_mean = 0.0288 dist_std = 1.0215 vf_loss = 12.0084 grad_norm = 0.3581 nat_grad_norm = 0.5606 cg_residual = 0.0001 step_size = 0.3846 reward = 0.0000 fps = 3 mse_loss = 1.0047 
2022-07-08 09:37:47.395114 - gail/main.py:174 - [TRPO] iter = 19000 dist_mean = -0.0137 dist_std = 1.0154 vf_loss = 1.5065 grad_norm = 0.3489 nat_grad_norm = 0.5931 cg_residual = 0.0003 step_size = 0.3623 reward = -0.0000 fps = 2 mse_loss = 1.0726 
2022-07-08 09:38:07.627925 - gail/main.py:174 - [TRPO] iter = 20000 dist_mean = 0.0375 dist_std = 1.0201 vf_loss = 1.9757 grad_norm = 0.3471 nat_grad_norm = 0.5715 cg_residual = 0.0000 step_size = 0.3821 reward = -0.0000 fps = 2 mse_loss = 0.9998 
2022-07-08 09:38:08.451686 - gail/main.py:201 - [Discriminator] iter = 20000 loss = 1.1496 grad_norm = 27.3559 grad_penalty = 2.1111 regularization = 0.0000 true_logits = 0.2078 fake_logits = -0.7537 true_prob = 0.5508 fake_prob = 0.3235 
2022-07-08 09:42:18.595056 - gail/main.py:142 - [Evaluate] iter = 20000 episode={ returns = -32.8444 lengths = 1000 } discounted_episode={ returns = -20.5234 lengths = 1000 } 
2022-07-08 09:42:38.921684 - gail/main.py:174 - [TRPO] iter = 21000 dist_mean = 0.0249 dist_std = 1.0216 vf_loss = 2.8469 grad_norm = 0.3170 nat_grad_norm = 0.5373 cg_residual = 0.0000 step_size = 0.4206 reward = -0.0000 fps = 3 mse_loss = 1.0349 
2022-07-08 09:42:58.780861 - gail/main.py:174 - [TRPO] iter = 22000 dist_mean = 0.0568 dist_std = 1.0162 vf_loss = 2.1972 grad_norm = 0.3541 nat_grad_norm = 0.6090 cg_residual = 0.0000 step_size = 0.3527 reward = 0.0000 fps = 3 mse_loss = 1.1193 
2022-07-08 09:43:17.816464 - gail/main.py:174 - [TRPO] iter = 23000 dist_mean = 0.0331 dist_std = 1.0218 vf_loss = 3.5304 grad_norm = 0.3232 nat_grad_norm = 0.5602 cg_residual = 0.0000 step_size = 0.3985 reward = -0.0000 fps = 3 mse_loss = 1.1045 
2022-07-08 09:43:37.366154 - gail/main.py:174 - [TRPO] iter = 24000 dist_mean = 0.0264 dist_std = 1.0242 vf_loss = 3.0527 grad_norm = 0.2965 nat_grad_norm = 0.4703 cg_residual = 0.0000 step_size = 0.4851 reward = 0.0000 fps = 3 mse_loss = 1.1268 
2022-07-08 09:43:56.926516 - gail/main.py:174 - [TRPO] iter = 25000 dist_mean = 0.0403 dist_std = 1.0321 vf_loss = 2.2743 grad_norm = 0.3114 nat_grad_norm = 0.5511 cg_residual = 0.0000 step_size = 0.4185 reward = -0.0000 fps = 2 mse_loss = 1.1435 
2022-07-08 09:43:57.710697 - gail/main.py:201 - [Discriminator] iter = 25000 loss = 0.2046 grad_norm = 20.0469 grad_penalty = 1.4515 regularization = 0.0000 true_logits = 0.2889 fake_logits = -0.9580 true_prob = 0.5697 fake_prob = 0.2811 
2022-07-08 09:47:55.830056 - gail/main.py:142 - [Evaluate] iter = 25000 episode={ returns = -29.1826 lengths = 1000 } discounted_episode={ returns = -18.5164 lengths = 1000 } 
2022-07-08 09:48:15.674536 - gail/main.py:174 - [TRPO] iter = 26000 dist_mean = 0.0517 dist_std = 1.0287 vf_loss = 4.5853 grad_norm = 0.3048 nat_grad_norm = 0.4883 cg_residual = 0.0000 step_size = 0.4531 reward = -0.0000 fps = 3 mse_loss = 1.1608 
2022-07-08 09:48:35.321549 - gail/main.py:174 - [TRPO] iter = 27000 dist_mean = 0.0533 dist_std = 1.0212 vf_loss = 2.8777 grad_norm = 0.3353 nat_grad_norm = 0.5534 cg_residual = 0.0000 step_size = 0.4101 reward = -0.0000 fps = 3 mse_loss = 1.0613 
2022-07-08 09:48:55.022842 - gail/main.py:174 - [TRPO] iter = 28000 dist_mean = 0.0523 dist_std = 1.0167 vf_loss = 0.5546 grad_norm = 0.3213 nat_grad_norm = 0.5261 cg_residual = 0.0000 step_size = 0.4329 reward = -0.0000 fps = 3 mse_loss = 1.0754 
2022-07-08 09:49:14.886788 - gail/main.py:174 - [TRPO] iter = 29000 dist_mean = 0.1236 dist_std = 1.0155 vf_loss = 1.2730 grad_norm = 0.3684 nat_grad_norm = 0.5696 cg_residual = 0.0001 step_size = 0.3779 reward = -0.0000 fps = 3 mse_loss = 1.0890 
2022-07-08 09:49:35.000380 - gail/main.py:174 - [TRPO] iter = 30000 dist_mean = 0.0554 dist_std = 1.0168 vf_loss = 1.0473 grad_norm = 0.3237 nat_grad_norm = 0.5172 cg_residual = 0.0000 step_size = 0.4342 reward = -0.0000 fps = 2 mse_loss = 1.0433 
2022-07-08 09:49:35.894928 - gail/main.py:201 - [Discriminator] iter = 30000 loss = -0.3026 grad_norm = 17.2650 grad_penalty = 1.2467 regularization = 0.0000 true_logits = 0.3698 fake_logits = -1.1796 true_prob = 0.5882 fake_prob = 0.2401 
2022-07-08 09:53:47.379173 - gail/main.py:142 - [Evaluate] iter = 30000 episode={ returns = -31.2151 lengths = 1000 } discounted_episode={ returns = -19.5088 lengths = 1000 } 
2022-07-08 09:54:08.487196 - gail/main.py:174 - [TRPO] iter = 31000 dist_mean = 0.0698 dist_std = 1.0104 vf_loss = 1.4108 grad_norm = 0.3304 nat_grad_norm = 0.5987 cg_residual = 0.0001 step_size = 0.4108 reward = 0.0000 fps = 3 mse_loss = 1.0862 
2022-07-08 09:54:29.408418 - gail/main.py:174 - [TRPO] iter = 32000 dist_mean = -0.0901 dist_std = 1.0178 vf_loss = 1.2166 grad_norm = 0.4585 nat_grad_norm = 0.8512 cg_residual = 0.0024 step_size = 0.3035 reward = 0.0000 fps = 3 mse_loss = 1.0479 
2022-07-08 09:54:49.656902 - gail/main.py:174 - [TRPO] iter = 33000 dist_mean = 0.1103 dist_std = 1.0217 vf_loss = 0.9871 grad_norm = 0.3649 nat_grad_norm = 0.6060 cg_residual = 0.0001 step_size = 0.3859 reward = 0.0000 fps = 3 mse_loss = 1.1601 
2022-07-08 09:55:09.574280 - gail/main.py:174 - [TRPO] iter = 34000 dist_mean = 0.1045 dist_std = 1.0166 vf_loss = 0.8172 grad_norm = 0.4214 nat_grad_norm = 0.5571 cg_residual = 0.0001 step_size = 0.3900 reward = 0.0000 fps = 2 mse_loss = 1.1570 
2022-07-08 09:55:29.313151 - gail/main.py:174 - [TRPO] iter = 35000 dist_mean = 0.1042 dist_std = 1.0133 vf_loss = 0.9728 grad_norm = 0.3330 nat_grad_norm = 0.5311 cg_residual = 0.0001 step_size = 0.4299 reward = -0.0000 fps = 2 mse_loss = 1.1221 
2022-07-08 09:55:30.072875 - gail/main.py:201 - [Discriminator] iter = 35000 loss = -0.5935 grad_norm = 20.3795 grad_penalty = 1.2482 regularization = 0.0000 true_logits = 0.4288 fake_logits = -1.4129 true_prob = 0.6011 fake_prob = 0.2009 
2022-07-08 09:59:19.457159 - gail/main.py:142 - [Evaluate] iter = 35000 episode={ returns = -33.4928 lengths = 1000 } discounted_episode={ returns = -21.0073 lengths = 1000 } 
2022-07-08 09:59:40.845033 - gail/main.py:174 - [TRPO] iter = 36000 dist_mean = 0.1275 dist_std = 1.0151 vf_loss = 2.7468 grad_norm = 0.3716 nat_grad_norm = 0.6467 cg_residual = 0.0001 step_size = 0.3593 reward = -0.0000 fps = 3 mse_loss = 1.1033 
2022-07-08 10:00:01.522485 - gail/main.py:174 - [TRPO] iter = 37000 dist_mean = 0.1173 dist_std = 1.0129 vf_loss = 0.6896 grad_norm = 0.3904 nat_grad_norm = 0.5693 cg_residual = 0.0001 step_size = 0.3958 reward = 0.0000 fps = 3 mse_loss = 1.1564 
2022-07-08 10:00:21.968286 - gail/main.py:174 - [TRPO] iter = 38000 dist_mean = 0.1204 dist_std = 1.0174 vf_loss = 0.4355 grad_norm = 0.5082 nat_grad_norm = 0.5706 cg_residual = 0.0001 step_size = 0.3470 reward = 0.0000 fps = 3 mse_loss = 1.1007 
2022-07-08 10:00:42.799772 - gail/main.py:174 - [TRPO] iter = 39000 dist_mean = 0.1397 dist_std = 1.0204 vf_loss = 0.4001 grad_norm = 0.4612 nat_grad_norm = 0.5855 cg_residual = 0.0001 step_size = 0.3586 reward = 0.0000 fps = 3 mse_loss = 0.9920 
2022-07-08 10:01:03.935434 - gail/main.py:174 - [TRPO] iter = 40000 dist_mean = 0.1456 dist_std = 1.0227 vf_loss = 0.5515 grad_norm = 0.4527 nat_grad_norm = 0.5435 cg_residual = 0.0001 step_size = 0.3910 reward = -0.0000 fps = 2 mse_loss = 1.0281 
2022-07-08 10:01:04.741196 - gail/main.py:201 - [Discriminator] iter = 40000 loss = -1.1275 grad_norm = 15.2255 grad_penalty = 0.9479 regularization = 0.0000 true_logits = 0.5212 fake_logits = -1.5542 true_prob = 0.6210 fake_prob = 0.1813 
2022-07-08 10:05:44.498326 - gail/main.py:142 - [Evaluate] iter = 40000 episode={ returns = -34.1951 lengths = 1000 } discounted_episode={ returns = -21.7724 lengths = 1000 } 
2022-07-08 10:06:06.852451 - gail/main.py:174 - [TRPO] iter = 41000 dist_mean = 0.1551 dist_std = 1.0181 vf_loss = 0.3698 grad_norm = 0.5068 nat_grad_norm = 0.5745 cg_residual = 0.0001 step_size = 0.3454 reward = 0.0000 fps = 3 mse_loss = 0.9402 
2022-07-08 10:06:30.229105 - gail/main.py:174 - [TRPO] iter = 42000 dist_mean = 0.1667 dist_std = 1.0137 vf_loss = 0.5348 grad_norm = 0.3802 nat_grad_norm = 0.5790 cg_residual = 0.0001 step_size = 0.3889 reward = 0.0000 fps = 3 mse_loss = 0.9016 
2022-07-08 10:06:53.723662 - gail/main.py:174 - [TRPO] iter = 43000 dist_mean = 0.3855 dist_std = 1.0122 vf_loss = 0.3970 grad_norm = 0.3657 nat_grad_norm = 0.7944 cg_residual = 0.0036 step_size = 0.3102 reward = 0.0000 fps = 2 mse_loss = 1.0559 
2022-07-08 10:07:16.383041 - gail/main.py:174 - [TRPO] iter = 44000 dist_mean = 0.2137 dist_std = 1.0122 vf_loss = 0.5646 grad_norm = 0.3842 nat_grad_norm = 0.5464 cg_residual = 0.0001 step_size = 0.4097 reward = 0.0000 fps = 2 mse_loss = 0.9620 
2022-07-08 10:07:38.588807 - gail/main.py:174 - [TRPO] iter = 45000 dist_mean = 0.2204 dist_std = 1.0104 vf_loss = 0.4237 grad_norm = 0.4770 nat_grad_norm = 0.5485 cg_residual = 0.0001 step_size = 0.3939 reward = -0.0000 fps = 2 mse_loss = 1.0136 
2022-07-08 10:07:39.382236 - gail/main.py:201 - [Discriminator] iter = 45000 loss = -1.5381 grad_norm = 13.5048 grad_penalty = 0.8845 regularization = 0.0000 true_logits = 0.6105 fake_logits = -1.8121 true_prob = 0.6391 fake_prob = 0.1479 
2022-07-08 10:12:22.384333 - gail/main.py:142 - [Evaluate] iter = 45000 episode={ returns = -33.3467 lengths = 1000 } discounted_episode={ returns = -20.8312 lengths = 1000 } 
2022-07-08 10:12:46.788428 - gail/main.py:174 - [TRPO] iter = 46000 dist_mean = 0.2291 dist_std = 1.0137 vf_loss = 0.4417 grad_norm = 0.4096 nat_grad_norm = 0.5569 cg_residual = 0.0001 step_size = 0.3971 reward = -0.0000 fps = 3 mse_loss = 1.0014 
2022-07-08 10:13:09.145389 - gail/main.py:174 - [TRPO] iter = 47000 dist_mean = 0.0323 dist_std = 1.0132 vf_loss = 0.3443 grad_norm = 0.5520 nat_grad_norm = 0.9630 cg_residual = 0.0038 step_size = 0.2728 reward = -0.0000 fps = 3 mse_loss = 1.1538 
2022-07-08 10:13:30.948709 - gail/main.py:174 - [TRPO] iter = 48000 dist_mean = 0.2324 dist_std = 1.0193 vf_loss = 0.5848 grad_norm = 0.3663 nat_grad_norm = 0.5649 cg_residual = 0.0002 step_size = 0.4119 reward = 0.0000 fps = 2 mse_loss = 1.0796 
2022-07-08 10:13:52.888561 - gail/main.py:174 - [TRPO] iter = 49000 dist_mean = 0.2317 dist_std = 1.0216 vf_loss = 0.5568 grad_norm = 0.4132 nat_grad_norm = 0.5796 cg_residual = 0.0003 step_size = 0.4018 reward = 0.0000 fps = 2 mse_loss = 1.0624 
2022-07-08 10:14:15.695348 - gail/main.py:174 - [TRPO] iter = 50000 dist_mean = 0.2381 dist_std = 1.0209 vf_loss = 0.4377 grad_norm = 0.4411 nat_grad_norm = 0.5880 cg_residual = 0.0002 step_size = 0.3853 reward = -0.0000 fps = 2 mse_loss = 1.0246 
2022-07-08 10:14:16.556983 - gail/main.py:201 - [Discriminator] iter = 50000 loss = -1.5074 grad_norm = 19.3714 grad_penalty = 0.9845 regularization = 0.0000 true_logits = 0.6667 fake_logits = -1.8252 true_prob = 0.6500 fake_prob = 0.1444 
2022-07-08 10:18:44.085538 - gail/main.py:142 - [Evaluate] iter = 50000 episode={ returns = -48.7324 lengths = 1000 } discounted_episode={ returns = -29.8998 lengths = 1000 } 
2022-07-08 10:19:04.227506 - gail/main.py:174 - [TRPO] iter = 51000 dist_mean = 0.2645 dist_std = 1.0218 vf_loss = 0.5650 grad_norm = 0.4244 nat_grad_norm = 0.5592 cg_residual = 0.0001 step_size = 0.3849 reward = -0.0000 fps = 3 mse_loss = 1.0329 
2022-07-08 10:19:26.650414 - gail/main.py:174 - [TRPO] iter = 52000 dist_mean = 0.2572 dist_std = 1.0198 vf_loss = 0.5545 grad_norm = 0.4299 nat_grad_norm = 0.5538 cg_residual = 0.0002 step_size = 0.4036 reward = -0.0000 fps = 3 mse_loss = 1.0814 
2022-07-08 10:19:49.607595 - gail/main.py:174 - [TRPO] iter = 53000 dist_mean = 0.2735 dist_std = 1.0219 vf_loss = 0.2840 grad_norm = 0.5379 nat_grad_norm = 0.6236 cg_residual = 0.0003 step_size = 0.3336 reward = -0.0000 fps = 3 mse_loss = 1.0514 
2022-07-08 10:20:11.901224 - gail/main.py:174 - [TRPO] iter = 54000 dist_mean = 0.2739 dist_std = 1.0253 vf_loss = 0.3985 grad_norm = 0.4405 nat_grad_norm = 0.6155 cg_residual = 0.0003 step_size = 0.3702 reward = -0.0000 fps = 2 mse_loss = 1.0410 
2022-07-08 10:20:34.921121 - gail/main.py:174 - [TRPO] iter = 55000 dist_mean = 0.2715 dist_std = 1.0276 vf_loss = 0.4056 grad_norm = 0.4407 nat_grad_norm = 0.5833 cg_residual = 0.0002 step_size = 0.3833 reward = 0.0000 fps = 2 mse_loss = 0.9731 
2022-07-08 10:20:35.698663 - gail/main.py:201 - [Discriminator] iter = 55000 loss = -1.8907 grad_norm = 19.5786 grad_penalty = 0.9540 regularization = 0.0000 true_logits = 0.7383 fake_logits = -2.1064 true_prob = 0.6645 fake_prob = 0.1135 
2022-07-08 10:25:07.364052 - gail/main.py:142 - [Evaluate] iter = 55000 episode={ returns = -61.2603 lengths = 1000 } discounted_episode={ returns = -35.3850 lengths = 1000 } 
2022-07-08 10:25:30.182669 - gail/main.py:174 - [TRPO] iter = 56000 dist_mean = 0.2864 dist_std = 1.0305 vf_loss = 0.3135 grad_norm = 0.4207 nat_grad_norm = 0.5822 cg_residual = 0.0003 step_size = 0.3905 reward = -0.0000 fps = 3 mse_loss = 1.0435 
2022-07-08 10:25:51.951479 - gail/main.py:174 - [TRPO] iter = 57000 dist_mean = 0.2899 dist_std = 1.0324 vf_loss = 0.5071 grad_norm = 0.4107 nat_grad_norm = 0.5510 cg_residual = 0.0002 step_size = 0.4007 reward = 0.0000 fps = 3 mse_loss = 1.0106 
2022-07-08 10:26:15.792426 - gail/main.py:174 - [TRPO] iter = 58000 dist_mean = 0.2898 dist_std = 1.0351 vf_loss = 0.3118 grad_norm = 0.4117 nat_grad_norm = 0.5517 cg_residual = 0.0003 step_size = 0.4117 reward = 0.0000 fps = 2 mse_loss = 1.0581 
2022-07-08 10:26:39.567937 - gail/main.py:174 - [TRPO] iter = 59000 dist_mean = 0.2946 dist_std = 1.0351 vf_loss = 0.3060 grad_norm = 0.5070 nat_grad_norm = 0.6147 cg_residual = 0.0003 step_size = 0.3563 reward = 0.0000 fps = 2 mse_loss = 0.9545 
2022-07-08 10:27:03.014487 - gail/main.py:174 - [TRPO] iter = 60000 dist_mean = 0.2892 dist_std = 1.0404 vf_loss = 0.3036 grad_norm = 0.4401 nat_grad_norm = 0.5820 cg_residual = 0.0004 step_size = 0.3924 reward = -0.0000 fps = 2 mse_loss = 0.9172 
2022-07-08 10:27:03.844584 - gail/main.py:201 - [Discriminator] iter = 60000 loss = -2.3826 grad_norm = 16.9095 grad_penalty = 0.7978 regularization = 0.0000 true_logits = 0.8491 fake_logits = -2.3312 true_prob = 0.6837 fake_prob = 0.0939 
2022-07-08 10:31:55.919080 - gail/main.py:142 - [Evaluate] iter = 60000 episode={ returns = -96.4857 lengths = 1000 } discounted_episode={ returns = -56.5462 lengths = 1000 } 
