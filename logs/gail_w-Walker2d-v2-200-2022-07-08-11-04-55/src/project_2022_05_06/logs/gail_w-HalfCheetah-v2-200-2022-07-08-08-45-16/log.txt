2022-07-08 08:45:16.996247 - utils/flags.py:257 - log_dir = logs/gail_w-HalfCheetah-v2-200-2022-07-08-08-45-16
2022-07-08 08:46:23.860754 - gail/utils/replay_buffer.py:86 - Load dataset from /content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/HalfCheetah-v2
2022-07-08 08:46:32.895047 - gail/main.py:80 - Expert Reward 6594.485577
2022-07-08 08:46:33.212061 - gail/main.py:84 - Original dataset size 3000
2022-07-08 08:46:33.246384 - gail/main.py:86 - Subsampled dataset size 3000
2022-07-08 08:46:33.249045 - gail/main.py:87 - np random: 864 random : 786
2022-07-08 08:46:33.250537 - gail/main.py:91 - Sampled obs: 0.6422, acs: 0.2570
2022-07-08 08:46:34.342044 - lunzi/nn/flat_param.py:17 - Enabling flattening... ['GaussianMLPPolicy_1/log_std:0', 'GaussianMLPPolicy_1/Variable:0', 'GaussianMLPPolicy_1/Variable_1:0', 'GaussianMLPPolicy_1/Variable_2:0', 'GaussianMLPPolicy_1/Variable_3:0', 'GaussianMLPPolicy_1/Variable_4:0', 'GaussianMLPPolicy_1/Variable_5:0']
2022-07-08 08:46:41.471704 - gail/discriminator/discriminator.py:30 - Use predefined normalization.
2022-07-08 08:46:41.484473 - gail/discriminator/discriminator.py:33 - Normalizer loc:[[-1.0330443e-01 -9.3026832e-03  2.9791275e-01  8.1040427e-02
   1.7489190e-01  4.0859219e-01 -3.0232381e-02 -9.5055901e-02
   7.1118622e+00  5.5424552e-03 -1.8441556e-01  3.1117105e-01
  -2.7393317e-01  2.6658252e-01  4.0665963e-01 -2.8198040e-01
   2.5822452e-01]] 
 scale:[[ 0.03102476  0.07177044  0.37663096  0.5276757   0.46577376  0.12057849
   0.35446092  0.3551628   1.290817    0.4731574   1.4939526  12.314076
  14.637607   14.150712    3.6817648  11.124868    9.962843  ]]
2022-07-08 08:46:45.034450 - gail/discriminator/discriminator.py:67 - Discriminator uses Wasserstein distance.
2022-07-08 08:46:45.036573 - gail/discriminator/discriminator.py:68 - [<tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable:0' shape=(23, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_1:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_2:0' shape=(100, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_3:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_4:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_5:0' shape=(1,) dtype=float32_ref>]
2022-07-08 08:46:45.038654 - gail/discriminator/discriminator.py:69 - Use gradient penalty regularization (coef = 10.000000)
2022-07-08 08:46:45.937868 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions_mean]
2022-07-08 08:47:54.728709 - gail/main.py:142 - [Evaluate] iter = 0 episode={ returns = -0.6418 lengths = 1000 } discounted_episode={ returns = 0.1100 lengths = 1000 } 
2022-07-08 08:47:54.732088 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions]
2022-07-08 08:48:02.743425 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [fake_states fake_actions] => [scaled_neural_reward]
2022-07-08 08:48:03.029580 - lunzi/nn/module.py:71 - [MLPVFunction] is making TensorFlow callables, key = [states] => [values]
2022-07-08 08:48:03.570178 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [] => [sync_old]
2022-07-08 08:48:03.813230 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss flat_grad dist_std mean_kl dist_mean]
2022-07-08 08:48:05.245900 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states tangents actions] => [hessian_vec_prod]
2022-07-08 08:48:07.819574 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [] => [get_flat]
2022-07-08 08:48:08.129811 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [feed_flat] => [set_flat]
2022-07-08 08:48:08.460037 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss mean_kl]
2022-07-08 08:48:08.979725 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [train_vf vf_loss]
2022-07-08 08:48:09.797119 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [vf_loss]
2022-07-08 08:48:10.085569 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states actions_] => [mse_loss]
2022-07-08 08:48:10.386048 - gail/main.py:174 - [TRPO] iter = 1000 dist_mean = -0.0000 dist_std = 1.0000 vf_loss = 0.4141 grad_norm = 0.2861 nat_grad_norm = 0.5188 cg_residual = 0.0000 step_size = 0.4310 reward = -0.0000 fps = 11 mse_loss = 0.9358 
2022-07-08 08:48:16.098252 - gail/main.py:174 - [TRPO] iter = 2000 dist_mean = 0.0087 dist_std = 1.0053 vf_loss = 1.6372 grad_norm = 0.3517 nat_grad_norm = 0.5130 cg_residual = 0.0000 step_size = 0.3746 reward = -0.0000 fps = 11 mse_loss = 0.9822 
2022-07-08 08:48:21.828192 - gail/main.py:174 - [TRPO] iter = 3000 dist_mean = 0.0030 dist_std = 1.0018 vf_loss = 2.1472 grad_norm = 0.3032 nat_grad_norm = 0.4694 cg_residual = 0.0000 step_size = 0.4389 reward = 0.0000 fps = 10 mse_loss = 0.9519 
2022-07-08 08:48:28.043640 - gail/main.py:174 - [TRPO] iter = 4000 dist_mean = -0.0095 dist_std = 1.0049 vf_loss = 2.1587 grad_norm = 0.3233 nat_grad_norm = 0.5197 cg_residual = 0.0000 step_size = 0.3919 reward = -0.0000 fps = 9 mse_loss = 0.9790 
2022-07-08 08:48:34.059060 - gail/main.py:174 - [TRPO] iter = 5000 dist_mean = -0.0356 dist_std = 1.0034 vf_loss = 2.8862 grad_norm = 0.3388 nat_grad_norm = 0.5097 cg_residual = 0.0000 step_size = 0.4144 reward = 0.0000 fps = 9 mse_loss = 1.0049 
2022-07-08 08:48:34.060089 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [true_states true_actions fake_states fake_actions true_masks] => [train loss true_logits fake_logits true_prob fake_prob grad_norm grad_penalty regularization]
2022-07-08 08:48:36.370280 - gail/main.py:201 - [Discriminator] iter = 5000 loss = 3.2285 grad_norm = 59.2099 grad_penalty = 3.0606 regularization = 0.0000 true_logits = 0.3494 fake_logits = 0.5172 true_prob = 0.5860 fake_prob = 0.6230 
2022-07-08 08:49:48.588903 - gail/main.py:142 - [Evaluate] iter = 5000 episode={ returns = -4.6817 lengths = 1000 } discounted_episode={ returns = -3.0303 lengths = 1000 } 
2022-07-08 08:49:54.440375 - gail/main.py:174 - [TRPO] iter = 6000 dist_mean = -0.0240 dist_std = 1.0106 vf_loss = 1.3900 grad_norm = 0.2931 nat_grad_norm = 0.6099 cg_residual = 0.0000 step_size = 0.3823 reward = 0.0000 fps = 12 mse_loss = 0.9997 
2022-07-08 08:50:00.485725 - gail/main.py:174 - [TRPO] iter = 7000 dist_mean = -0.0243 dist_std = 1.0100 vf_loss = 2.5572 grad_norm = 0.3050 nat_grad_norm = 0.6084 cg_residual = 0.0000 step_size = 0.3777 reward = -0.0000 fps = 11 mse_loss = 0.9796 
2022-07-08 08:50:10.983419 - gail/main.py:174 - [TRPO] iter = 8000 dist_mean = -0.0264 dist_std = 1.0076 vf_loss = 3.1389 grad_norm = 0.3330 nat_grad_norm = 0.6053 cg_residual = 0.0000 step_size = 0.3607 reward = 0.0000 fps = 10 mse_loss = 0.9699 
2022-07-08 08:50:17.594926 - gail/main.py:174 - [TRPO] iter = 9000 dist_mean = -0.0240 dist_std = 1.0043 vf_loss = 1.0427 grad_norm = 0.2558 nat_grad_norm = 0.5970 cg_residual = 0.0000 step_size = 0.4260 reward = 0.0000 fps = 9 mse_loss = 0.9951 
2022-07-08 08:50:24.115513 - gail/main.py:174 - [TRPO] iter = 10000 dist_mean = -0.0277 dist_std = 1.0014 vf_loss = 1.8329 grad_norm = 0.2910 nat_grad_norm = 0.5550 cg_residual = 0.0000 step_size = 0.4127 reward = -0.0000 fps = 9 mse_loss = 0.9423 
2022-07-08 08:50:24.352623 - gail/main.py:201 - [Discriminator] iter = 10000 loss = 2.6530 grad_norm = 88.6811 grad_penalty = 2.4638 regularization = 0.0000 true_logits = 0.3660 fake_logits = 0.5552 true_prob = 0.5900 fake_prob = 0.6121 
2022-07-08 08:51:33.367697 - gail/main.py:142 - [Evaluate] iter = 10000 episode={ returns = -6.8716 lengths = 1000 } discounted_episode={ returns = -4.4656 lengths = 1000 } 
2022-07-08 08:51:39.240123 - gail/main.py:174 - [TRPO] iter = 11000 dist_mean = -0.0260 dist_std = 1.0015 vf_loss = 0.4262 grad_norm = 0.3406 nat_grad_norm = 0.8236 cg_residual = 0.0000 step_size = 0.3187 reward = -0.0000 fps = 13 mse_loss = 0.9629 
2022-07-08 08:51:44.866229 - gail/main.py:174 - [TRPO] iter = 12000 dist_mean = -0.0410 dist_std = 1.0010 vf_loss = 0.2303 grad_norm = 0.3133 nat_grad_norm = 0.6312 cg_residual = 0.0000 step_size = 0.3747 reward = -0.0000 fps = 12 mse_loss = 1.0848 
2022-07-08 08:51:50.853280 - gail/main.py:174 - [TRPO] iter = 13000 dist_mean = -0.0427 dist_std = 1.0019 vf_loss = 0.2669 grad_norm = 0.2932 nat_grad_norm = 0.5889 cg_residual = 0.0000 step_size = 0.4030 reward = -0.0000 fps = 11 mse_loss = 1.1235 
2022-07-08 08:51:56.865768 - gail/main.py:174 - [TRPO] iter = 14000 dist_mean = -0.0660 dist_std = 0.9986 vf_loss = 1.0967 grad_norm = 0.3189 nat_grad_norm = 0.5316 cg_residual = 0.0000 step_size = 0.4204 reward = 0.0000 fps = 10 mse_loss = 1.1412 
2022-07-08 08:52:02.790660 - gail/main.py:174 - [TRPO] iter = 15000 dist_mean = -0.0320 dist_std = 1.0015 vf_loss = 0.2936 grad_norm = 0.3352 nat_grad_norm = 0.5151 cg_residual = 0.0000 step_size = 0.4181 reward = 0.0000 fps = 10 mse_loss = 1.1984 
2022-07-08 08:52:03.010577 - gail/main.py:201 - [Discriminator] iter = 15000 loss = -0.8407 grad_norm = 51.4457 grad_penalty = 2.0545 regularization = 0.0000 true_logits = 0.3722 fake_logits = -2.5229 true_prob = 0.5914 fake_prob = 0.0960 
2022-07-08 08:53:10.337260 - gail/main.py:142 - [Evaluate] iter = 15000 episode={ returns = -60.8179 lengths = 1000 } discounted_episode={ returns = -40.2974 lengths = 1000 } 
2022-07-08 08:53:16.092566 - gail/main.py:174 - [TRPO] iter = 16000 dist_mean = -0.0864 dist_std = 1.0041 vf_loss = 1.8560 grad_norm = 0.3647 nat_grad_norm = 0.6201 cg_residual = 0.0006 step_size = 0.3678 reward = 0.0000 fps = 13 mse_loss = 1.2406 
2022-07-08 08:53:21.962940 - gail/main.py:174 - [TRPO] iter = 17000 dist_mean = -0.0365 dist_std = 1.0079 vf_loss = 0.6551 grad_norm = 0.3346 nat_grad_norm = 0.6941 cg_residual = 0.0000 step_size = 0.3407 reward = -0.0000 fps = 12 mse_loss = 1.1828 
2022-07-08 08:53:28.291223 - gail/main.py:174 - [TRPO] iter = 18000 dist_mean = -0.0559 dist_std = 1.0092 vf_loss = 1.8232 grad_norm = 0.3336 nat_grad_norm = 0.6110 cg_residual = 0.0000 step_size = 0.3796 reward = -0.0000 fps = 11 mse_loss = 1.1187 
2022-07-08 08:53:34.583780 - gail/main.py:174 - [TRPO] iter = 19000 dist_mean = -0.0289 dist_std = 1.0100 vf_loss = 1.4518 grad_norm = 0.3555 nat_grad_norm = 0.8006 cg_residual = 0.0000 step_size = 0.2980 reward = 0.0000 fps = 10 mse_loss = 1.1197 
2022-07-08 08:53:40.335657 - gail/main.py:174 - [TRPO] iter = 20000 dist_mean = -0.0733 dist_std = 1.0085 vf_loss = 3.8328 grad_norm = 0.3101 nat_grad_norm = 0.6035 cg_residual = 0.0000 step_size = 0.4160 reward = 0.0000 fps = 10 mse_loss = 1.1207 
2022-07-08 08:53:40.575723 - gail/main.py:201 - [Discriminator] iter = 20000 loss = -0.5113 grad_norm = 27.3569 grad_penalty = 1.6425 regularization = 0.0000 true_logits = 0.4592 fake_logits = -1.6946 true_prob = 0.6119 fake_prob = 0.2288 
2022-07-08 08:54:47.164046 - gail/main.py:142 - [Evaluate] iter = 20000 episode={ returns = -25.3386 lengths = 1000 } discounted_episode={ returns = -15.5967 lengths = 1000 } 
2022-07-08 08:54:53.279473 - gail/main.py:174 - [TRPO] iter = 21000 dist_mean = -0.0301 dist_std = 1.0128 vf_loss = 2.0585 grad_norm = 0.3148 nat_grad_norm = 0.5895 cg_residual = 0.0000 step_size = 0.3921 reward = -0.0000 fps = 13 mse_loss = 1.0922 
2022-07-08 08:54:59.760402 - gail/main.py:174 - [TRPO] iter = 22000 dist_mean = -0.0699 dist_std = 1.0152 vf_loss = 4.8085 grad_norm = 0.3111 nat_grad_norm = 0.5994 cg_residual = 0.0000 step_size = 0.4035 reward = 0.0000 fps = 12 mse_loss = 1.1281 
2022-07-08 08:55:06.536418 - gail/main.py:174 - [TRPO] iter = 23000 dist_mean = -0.0174 dist_std = 1.0155 vf_loss = 1.1725 grad_norm = 0.3290 nat_grad_norm = 0.7889 cg_residual = 0.0000 step_size = 0.3398 reward = 0.0000 fps = 11 mse_loss = 1.0657 
2022-07-08 08:55:13.147228 - gail/main.py:174 - [TRPO] iter = 24000 dist_mean = -0.0662 dist_std = 1.0166 vf_loss = 3.5571 grad_norm = 0.3203 nat_grad_norm = 0.5250 cg_residual = 0.0000 step_size = 0.4207 reward = 0.0000 fps = 10 mse_loss = 1.0829 
2022-07-08 08:55:18.921814 - gail/main.py:174 - [TRPO] iter = 25000 dist_mean = -0.0794 dist_std = 1.0126 vf_loss = 7.0886 grad_norm = 0.3970 nat_grad_norm = 0.5752 cg_residual = 0.0001 step_size = 0.3858 reward = 0.0000 fps = 10 mse_loss = 1.0767 
2022-07-08 08:55:19.128371 - gail/main.py:201 - [Discriminator] iter = 25000 loss = -0.4800 grad_norm = 20.0757 grad_penalty = 1.2794 regularization = 0.0000 true_logits = 0.5141 fake_logits = -1.2453 true_prob = 0.6246 fake_prob = 0.2872 
2022-07-08 08:56:26.090108 - gail/main.py:142 - [Evaluate] iter = 25000 episode={ returns = -56.8373 lengths = 1000 } discounted_episode={ returns = -35.3050 lengths = 1000 } 
2022-07-08 08:56:32.176813 - gail/main.py:174 - [TRPO] iter = 26000 dist_mean = -0.0369 dist_std = 1.0160 vf_loss = 4.8016 grad_norm = 0.3486 nat_grad_norm = 0.6777 cg_residual = 0.0000 step_size = 0.3624 reward = 0.0000 fps = 13 mse_loss = 1.1096 
2022-07-08 08:56:38.570491 - gail/main.py:174 - [TRPO] iter = 27000 dist_mean = -0.1170 dist_std = 1.0130 vf_loss = 9.8445 grad_norm = 0.3750 nat_grad_norm = 0.5894 cg_residual = 0.0002 step_size = 0.3764 reward = 0.0000 fps = 12 mse_loss = 1.1774 
2022-07-08 08:56:44.264192 - gail/main.py:174 - [TRPO] iter = 28000 dist_mean = -0.1107 dist_std = 1.0162 vf_loss = 3.4319 grad_norm = 0.2922 nat_grad_norm = 0.5508 cg_residual = 0.0004 step_size = 0.4436 reward = 0.0000 fps = 11 mse_loss = 1.0981 
2022-07-08 08:56:49.762784 - gail/main.py:174 - [TRPO] iter = 29000 dist_mean = -0.1238 dist_std = 1.0147 vf_loss = 5.0379 grad_norm = 0.3054 nat_grad_norm = 0.5759 cg_residual = 0.0004 step_size = 0.4317 reward = 0.0000 fps = 11 mse_loss = 1.0942 
2022-07-08 08:56:55.688524 - gail/main.py:174 - [TRPO] iter = 30000 dist_mean = -0.0070 dist_std = 1.0192 vf_loss = 3.3210 grad_norm = 0.3411 nat_grad_norm = 0.6142 cg_residual = 0.0000 step_size = 0.3785 reward = -0.0000 fps = 10 mse_loss = 1.0909 
2022-07-08 08:56:55.917320 - gail/main.py:201 - [Discriminator] iter = 30000 loss = -1.9315 grad_norm = 25.5681 grad_penalty = 1.2411 regularization = 0.0000 true_logits = 0.5777 fake_logits = -2.5949 true_prob = 0.6390 fake_prob = 0.0899 
2022-07-08 08:58:02.654330 - gail/main.py:142 - [Evaluate] iter = 30000 episode={ returns = 17.0251 lengths = 1000 } discounted_episode={ returns = 10.5760 lengths = 1000 } 
2022-07-08 08:58:08.292393 - gail/main.py:174 - [TRPO] iter = 31000 dist_mean = -0.0150 dist_std = 1.0270 vf_loss = 3.1839 grad_norm = 0.3266 nat_grad_norm = 0.5777 cg_residual = 0.0000 step_size = 0.3901 reward = -0.0000 fps = 13 mse_loss = 1.1227 
2022-07-08 08:58:14.202168 - gail/main.py:174 - [TRPO] iter = 32000 dist_mean = -0.0330 dist_std = 1.0269 vf_loss = 5.5782 grad_norm = 0.3140 nat_grad_norm = 0.5685 cg_residual = 0.0000 step_size = 0.4239 reward = -0.0000 fps = 12 mse_loss = 1.0809 
2022-07-08 08:58:20.045792 - gail/main.py:174 - [TRPO] iter = 33000 dist_mean = -0.0992 dist_std = 1.0289 vf_loss = 4.8228 grad_norm = 0.3678 nat_grad_norm = 0.5714 cg_residual = 0.0002 step_size = 0.4058 reward = 0.0000 fps = 11 mse_loss = 1.1236 
2022-07-08 08:58:25.873238 - gail/main.py:174 - [TRPO] iter = 34000 dist_mean = -0.0510 dist_std = 1.0302 vf_loss = 2.0687 grad_norm = 0.3693 nat_grad_norm = 0.6430 cg_residual = 0.0004 step_size = 0.3805 reward = 0.0000 fps = 11 mse_loss = 1.0947 
2022-07-08 08:58:31.727062 - gail/main.py:174 - [TRPO] iter = 35000 dist_mean = -0.0897 dist_std = 1.0241 vf_loss = 3.7127 grad_norm = 0.3298 nat_grad_norm = 0.5750 cg_residual = 0.0001 step_size = 0.4112 reward = 0.0000 fps = 10 mse_loss = 1.1487 
2022-07-08 08:58:31.984465 - gail/main.py:201 - [Discriminator] iter = 35000 loss = 0.0243 grad_norm = 30.2598 grad_penalty = 1.4084 regularization = 0.0000 true_logits = 0.6174 fake_logits = -0.7667 true_prob = 0.6478 fake_prob = 0.3430 
2022-07-08 08:59:39.735551 - gail/main.py:142 - [Evaluate] iter = 35000 episode={ returns = 162.1926 lengths = 1000 } discounted_episode={ returns = 96.1086 lengths = 1000 } 
2022-07-08 08:59:45.630600 - gail/main.py:174 - [TRPO] iter = 36000 dist_mean = -0.0271 dist_std = 1.0272 vf_loss = 2.6562 grad_norm = 0.3226 nat_grad_norm = 0.5643 cg_residual = 0.0000 step_size = 0.4140 reward = 0.0000 fps = 13 mse_loss = 1.1758 
2022-07-08 08:59:51.248808 - gail/main.py:174 - [TRPO] iter = 37000 dist_mean = -0.0524 dist_std = 1.0254 vf_loss = 1.4240 grad_norm = 0.3810 nat_grad_norm = 0.7172 cg_residual = 0.0006 step_size = 0.3378 reward = 0.0000 fps = 12 mse_loss = 1.2621 
2022-07-08 08:59:56.972478 - gail/main.py:174 - [TRPO] iter = 38000 dist_mean = 0.0201 dist_std = 1.0314 vf_loss = 2.1474 grad_norm = 0.3741 nat_grad_norm = 0.7603 cg_residual = 0.0000 step_size = 0.3179 reward = 0.0000 fps = 11 mse_loss = 1.2820 
2022-07-08 09:00:03.010298 - gail/main.py:174 - [TRPO] iter = 39000 dist_mean = 0.0285 dist_std = 1.0295 vf_loss = 0.6608 grad_norm = 0.4024 nat_grad_norm = 0.7745 cg_residual = 0.0001 step_size = 0.3127 reward = -0.0000 fps = 10 mse_loss = 1.2558 
2022-07-08 09:00:09.953796 - gail/main.py:174 - [TRPO] iter = 40000 dist_mean = -0.0507 dist_std = 1.0329 vf_loss = 2.5836 grad_norm = 0.3553 nat_grad_norm = 0.5985 cg_residual = 0.0000 step_size = 0.3932 reward = 0.0000 fps = 10 mse_loss = 1.2950 
2022-07-08 09:00:10.214913 - gail/main.py:201 - [Discriminator] iter = 40000 loss = -1.1525 grad_norm = 21.9718 grad_penalty = 1.3343 regularization = 0.0000 true_logits = 0.6464 fake_logits = -1.8405 true_prob = 0.6537 fake_prob = 0.1989 
2022-07-08 09:01:25.263439 - gail/main.py:142 - [Evaluate] iter = 40000 episode={ returns = 356.5229 lengths = 1000 } discounted_episode={ returns = 223.8941 lengths = 1000 } 
2022-07-08 09:01:31.014812 - gail/main.py:174 - [TRPO] iter = 41000 dist_mean = -0.0245 dist_std = 1.0384 vf_loss = 5.7357 grad_norm = 0.3393 nat_grad_norm = 0.6239 cg_residual = 0.0000 step_size = 0.3883 reward = -0.0000 fps = 12 mse_loss = 1.3906 
2022-07-08 09:01:36.690844 - gail/main.py:174 - [TRPO] iter = 42000 dist_mean = -0.0526 dist_std = 1.0384 vf_loss = 9.1997 grad_norm = 0.3113 nat_grad_norm = 0.5990 cg_residual = 0.0000 step_size = 0.4148 reward = 0.0000 fps = 11 mse_loss = 1.4081 
2022-07-08 09:01:42.589689 - gail/main.py:174 - [TRPO] iter = 43000 dist_mean = -0.0132 dist_std = 1.0334 vf_loss = 4.0203 grad_norm = 0.2786 nat_grad_norm = 0.5468 cg_residual = 0.0000 step_size = 0.4816 reward = 0.0000 fps = 10 mse_loss = 1.4938 
2022-07-08 09:01:48.423912 - gail/main.py:174 - [TRPO] iter = 44000 dist_mean = 0.0170 dist_std = 1.0367 vf_loss = 2.4584 grad_norm = 0.3049 nat_grad_norm = 0.6305 cg_residual = 0.0011 step_size = 0.4001 reward = 0.0000 fps = 10 mse_loss = 1.5395 
2022-07-08 09:01:54.143625 - gail/main.py:174 - [TRPO] iter = 45000 dist_mean = 0.0237 dist_std = 1.0340 vf_loss = 6.1691 grad_norm = 0.3537 nat_grad_norm = 0.6847 cg_residual = 0.0000 step_size = 0.3759 reward = 0.0000 fps = 9 mse_loss = 1.5703 
2022-07-08 09:01:54.362890 - gail/main.py:201 - [Discriminator] iter = 45000 loss = -2.2016 grad_norm = 20.3683 grad_penalty = 1.1765 regularization = 0.0000 true_logits = 0.7290 fake_logits = -2.6492 true_prob = 0.6714 fake_prob = 0.1091 
2022-07-08 09:03:02.500309 - gail/main.py:142 - [Evaluate] iter = 45000 episode={ returns = 306.8668 lengths = 1000 } discounted_episode={ returns = 194.8634 lengths = 1000 } 
2022-07-08 09:03:08.664434 - gail/main.py:174 - [TRPO] iter = 46000 dist_mean = -0.0548 dist_std = 1.0311 vf_loss = 8.3197 grad_norm = 0.3244 nat_grad_norm = 0.5766 cg_residual = 0.0001 step_size = 0.4250 reward = 0.0000 fps = 13 mse_loss = 1.6330 
2022-07-08 09:03:14.466229 - gail/main.py:174 - [TRPO] iter = 47000 dist_mean = -0.0369 dist_std = 1.0385 vf_loss = 2.8471 grad_norm = 0.3710 nat_grad_norm = 0.6975 cg_residual = 0.0011 step_size = 0.3383 reward = 0.0000 fps = 12 mse_loss = 1.7711 
2022-07-08 09:03:19.958040 - gail/main.py:174 - [TRPO] iter = 48000 dist_mean = -0.0529 dist_std = 1.0382 vf_loss = 1.4429 grad_norm = 0.3577 nat_grad_norm = 0.6201 cg_residual = 0.0001 step_size = 0.3957 reward = 0.0000 fps = 11 mse_loss = 1.8224 
2022-07-08 09:03:25.989782 - gail/main.py:174 - [TRPO] iter = 49000 dist_mean = 0.0436 dist_std = 1.0404 vf_loss = 2.1715 grad_norm = 0.3275 nat_grad_norm = 0.6237 cg_residual = 0.0000 step_size = 0.3895 reward = 0.0000 fps = 10 mse_loss = 1.8796 
2022-07-08 09:03:32.399854 - gail/main.py:174 - [TRPO] iter = 50000 dist_mean = 0.0228 dist_std = 1.0384 vf_loss = 3.1236 grad_norm = 0.3255 nat_grad_norm = 0.6122 cg_residual = 0.0000 step_size = 0.4129 reward = 0.0000 fps = 10 mse_loss = 1.7930 
2022-07-08 09:03:32.694731 - gail/main.py:201 - [Discriminator] iter = 50000 loss = -2.5619 grad_norm = 15.2932 grad_penalty = 0.9944 regularization = 0.0000 true_logits = 0.8209 fake_logits = -2.7354 true_prob = 0.6908 fake_prob = 0.1124 
2022-07-08 09:05:01.510914 - gail/main.py:142 - [Evaluate] iter = 50000 episode={ returns = -10.5158 lengths = 1000 } discounted_episode={ returns = -5.4559 lengths = 1000 } 
2022-07-08 09:05:14.891157 - gail/main.py:174 - [TRPO] iter = 51000 dist_mean = -0.0143 dist_std = 1.0385 vf_loss = 5.6458 grad_norm = 0.4316 nat_grad_norm = 0.6731 cg_residual = 0.0001 step_size = 0.3445 reward = 0.0000 fps = 9 mse_loss = 1.8708 
2022-07-08 09:05:28.605260 - gail/main.py:174 - [TRPO] iter = 52000 dist_mean = 0.0335 dist_std = 1.0476 vf_loss = 8.3126 grad_norm = 0.3674 nat_grad_norm = 0.7518 cg_residual = 0.0000 step_size = 0.3380 reward = 0.0000 fps = 8 mse_loss = 1.8890 
2022-07-08 09:05:42.585152 - gail/main.py:174 - [TRPO] iter = 53000 dist_mean = 0.0323 dist_std = 1.0494 vf_loss = 1.6635 grad_norm = 0.3905 nat_grad_norm = 0.6819 cg_residual = 0.0002 step_size = 0.3456 reward = -0.0000 fps = 7 mse_loss = 1.9585 
2022-07-08 09:05:56.090643 - gail/main.py:174 - [TRPO] iter = 54000 dist_mean = 0.0298 dist_std = 1.0503 vf_loss = 1.2810 grad_norm = 0.3399 nat_grad_norm = 0.6250 cg_residual = 0.0003 step_size = 0.3728 reward = -0.0000 fps = 6 mse_loss = 2.1630 
2022-07-08 09:06:09.125637 - gail/main.py:174 - [TRPO] iter = 55000 dist_mean = -0.0300 dist_std = 1.0508 vf_loss = 2.9118 grad_norm = 0.3723 nat_grad_norm = 0.5928 cg_residual = 0.0001 step_size = 0.4060 reward = 0.0000 fps = 6 mse_loss = 2.0450 
2022-07-08 09:06:09.600808 - gail/main.py:201 - [Discriminator] iter = 55000 loss = -1.5895 grad_norm = 18.9950 grad_penalty = 1.1853 regularization = 0.0000 true_logits = 0.9502 fake_logits = -1.8246 true_prob = 0.7162 fake_prob = 0.1836 
2022-07-08 09:08:46.669153 - gail/main.py:142 - [Evaluate] iter = 55000 episode={ returns = -24.9582 lengths = 1000 } discounted_episode={ returns = -16.3155 lengths = 1000 } 
2022-07-08 09:09:00.221756 - gail/main.py:174 - [TRPO] iter = 56000 dist_mean = -0.0658 dist_std = 1.0503 vf_loss = 4.7005 grad_norm = 0.3944 nat_grad_norm = 0.7810 cg_residual = 0.0008 step_size = 0.3530 reward = -0.0000 fps = 5 mse_loss = 2.2479 
2022-07-08 09:09:19.329851 - gail/main.py:174 - [TRPO] iter = 57000 dist_mean = 0.0451 dist_std = 1.0528 vf_loss = 2.7114 grad_norm = 0.3908 nat_grad_norm = 0.7676 cg_residual = 0.0002 step_size = 0.3197 reward = 0.0000 fps = 5 mse_loss = 2.2251 
2022-07-08 09:09:38.653818 - gail/main.py:174 - [TRPO] iter = 58000 dist_mean = 0.0267 dist_std = 1.0566 vf_loss = 1.2706 grad_norm = 0.3707 nat_grad_norm = 0.6691 cg_residual = 0.0000 step_size = 0.3675 reward = 0.0000 fps = 4 mse_loss = 2.2996 
2022-07-08 09:09:55.185392 - gail/main.py:174 - [TRPO] iter = 59000 dist_mean = 0.0341 dist_std = 1.0567 vf_loss = 2.3063 grad_norm = 0.3293 nat_grad_norm = 0.6792 cg_residual = 0.0001 step_size = 0.3888 reward = -0.0000 fps = 4 mse_loss = 2.3944 
2022-07-08 09:10:13.202623 - gail/main.py:174 - [TRPO] iter = 60000 dist_mean = -0.0315 dist_std = 1.0542 vf_loss = 2.2479 grad_norm = 0.4484 nat_grad_norm = 0.7907 cg_residual = 0.0003 step_size = 0.3269 reward = -0.0000 fps = 4 mse_loss = 2.4027 
2022-07-08 09:10:13.974426 - gail/main.py:201 - [Discriminator] iter = 60000 loss = -1.8656 grad_norm = 12.5605 grad_penalty = 0.7667 regularization = 0.0000 true_logits = 1.0443 fake_logits = -1.5881 true_prob = 0.7344 fake_prob = 0.1980 
2022-07-08 09:15:03.346541 - gail/main.py:142 - [Evaluate] iter = 60000 episode={ returns = 51.4031 lengths = 1000 } discounted_episode={ returns = 30.5173 lengths = 1000 } 
2022-07-08 09:15:27.074443 - gail/main.py:174 - [TRPO] iter = 61000 dist_mean = 0.0778 dist_std = 1.0534 vf_loss = 3.5281 grad_norm = 0.3621 nat_grad_norm = 0.7092 cg_residual = 0.0004 step_size = 0.3588 reward = -0.0000 fps = 3 mse_loss = 2.2969 
2022-07-08 09:15:51.142417 - gail/main.py:174 - [TRPO] iter = 62000 dist_mean = -0.0899 dist_std = 1.0584 vf_loss = 4.3099 grad_norm = 0.4426 nat_grad_norm = 0.5649 cg_residual = 0.0005 step_size = 0.3948 reward = -0.0000 fps = 2 mse_loss = 2.3894 
2022-07-08 09:16:14.658934 - gail/main.py:174 - [TRPO] iter = 63000 dist_mean = -0.0873 dist_std = 1.0619 vf_loss = 2.0751 grad_norm = 0.4190 nat_grad_norm = 0.6806 cg_residual = 0.0004 step_size = 0.3459 reward = 0.0000 fps = 2 mse_loss = 2.4504 
2022-07-08 09:16:37.228099 - gail/main.py:174 - [TRPO] iter = 64000 dist_mean = 0.0860 dist_std = 1.0644 vf_loss = 1.5541 grad_norm = 0.3287 nat_grad_norm = 0.7104 cg_residual = 0.0003 step_size = 0.3696 reward = 0.0000 fps = 2 mse_loss = 2.2479 
2022-07-08 09:17:00.993641 - gail/main.py:174 - [TRPO] iter = 65000 dist_mean = -0.0649 dist_std = 1.0632 vf_loss = 2.1692 grad_norm = 0.4576 nat_grad_norm = 0.7017 cg_residual = 0.0005 step_size = 0.3287 reward = -0.0000 fps = 2 mse_loss = 2.2323 
2022-07-08 09:17:01.773974 - gail/main.py:201 - [Discriminator] iter = 65000 loss = -1.7858 grad_norm = 15.3684 grad_penalty = 0.8344 regularization = 0.0000 true_logits = 1.2067 fake_logits = -1.4136 true_prob = 0.7624 fake_prob = 0.2050 
2022-07-08 09:21:38.134068 - gail/main.py:142 - [Evaluate] iter = 65000 episode={ returns = -29.0521 lengths = 1000 } discounted_episode={ returns = -17.7355 lengths = 1000 } 
2022-07-08 09:22:06.358037 - gail/main.py:174 - [TRPO] iter = 66000 dist_mean = 0.0356 dist_std = 1.0623 vf_loss = 0.7525 grad_norm = 0.3175 nat_grad_norm = 0.5700 cg_residual = 0.0000 step_size = 0.4302 reward = 0.0000 fps = 3 mse_loss = 2.4071 
2022-07-08 09:22:29.062313 - gail/main.py:174 - [TRPO] iter = 67000 dist_mean = 0.0367 dist_std = 1.0622 vf_loss = 0.6332 grad_norm = 0.3388 nat_grad_norm = 0.6170 cg_residual = 0.0000 step_size = 0.3996 reward = 0.0000 fps = 3 mse_loss = 2.3480 
2022-07-08 09:22:51.264233 - gail/main.py:174 - [TRPO] iter = 68000 dist_mean = -0.0554 dist_std = 1.0667 vf_loss = 2.7488 grad_norm = 0.4356 nat_grad_norm = 0.6004 cg_residual = 0.0005 step_size = 0.3905 reward = -0.0000 fps = 2 mse_loss = 2.4374 
2022-07-08 09:23:14.192147 - gail/main.py:174 - [TRPO] iter = 69000 dist_mean = -0.0108 dist_std = 1.0708 vf_loss = 2.7046 grad_norm = 0.3881 nat_grad_norm = 0.7885 cg_residual = 0.0001 step_size = 0.3349 reward = 0.0000 fps = 2 mse_loss = 2.1560 
2022-07-08 09:23:36.838542 - gail/main.py:174 - [TRPO] iter = 70000 dist_mean = 0.0257 dist_std = 1.0674 vf_loss = 1.2304 grad_norm = 0.3260 nat_grad_norm = 0.5956 cg_residual = 0.0000 step_size = 0.4431 reward = -0.0000 fps = 2 mse_loss = 2.1993 
2022-07-08 09:23:37.713368 - gail/main.py:201 - [Discriminator] iter = 70000 loss = -3.0960 grad_norm = 16.5851 grad_penalty = 0.8603 regularization = 0.0000 true_logits = 1.3636 fake_logits = -2.5927 true_prob = 0.7879 fake_prob = 0.1059 
2022-07-08 09:27:59.592948 - gail/main.py:142 - [Evaluate] iter = 70000 episode={ returns = -53.5043 lengths = 1000 } discounted_episode={ returns = -33.5695 lengths = 1000 } 
2022-07-08 09:28:19.596527 - gail/main.py:174 - [TRPO] iter = 71000 dist_mean = 0.0597 dist_std = 1.0691 vf_loss = 1.8451 grad_norm = 0.3784 nat_grad_norm = 0.6469 cg_residual = 0.0002 step_size = 0.3703 reward = 0.0000 fps = 3 mse_loss = 2.2388 
2022-07-08 09:28:41.658275 - gail/main.py:174 - [TRPO] iter = 72000 dist_mean = -0.0397 dist_std = 1.0701 vf_loss = 1.9641 grad_norm = 0.4350 nat_grad_norm = 0.7126 cg_residual = 0.0004 step_size = 0.3538 reward = 0.0000 fps = 3 mse_loss = 2.3643 
2022-07-08 09:29:03.094637 - gail/main.py:174 - [TRPO] iter = 73000 dist_mean = -0.0448 dist_std = 1.0705 vf_loss = 1.3145 grad_norm = 0.4351 nat_grad_norm = 0.6569 cg_residual = 0.0001 step_size = 0.3882 reward = 0.0000 fps = 3 mse_loss = 2.2035 
2022-07-08 09:29:24.753085 - gail/main.py:174 - [TRPO] iter = 74000 dist_mean = -0.0198 dist_std = 1.0651 vf_loss = 1.0878 grad_norm = 0.6272 nat_grad_norm = 0.7042 cg_residual = 0.0006 step_size = 0.2963 reward = -0.0000 fps = 2 mse_loss = 2.2979 
2022-07-08 09:29:45.914294 - gail/main.py:174 - [TRPO] iter = 75000 dist_mean = 0.0557 dist_std = 1.0644 vf_loss = 0.5233 grad_norm = 0.3525 nat_grad_norm = 0.5983 cg_residual = 0.0000 step_size = 0.4119 reward = -0.0000 fps = 2 mse_loss = 2.3460 
2022-07-08 09:29:46.725493 - gail/main.py:201 - [Discriminator] iter = 75000 loss = -3.1912 grad_norm = 17.6258 grad_penalty = 0.9804 regularization = 0.0000 true_logits = 1.4770 fake_logits = -2.6946 true_prob = 0.8037 fake_prob = 0.1029 
2022-07-08 09:34:07.785093 - gail/main.py:142 - [Evaluate] iter = 75000 episode={ returns = -48.5185 lengths = 1000 } discounted_episode={ returns = -30.3311 lengths = 1000 } 
2022-07-08 09:34:28.483745 - gail/main.py:174 - [TRPO] iter = 76000 dist_mean = 0.0331 dist_std = 1.0730 vf_loss = 1.7283 grad_norm = 0.4220 nat_grad_norm = 0.6621 cg_residual = 0.0001 step_size = 0.3603 reward = 0.0000 fps = 3 mse_loss = 2.2728 
2022-07-08 09:34:49.071281 - gail/main.py:174 - [TRPO] iter = 77000 dist_mean = 0.1663 dist_std = 1.0776 vf_loss = 2.2600 grad_norm = 0.4967 nat_grad_norm = 0.7771 cg_residual = 0.0030 step_size = 0.2837 reward = 0.0000 fps = 3 mse_loss = 2.5422 
2022-07-08 09:35:09.620044 - gail/main.py:174 - [TRPO] iter = 78000 dist_mean = 0.0763 dist_std = 1.0768 vf_loss = 0.7858 grad_norm = 0.3463 nat_grad_norm = 0.6113 cg_residual = 0.0000 step_size = 0.4142 reward = -0.0000 fps = 3 mse_loss = 2.5296 
2022-07-08 09:35:30.064276 - gail/main.py:174 - [TRPO] iter = 79000 dist_mean = 0.0411 dist_std = 1.0790 vf_loss = 1.1161 grad_norm = 0.4438 nat_grad_norm = 0.6601 cg_residual = 0.0002 step_size = 0.3741 reward = -0.0000 fps = 2 mse_loss = 2.6157 
2022-07-08 09:35:50.864303 - gail/main.py:174 - [TRPO] iter = 80000 dist_mean = 0.1373 dist_std = 1.0798 vf_loss = 1.6394 grad_norm = 0.4384 nat_grad_norm = 0.6693 cg_residual = 0.0001 step_size = 0.3521 reward = 0.0000 fps = 2 mse_loss = 2.4441 
2022-07-08 09:35:51.646627 - gail/main.py:201 - [Discriminator] iter = 80000 loss = -3.9316 grad_norm = 16.7356 grad_penalty = 0.8092 regularization = 0.0000 true_logits = 1.5637 fake_logits = -3.1772 true_prob = 0.8144 fake_prob = 0.0738 
2022-07-08 09:39:57.585475 - gail/main.py:142 - [Evaluate] iter = 80000 episode={ returns = -57.3591 lengths = 1000 } discounted_episode={ returns = -35.4925 lengths = 1000 } 
2022-07-08 09:40:19.233649 - gail/main.py:174 - [TRPO] iter = 81000 dist_mean = 0.1494 dist_std = 1.0846 vf_loss = 2.6713 grad_norm = 0.3713 nat_grad_norm = 0.7750 cg_residual = 0.0001 step_size = 0.3650 reward = -0.0000 fps = 3 mse_loss = 2.4819 
2022-07-08 09:40:40.267033 - gail/main.py:174 - [TRPO] iter = 82000 dist_mean = 0.0667 dist_std = 1.0819 vf_loss = 1.9385 grad_norm = 0.3960 nat_grad_norm = 0.6973 cg_residual = 0.0002 step_size = 0.3619 reward = -0.0000 fps = 3 mse_loss = 2.7924 
2022-07-08 09:41:00.116632 - gail/main.py:174 - [TRPO] iter = 83000 dist_mean = 0.1179 dist_std = 1.0820 vf_loss = 3.1700 grad_norm = 0.3824 nat_grad_norm = 0.6551 cg_residual = 0.0001 step_size = 0.3922 reward = -0.0000 fps = 3 mse_loss = 2.6400 
2022-07-08 09:41:20.322516 - gail/main.py:174 - [TRPO] iter = 84000 dist_mean = 0.0955 dist_std = 1.0769 vf_loss = 1.5132 grad_norm = 0.3619 nat_grad_norm = 0.5676 cg_residual = 0.0002 step_size = 0.4599 reward = 0.0000 fps = 3 mse_loss = 2.6799 
2022-07-08 09:41:40.937230 - gail/main.py:174 - [TRPO] iter = 85000 dist_mean = 0.1332 dist_std = 1.0782 vf_loss = 2.8180 grad_norm = 0.3358 nat_grad_norm = 0.6352 cg_residual = 0.0001 step_size = 0.4197 reward = 0.0000 fps = 2 mse_loss = 2.8073 
2022-07-08 09:41:41.693243 - gail/main.py:201 - [Discriminator] iter = 85000 loss = -3.3325 grad_norm = 14.5952 grad_penalty = 0.7260 regularization = 0.0000 true_logits = 1.6360 fake_logits = -2.4225 true_prob = 0.8214 fake_prob = 0.1135 
2022-07-08 09:45:43.546052 - gail/main.py:142 - [Evaluate] iter = 85000 episode={ returns = -79.9085 lengths = 1000 } discounted_episode={ returns = -50.5189 lengths = 1000 } 
2022-07-08 09:46:02.602485 - gail/main.py:174 - [TRPO] iter = 86000 dist_mean = 0.1075 dist_std = 1.0782 vf_loss = 2.8145 grad_norm = 0.4134 nat_grad_norm = 0.8263 cg_residual = 0.0007 step_size = 0.3375 reward = -0.0000 fps = 3 mse_loss = 2.5981 
2022-07-08 09:46:22.239384 - gail/main.py:174 - [TRPO] iter = 87000 dist_mean = 0.1141 dist_std = 1.0804 vf_loss = 2.4193 grad_norm = 0.3963 nat_grad_norm = 0.6863 cg_residual = 0.0004 step_size = 0.3812 reward = -0.0000 fps = 3 mse_loss = 2.5642 
2022-07-08 09:46:42.060038 - gail/main.py:174 - [TRPO] iter = 88000 dist_mean = 0.1437 dist_std = 1.0788 vf_loss = 0.6438 grad_norm = 0.4406 nat_grad_norm = 0.7202 cg_residual = 0.0005 step_size = 0.3627 reward = 0.0000 fps = 3 mse_loss = 2.4273 
2022-07-08 09:47:01.887061 - gail/main.py:174 - [TRPO] iter = 89000 dist_mean = 0.1761 dist_std = 1.0820 vf_loss = 2.9405 grad_norm = 0.3681 nat_grad_norm = 0.7021 cg_residual = 0.0003 step_size = 0.3685 reward = 0.0000 fps = 3 mse_loss = 2.7978 
2022-07-08 09:47:21.503143 - gail/main.py:174 - [TRPO] iter = 90000 dist_mean = 0.1482 dist_std = 1.0866 vf_loss = 0.8509 grad_norm = 0.4495 nat_grad_norm = 0.7888 cg_residual = 0.0010 step_size = 0.3255 reward = 0.0000 fps = 2 mse_loss = 2.5717 
2022-07-08 09:47:22.229703 - gail/main.py:201 - [Discriminator] iter = 90000 loss = -3.2665 grad_norm = 13.8824 grad_penalty = 0.6262 regularization = 0.0000 true_logits = 1.8231 fake_logits = -2.0696 true_prob = 0.8429 fake_prob = 0.1220 
2022-07-08 09:51:21.952443 - gail/main.py:142 - [Evaluate] iter = 90000 episode={ returns = -98.8393 lengths = 1000 } discounted_episode={ returns = -62.1362 lengths = 1000 } 
2022-07-08 09:51:40.956517 - gail/main.py:174 - [TRPO] iter = 91000 dist_mean = 0.2562 dist_std = 1.0874 vf_loss = 3.1742 grad_norm = 0.4179 nat_grad_norm = 0.7460 cg_residual = 0.0004 step_size = 0.3462 reward = -0.0000 fps = 3 mse_loss = 2.4834 
2022-07-08 09:52:05.856604 - gail/main.py:174 - [TRPO] iter = 92000 dist_mean = 0.1636 dist_std = 1.0881 vf_loss = 0.6979 grad_norm = 0.5118 nat_grad_norm = 0.6796 cg_residual = 0.0007 step_size = 0.3451 reward = 0.0000 fps = 3 mse_loss = 2.5723 
2022-07-08 09:52:28.087281 - gail/main.py:174 - [TRPO] iter = 93000 dist_mean = 0.1656 dist_std = 1.0937 vf_loss = 0.5945 grad_norm = 0.6086 nat_grad_norm = 0.7807 cg_residual = 0.0009 step_size = 0.3020 reward = 0.0000 fps = 3 mse_loss = 2.5729 
2022-07-08 09:52:47.795031 - gail/main.py:174 - [TRPO] iter = 94000 dist_mean = 0.1905 dist_std = 1.0872 vf_loss = 0.7827 grad_norm = 0.5024 nat_grad_norm = 0.7729 cg_residual = 0.0005 step_size = 0.3229 reward = -0.0000 fps = 3 mse_loss = 2.5217 
2022-07-08 09:53:07.819910 - gail/main.py:174 - [TRPO] iter = 95000 dist_mean = 0.1998 dist_std = 1.0825 vf_loss = 0.3888 grad_norm = 0.6652 nat_grad_norm = 0.7232 cg_residual = 0.0008 step_size = 0.3142 reward = 0.0000 fps = 2 mse_loss = 2.3251 
2022-07-08 09:53:08.549703 - gail/main.py:201 - [Discriminator] iter = 95000 loss = -3.6362 grad_norm = 11.8206 grad_penalty = 0.5964 regularization = 0.0000 true_logits = 2.0540 fake_logits = -2.1786 true_prob = 0.8628 fake_prob = 0.1118 
2022-07-08 09:57:05.207653 - gail/main.py:142 - [Evaluate] iter = 95000 episode={ returns = -140.9254 lengths = 1000 } discounted_episode={ returns = -88.5145 lengths = 1000 } 
2022-07-08 09:57:24.586616 - gail/main.py:174 - [TRPO] iter = 96000 dist_mean = 0.2648 dist_std = 1.0820 vf_loss = 2.7129 grad_norm = 0.4246 nat_grad_norm = 0.6975 cg_residual = 0.0003 step_size = 0.3710 reward = -0.0000 fps = 3 mse_loss = 2.4130 
2022-07-08 09:57:43.253619 - gail/main.py:174 - [TRPO] iter = 97000 dist_mean = 0.2260 dist_std = 1.0864 vf_loss = 0.5173 grad_norm = 0.5460 nat_grad_norm = 0.7602 cg_residual = 0.0004 step_size = 0.3336 reward = 0.0000 fps = 3 mse_loss = 2.4713 
2022-07-08 09:58:02.252144 - gail/main.py:174 - [TRPO] iter = 98000 dist_mean = 0.2314 dist_std = 1.0892 vf_loss = 0.4585 grad_norm = 0.5281 nat_grad_norm = 0.6398 cg_residual = 0.0005 step_size = 0.3528 reward = -0.0000 fps = 3 mse_loss = 2.4111 
2022-07-08 09:58:21.635245 - gail/main.py:174 - [TRPO] iter = 99000 dist_mean = 0.2516 dist_std = 1.0893 vf_loss = 0.3418 grad_norm = 0.6402 nat_grad_norm = 0.6709 cg_residual = 0.0008 step_size = 0.3291 reward = -0.0000 fps = 3 mse_loss = 2.1918 
2022-07-08 09:58:42.648907 - gail/main.py:174 - [TRPO] iter = 100000 dist_mean = 0.2610 dist_std = 1.0867 vf_loss = 0.3129 grad_norm = 0.5849 nat_grad_norm = 0.6337 cg_residual = 0.0005 step_size = 0.3461 reward = -0.0000 fps = 2 mse_loss = 2.0518 
2022-07-08 09:58:43.432897 - gail/main.py:201 - [Discriminator] iter = 100000 loss = -3.9273 grad_norm = 13.9198 grad_penalty = 0.5934 regularization = 0.0000 true_logits = 2.2188 fake_logits = -2.3018 true_prob = 0.8767 fake_prob = 0.1002 
2022-07-08 10:03:06.720169 - gail/main.py:142 - [Evaluate] iter = 100000 episode={ returns = -278.2846 lengths = 1000 } discounted_episode={ returns = -174.4393 lengths = 1000 } 
2022-07-08 10:03:28.495832 - gail/main.py:174 - [TRPO] iter = 101000 dist_mean = 0.2815 dist_std = 1.0956 vf_loss = 0.4247 grad_norm = 0.6167 nat_grad_norm = 0.7402 cg_residual = 0.0013 step_size = 0.3146 reward = 0.0000 fps = 3 mse_loss = 2.2074 
2022-07-08 10:03:51.511520 - gail/main.py:174 - [TRPO] iter = 102000 dist_mean = 0.3433 dist_std = 1.0903 vf_loss = 3.1822 grad_norm = 0.3788 nat_grad_norm = 0.7361 cg_residual = 0.0003 step_size = 0.3786 reward = -0.0000 fps = 3 mse_loss = 2.1808 
2022-07-08 10:04:14.913355 - gail/main.py:174 - [TRPO] iter = 103000 dist_mean = 0.3174 dist_std = 1.0904 vf_loss = 0.4227 grad_norm = 0.5011 nat_grad_norm = 0.8565 cg_residual = 0.0008 step_size = 0.3092 reward = -0.0000 fps = 3 mse_loss = 2.2371 
2022-07-08 10:04:36.705093 - gail/main.py:174 - [TRPO] iter = 104000 dist_mean = 0.3157 dist_std = 1.0991 vf_loss = 0.2359 grad_norm = 0.5371 nat_grad_norm = 0.7449 cg_residual = 0.0008 step_size = 0.3292 reward = 0.0000 fps = 2 mse_loss = 2.3475 
2022-07-08 10:04:59.443355 - gail/main.py:174 - [TRPO] iter = 105000 dist_mean = 0.3305 dist_std = 1.0964 vf_loss = 0.3165 grad_norm = 0.6879 nat_grad_norm = 0.6585 cg_residual = 0.0013 step_size = 0.3226 reward = 0.0000 fps = 2 mse_loss = 2.3091 
2022-07-08 10:05:00.168508 - gail/main.py:201 - [Discriminator] iter = 105000 loss = -4.3865 grad_norm = 12.2841 grad_penalty = 0.6295 regularization = 0.0000 true_logits = 2.3967 fake_logits = -2.6193 true_prob = 0.8890 fake_prob = 0.0771 
2022-07-08 10:09:37.430816 - gail/main.py:142 - [Evaluate] iter = 105000 episode={ returns = -366.5159 lengths = 1000 } discounted_episode={ returns = -230.2808 lengths = 1000 } 
2022-07-08 10:10:01.228290 - gail/main.py:174 - [TRPO] iter = 106000 dist_mean = 0.4119 dist_std = 1.0941 vf_loss = 2.9257 grad_norm = 0.4465 nat_grad_norm = 0.7563 cg_residual = 0.0018 step_size = 0.3366 reward = -0.0000 fps = 3 mse_loss = 2.1938 
2022-07-08 10:10:23.728362 - gail/main.py:174 - [TRPO] iter = 107000 dist_mean = 0.3562 dist_std = 1.0967 vf_loss = 0.2509 grad_norm = 0.4693 nat_grad_norm = 0.6420 cg_residual = 0.0013 step_size = 0.3711 reward = 0.0000 fps = 3 mse_loss = 2.2769 
2022-07-08 10:10:45.227326 - gail/main.py:174 - [TRPO] iter = 108000 dist_mean = 0.3584 dist_std = 1.0900 vf_loss = 2.9429 grad_norm = 0.4354 nat_grad_norm = 0.7514 cg_residual = 0.0006 step_size = 0.3475 reward = -0.0000 fps = 2 mse_loss = 2.3699 
2022-07-08 10:11:07.706402 - gail/main.py:174 - [TRPO] iter = 109000 dist_mean = 0.3586 dist_std = 1.0897 vf_loss = 0.5255 grad_norm = 0.5519 nat_grad_norm = 0.8190 cg_residual = 0.0025 step_size = 0.2989 reward = 0.0000 fps = 2 mse_loss = 2.3553 
2022-07-08 10:11:31.272783 - gail/main.py:174 - [TRPO] iter = 110000 dist_mean = 0.3976 dist_std = 1.0891 vf_loss = 3.0411 grad_norm = 0.4337 nat_grad_norm = 0.6592 cg_residual = 0.0005 step_size = 0.3772 reward = -0.0000 fps = 2 mse_loss = 2.1168 
2022-07-08 10:11:32.124335 - gail/main.py:201 - [Discriminator] iter = 110000 loss = -5.5297 grad_norm = 19.1933 grad_penalty = 0.9583 regularization = 0.0000 true_logits = 2.4615 fake_logits = -4.0265 true_prob = 0.8842 fake_prob = 0.0317 
2022-07-08 10:16:15.110960 - gail/main.py:142 - [Evaluate] iter = 110000 episode={ returns = -389.5224 lengths = 1000 } discounted_episode={ returns = -244.2882 lengths = 1000 } 
2022-07-08 10:16:36.996405 - gail/main.py:174 - [TRPO] iter = 111000 dist_mean = 0.3681 dist_std = 1.0858 vf_loss = 0.4334 grad_norm = 0.4255 nat_grad_norm = 0.8465 cg_residual = 0.0017 step_size = 0.3159 reward = 0.0000 fps = 3 mse_loss = 2.1884 
2022-07-08 10:16:58.741392 - gail/main.py:174 - [TRPO] iter = 112000 dist_mean = 0.3855 dist_std = 1.0839 vf_loss = 0.3231 grad_norm = 0.4939 nat_grad_norm = 0.7017 cg_residual = 0.0018 step_size = 0.3544 reward = -0.0000 fps = 3 mse_loss = 2.0560 
2022-07-08 10:17:20.781819 - gail/main.py:174 - [TRPO] iter = 113000 dist_mean = 0.4006 dist_std = 1.0853 vf_loss = 0.3334 grad_norm = 0.5621 nat_grad_norm = 0.7626 cg_residual = 0.0017 step_size = 0.3418 reward = 0.0000 fps = 2 mse_loss = 1.9552 
2022-07-08 10:17:43.210739 - gail/main.py:174 - [TRPO] iter = 114000 dist_mean = 0.3774 dist_std = 1.0819 vf_loss = 0.3217 grad_norm = 0.5278 nat_grad_norm = 0.6607 cg_residual = 0.0017 step_size = 0.3611 reward = -0.0000 fps = 2 mse_loss = 1.9372 
2022-07-08 10:18:04.836070 - gail/main.py:174 - [TRPO] iter = 115000 dist_mean = 0.4004 dist_std = 1.0807 vf_loss = 0.2065 grad_norm = 0.5848 nat_grad_norm = 0.6729 cg_residual = 0.0020 step_size = 0.3442 reward = -0.0000 fps = 2 mse_loss = 1.9532 
2022-07-08 10:18:05.577746 - gail/main.py:201 - [Discriminator] iter = 115000 loss = -5.1405 grad_norm = 17.1966 grad_penalty = 0.6547 regularization = 0.0000 true_logits = 2.5050 fake_logits = -3.2903 true_prob = 0.8906 fake_prob = 0.0424 
2022-07-08 10:22:39.781511 - gail/main.py:142 - [Evaluate] iter = 115000 episode={ returns = -402.7887 lengths = 1000 } discounted_episode={ returns = -252.9549 lengths = 1000 } 
2022-07-08 10:23:02.765112 - gail/main.py:174 - [TRPO] iter = 116000 dist_mean = 0.3967 dist_std = 1.0875 vf_loss = 0.2457 grad_norm = 0.4668 nat_grad_norm = 0.6437 cg_residual = 0.0018 step_size = 0.3899 reward = 0.0000 fps = 3 mse_loss = 1.7806 
2022-07-08 10:23:25.127470 - gail/main.py:174 - [TRPO] iter = 117000 dist_mean = 0.3810 dist_std = 1.0868 vf_loss = 0.2871 grad_norm = 0.5175 nat_grad_norm = 0.7096 cg_residual = 0.0019 step_size = 0.3464 reward = 0.0000 fps = 3 mse_loss = 1.8347 
2022-07-08 10:23:47.725632 - gail/main.py:174 - [TRPO] iter = 118000 dist_mean = 0.3812 dist_std = 1.0811 vf_loss = 0.2313 grad_norm = 0.5149 nat_grad_norm = 0.6893 cg_residual = 0.0027 step_size = 0.3576 reward = 0.0000 fps = 2 mse_loss = 1.8246 
2022-07-08 10:24:10.278947 - gail/main.py:174 - [TRPO] iter = 119000 dist_mean = 0.3684 dist_std = 1.0815 vf_loss = 0.2191 grad_norm = 0.4181 nat_grad_norm = 0.7060 cg_residual = 0.0013 step_size = 0.3710 reward = 0.0000 fps = 2 mse_loss = 1.7869 
2022-07-08 10:24:32.468834 - gail/main.py:174 - [TRPO] iter = 120000 dist_mean = 0.3729 dist_std = 1.0849 vf_loss = 0.2849 grad_norm = 0.5201 nat_grad_norm = 0.6758 cg_residual = 0.0017 step_size = 0.3670 reward = -0.0000 fps = 2 mse_loss = 1.6011 
2022-07-08 10:24:33.284642 - gail/main.py:201 - [Discriminator] iter = 120000 loss = -5.4737 grad_norm = 19.2659 grad_penalty = 0.7431 regularization = 0.0000 true_logits = 2.6030 fake_logits = -3.6138 true_prob = 0.9009 fake_prob = 0.0311 
2022-07-08 10:29:13.193646 - gail/main.py:142 - [Evaluate] iter = 120000 episode={ returns = -326.4431 lengths = 1000 } discounted_episode={ returns = -207.6537 lengths = 1000 } 
2022-07-08 10:29:37.213032 - gail/main.py:174 - [TRPO] iter = 121000 dist_mean = 0.3591 dist_std = 1.0879 vf_loss = 0.4128 grad_norm = 0.5034 nat_grad_norm = 0.7270 cg_residual = 0.0018 step_size = 0.3513 reward = -0.0000 fps = 3 mse_loss = 1.5429 
2022-07-08 10:30:01.371080 - gail/main.py:174 - [TRPO] iter = 122000 dist_mean = 0.3198 dist_std = 1.0890 vf_loss = 0.2246 grad_norm = 0.4650 nat_grad_norm = 0.6835 cg_residual = 0.0016 step_size = 0.3733 reward = -0.0000 fps = 3 mse_loss = 1.6444 
2022-07-08 10:30:25.437004 - gail/main.py:174 - [TRPO] iter = 123000 dist_mean = 0.3178 dist_std = 1.0849 vf_loss = 0.2775 grad_norm = 0.5314 nat_grad_norm = 0.7069 cg_residual = 0.0016 step_size = 0.3444 reward = -0.0000 fps = 2 mse_loss = 1.5502 
2022-07-08 10:30:49.868198 - gail/main.py:174 - [TRPO] iter = 124000 dist_mean = 0.3105 dist_std = 1.0851 vf_loss = 0.2397 grad_norm = 0.4174 nat_grad_norm = 0.7325 cg_residual = 0.0013 step_size = 0.3767 reward = -0.0000 fps = 2 mse_loss = 1.4887 
2022-07-08 10:31:12.909741 - gail/main.py:174 - [TRPO] iter = 125000 dist_mean = 0.3126 dist_std = 1.0877 vf_loss = 0.3652 grad_norm = 0.4276 nat_grad_norm = 0.7131 cg_residual = 0.0017 step_size = 0.3638 reward = 0.0000 fps = 2 mse_loss = 1.4913 
2022-07-08 10:31:13.737698 - gail/main.py:201 - [Discriminator] iter = 125000 loss = -6.0303 grad_norm = 21.4490 grad_penalty = 0.8034 regularization = 0.0000 true_logits = 2.6570 fake_logits = -4.1767 true_prob = 0.9004 fake_prob = 0.0189 
2022-07-08 10:33:20.818427 - gail/main.py:142 - [Evaluate] iter = 125000 episode={ returns = -267.5156 lengths = 1000 } discounted_episode={ returns = -167.8569 lengths = 1000 } 
2022-07-08 10:33:29.055255 - gail/main.py:174 - [TRPO] iter = 126000 dist_mean = 0.2990 dist_std = 1.0854 vf_loss = 0.4686 grad_norm = 0.4020 nat_grad_norm = 0.6934 cg_residual = 0.0024 step_size = 0.3928 reward = -0.0000 fps = 7 mse_loss = 1.5088 
2022-07-08 10:33:36.795160 - gail/main.py:174 - [TRPO] iter = 127000 dist_mean = 0.2992 dist_std = 1.0820 vf_loss = 0.3768 grad_norm = 0.5206 nat_grad_norm = 0.7560 cg_residual = 0.0020 step_size = 0.3551 reward = 0.0000 fps = 6 mse_loss = 1.4760 
2022-07-08 10:33:44.891172 - gail/main.py:174 - [TRPO] iter = 128000 dist_mean = 0.2732 dist_std = 1.0855 vf_loss = 0.5260 grad_norm = 0.4189 nat_grad_norm = 0.7275 cg_residual = 0.0017 step_size = 0.3852 reward = -0.0000 fps = 6 mse_loss = 1.5677 
2022-07-08 10:33:52.850890 - gail/main.py:174 - [TRPO] iter = 129000 dist_mean = 0.2625 dist_std = 1.0883 vf_loss = 0.3969 grad_norm = 0.5018 nat_grad_norm = 0.7345 cg_residual = 0.0027 step_size = 0.3425 reward = 0.0000 fps = 6 mse_loss = 1.5398 
