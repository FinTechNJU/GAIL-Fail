2022-07-08 09:09:32.684183 - utils/flags.py:257 - log_dir = logs/gail_w-Walker2d-v2-100-2022-07-08-09-09-32
2022-07-08 09:09:56.529569 - gail/utils/replay_buffer.py:86 - Load dataset from /content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/Walker2d-v2
2022-07-08 09:10:25.412405 - gail/main.py:80 - Expert Reward 5150.674112
2022-07-08 09:10:26.664628 - gail/main.py:84 - Original dataset size 3000
2022-07-08 09:10:26.813392 - gail/main.py:86 - Subsampled dataset size 3000
2022-07-08 09:10:26.843414 - gail/main.py:87 - np random: 864 random : 786
2022-07-08 09:10:26.846182 - gail/main.py:91 - Sampled obs: 0.0531, acs: 0.2269
2022-07-08 09:10:30.509364 - lunzi/nn/flat_param.py:17 - Enabling flattening... ['GaussianMLPPolicy_1/log_std:0', 'GaussianMLPPolicy_1/Variable:0', 'GaussianMLPPolicy_1/Variable_1:0', 'GaussianMLPPolicy_1/Variable_2:0', 'GaussianMLPPolicy_1/Variable_3:0', 'GaussianMLPPolicy_1/Variable_4:0', 'GaussianMLPPolicy_1/Variable_5:0']
2022-07-08 09:11:12.353067 - gail/discriminator/discriminator.py:30 - Use predefined normalization.
2022-07-08 09:11:12.404184 - gail/discriminator/discriminator.py:33 - Normalizer loc:[[ 1.2194959e+00  2.4445076e-01 -7.8132987e-02 -2.6673764e-01
   1.8222688e-01 -9.5077172e-02 -3.3649772e-01  5.3370733e-02
   4.1614923e+00  4.1431887e-03  3.8142569e-02 -2.6013174e-03
  -1.0202496e-02  5.6982285e-01  2.9836079e-02 -1.5763690e-01
   1.7689442e-02]] 
 scale:[[0.06687175 0.23681822 0.23042987 0.33821535 0.664349   0.20301929
  0.42807332 0.7138035  0.986894   0.65049744 2.0363257  2.3816926
  3.7250905  6.026913   2.0511289  4.406521   6.1475325 ]]
2022-07-08 09:11:31.532255 - gail/discriminator/discriminator.py:67 - Discriminator uses Wasserstein distance.
2022-07-08 09:11:31.542892 - gail/discriminator/discriminator.py:68 - [<tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable:0' shape=(23, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_1:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_2:0' shape=(100, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_3:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_4:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_5:0' shape=(1,) dtype=float32_ref>]
2022-07-08 09:11:31.549331 - gail/discriminator/discriminator.py:69 - Use gradient penalty regularization (coef = 10.000000)
2022-07-08 09:11:35.501619 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions_mean]
2022-07-08 09:12:56.377644 - gail/main.py:142 - [Evaluate] iter = 0 episode={ returns = 154.4648 lengths = 185 } discounted_episode={ returns = 152.4866 lengths = 170 } 
2022-07-08 09:12:56.387066 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions]
2022-07-08 09:13:35.447825 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [fake_states fake_actions] => [scaled_neural_reward]
2022-07-08 09:13:36.648169 - lunzi/nn/module.py:71 - [MLPVFunction] is making TensorFlow callables, key = [states] => [values]
2022-07-08 09:13:38.725076 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [] => [sync_old]
2022-07-08 09:13:39.624053 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss flat_grad dist_std mean_kl dist_mean]
2022-07-08 09:13:45.662414 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states tangents actions] => [hessian_vec_prod]
2022-07-08 09:13:56.332031 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [] => [get_flat]
2022-07-08 09:13:57.447724 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [feed_flat] => [set_flat]
2022-07-08 09:13:58.490247 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss mean_kl]
2022-07-08 09:14:00.282395 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [train_vf vf_loss]
2022-07-08 09:14:03.014695 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [vf_loss]
2022-07-08 09:14:04.018100 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states actions_] => [mse_loss]
2022-07-08 09:14:05.065027 - gail/main.py:174 - [TRPO] iter = 1000 dist_mean = -0.0000 dist_std = 1.0000 vf_loss = 0.6732 grad_norm = 0.3302 nat_grad_norm = 0.4330 cg_residual = 0.0000 step_size = 0.4424 reward = -0.0000 fps = 6 mse_loss = 0.3947 
2022-07-08 09:14:34.347873 - gail/main.py:174 - [TRPO] iter = 2000 dist_mean = -0.0054 dist_std = 0.9958 vf_loss = 0.6101 grad_norm = 0.4075 nat_grad_norm = 0.4600 cg_residual = 0.0000 step_size = 0.3874 reward = 0.0000 fps = 5 mse_loss = 0.4196 
2022-07-08 09:15:05.670654 - gail/main.py:174 - [TRPO] iter = 3000 dist_mean = -0.0084 dist_std = 0.9990 vf_loss = 0.8399 grad_norm = 0.4533 nat_grad_norm = 0.5118 cg_residual = 0.0000 step_size = 0.3425 reward = -0.0000 fps = 4 mse_loss = 0.4391 
2022-07-08 09:15:35.297729 - gail/main.py:174 - [TRPO] iter = 4000 dist_mean = -0.0088 dist_std = 0.9946 vf_loss = 0.6656 grad_norm = 0.4738 nat_grad_norm = 0.4880 cg_residual = 0.0001 step_size = 0.3526 reward = 0.0000 fps = 4 mse_loss = 0.4331 
2022-07-08 09:16:05.035684 - gail/main.py:174 - [TRPO] iter = 5000 dist_mean = -0.0022 dist_std = 0.9907 vf_loss = 0.7723 grad_norm = 0.5320 nat_grad_norm = 0.4963 cg_residual = 0.0001 step_size = 0.3217 reward = -0.0000 fps = 3 mse_loss = 0.4623 
2022-07-08 09:16:05.038247 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [true_states true_actions fake_states fake_actions true_masks] => [train loss true_logits fake_logits true_prob fake_prob grad_norm grad_penalty regularization]
2022-07-08 09:16:13.706280 - gail/main.py:201 - [Discriminator] iter = 5000 loss = 1.8659 grad_norm = 13.8695 grad_penalty = 1.5255 regularization = 0.0000 true_logits = -0.0193 fake_logits = 0.3212 true_prob = 0.4952 fake_prob = 0.5761 
2022-07-08 09:16:20.581058 - gail/main.py:142 - [Evaluate] iter = 5000 episode={ returns = -7.1408 lengths = 19 } discounted_episode={ returns = -7.0578 lengths = 19 } 
2022-07-08 09:16:50.947092 - gail/main.py:174 - [TRPO] iter = 6000 dist_mean = -0.0121 dist_std = 0.9852 vf_loss = 0.7109 grad_norm = 0.5759 nat_grad_norm = 0.5205 cg_residual = 0.0001 step_size = 0.3021 reward = 0.0000 fps = 26 mse_loss = 0.4968 
2022-07-08 09:17:20.742978 - gail/main.py:174 - [TRPO] iter = 7000 dist_mean = 0.0041 dist_std = 0.9777 vf_loss = 0.3959 grad_norm = 0.5019 nat_grad_norm = 0.4932 cg_residual = 0.0002 step_size = 0.3491 reward = -0.0000 fps = 14 mse_loss = 0.5500 
2022-07-08 09:17:50.136734 - gail/main.py:174 - [TRPO] iter = 8000 dist_mean = 0.0182 dist_std = 0.9747 vf_loss = 0.3301 grad_norm = 0.5295 nat_grad_norm = 0.5267 cg_residual = 0.0003 step_size = 0.3311 reward = 0.0000 fps = 10 mse_loss = 0.5573 
2022-07-08 09:18:18.069336 - gail/main.py:174 - [TRPO] iter = 9000 dist_mean = 0.0292 dist_std = 0.9685 vf_loss = 0.2379 grad_norm = 0.5613 nat_grad_norm = 0.4399 cg_residual = 0.0002 step_size = 0.3557 reward = 0.0000 fps = 8 mse_loss = 0.5984 
2022-07-08 09:18:47.788261 - gail/main.py:174 - [TRPO] iter = 10000 dist_mean = 0.0372 dist_std = 0.9627 vf_loss = 0.2773 grad_norm = 0.5062 nat_grad_norm = 0.4898 cg_residual = 0.0004 step_size = 0.3592 reward = -0.0000 fps = 6 mse_loss = 0.6384 
2022-07-08 09:18:48.594454 - gail/main.py:201 - [Discriminator] iter = 10000 loss = 0.8904 grad_norm = 10.2298 grad_penalty = 0.8741 regularization = 0.0000 true_logits = -0.0085 fake_logits = 0.0078 true_prob = 0.4981 fake_prob = 0.5024 
2022-07-08 09:18:54.411450 - gail/main.py:142 - [Evaluate] iter = 10000 episode={ returns = -11.6015 lengths = 15 } discounted_episode={ returns = -11.5358 lengths = 15 } 
2022-07-08 09:19:23.773134 - gail/main.py:174 - [TRPO] iter = 11000 dist_mean = 0.0402 dist_std = 0.9561 vf_loss = 0.2551 grad_norm = 0.5274 nat_grad_norm = 0.4791 cg_residual = 0.0004 step_size = 0.3638 reward = 0.0000 fps = 28 mse_loss = 0.5759 
2022-07-08 09:19:52.065689 - gail/main.py:174 - [TRPO] iter = 12000 dist_mean = 0.0639 dist_std = 0.9534 vf_loss = 0.2240 grad_norm = 0.6430 nat_grad_norm = 0.5001 cg_residual = 0.0007 step_size = 0.3312 reward = 0.0000 fps = 15 mse_loss = 0.6167 
2022-07-08 09:20:21.705316 - gail/main.py:174 - [TRPO] iter = 13000 dist_mean = 0.0898 dist_std = 0.9486 vf_loss = 0.1727 grad_norm = 0.5995 nat_grad_norm = 0.5065 cg_residual = 0.0007 step_size = 0.3228 reward = -0.0000 fps = 10 mse_loss = 0.6181 
2022-07-08 09:20:50.906264 - gail/main.py:174 - [TRPO] iter = 14000 dist_mean = 0.1051 dist_std = 0.9354 vf_loss = 0.1022 grad_norm = 0.5674 nat_grad_norm = 0.5401 cg_residual = 0.0008 step_size = 0.3413 reward = 0.0000 fps = 8 mse_loss = 0.6735 
2022-07-08 09:21:18.563457 - gail/main.py:174 - [TRPO] iter = 15000 dist_mean = 0.1208 dist_std = 0.9252 vf_loss = 0.1410 grad_norm = 0.6225 nat_grad_norm = 0.5543 cg_residual = 0.0008 step_size = 0.3204 reward = -0.0000 fps = 6 mse_loss = 0.6972 
2022-07-08 09:21:19.334294 - gail/main.py:201 - [Discriminator] iter = 15000 loss = -0.0435 grad_norm = 9.8571 grad_penalty = 0.5803 regularization = 0.0000 true_logits = 0.0030 fake_logits = -0.6208 true_prob = 0.5012 fake_prob = 0.3543 
2022-07-08 09:21:25.217682 - gail/main.py:142 - [Evaluate] iter = 15000 episode={ returns = -11.1722 lengths = 15 } discounted_episode={ returns = -10.9988 lengths = 15 } 
2022-07-08 09:22:00.129836 - gail/main.py:174 - [TRPO] iter = 16000 dist_mean = 0.1453 dist_std = 0.9156 vf_loss = 0.0961 grad_norm = 0.5960 nat_grad_norm = 0.4166 cg_residual = 0.0009 step_size = 0.3738 reward = -0.0000 fps = 24 mse_loss = 0.6643 
2022-07-08 09:22:28.924863 - gail/main.py:174 - [TRPO] iter = 17000 dist_mean = 0.1552 dist_std = 0.9177 vf_loss = 0.1886 grad_norm = 0.6683 nat_grad_norm = 0.4832 cg_residual = 0.0007 step_size = 0.3162 reward = 0.0000 fps = 14 mse_loss = 0.6757 
2022-07-08 09:22:58.781216 - gail/main.py:174 - [TRPO] iter = 18000 dist_mean = 0.1747 dist_std = 0.9134 vf_loss = 0.1913 grad_norm = 0.7476 nat_grad_norm = 0.5109 cg_residual = 0.0010 step_size = 0.2720 reward = -0.0000 fps = 10 mse_loss = 0.6239 
2022-07-08 09:23:28.060557 - gail/main.py:174 - [TRPO] iter = 19000 dist_mean = 0.2023 dist_std = 0.9109 vf_loss = 0.2039 grad_norm = 0.7696 nat_grad_norm = 0.4972 cg_residual = 0.0008 step_size = 0.2813 reward = -0.0000 fps = 7 mse_loss = 0.5965 
2022-07-08 09:23:57.047060 - gail/main.py:174 - [TRPO] iter = 20000 dist_mean = 0.2250 dist_std = 0.9122 vf_loss = 0.2915 grad_norm = 0.7985 nat_grad_norm = 0.4829 cg_residual = 0.0005 step_size = 0.2812 reward = 0.0000 fps = 6 mse_loss = 0.5884 
2022-07-08 09:23:57.844309 - gail/main.py:201 - [Discriminator] iter = 20000 loss = -0.8522 grad_norm = 8.3713 grad_penalty = 0.4744 regularization = 0.0000 true_logits = 0.0393 fake_logits = -1.2872 true_prob = 0.5105 fake_prob = 0.2318 
2022-07-08 09:24:07.425426 - gail/main.py:142 - [Evaluate] iter = 20000 episode={ returns = -2.0548 lengths = 24 } discounted_episode={ returns = -2.0434 lengths = 24 } 
2022-07-08 09:24:34.828062 - gail/main.py:174 - [TRPO] iter = 21000 dist_mean = 0.2396 dist_std = 0.9070 vf_loss = 0.3936 grad_norm = 0.6827 nat_grad_norm = 0.5532 cg_residual = 0.0011 step_size = 0.3052 reward = 0.0000 fps = 27 mse_loss = 0.5812 
2022-07-08 09:25:02.603178 - gail/main.py:174 - [TRPO] iter = 22000 dist_mean = 0.2644 dist_std = 0.9060 vf_loss = 0.8316 grad_norm = 0.7997 nat_grad_norm = 0.5036 cg_residual = 0.0006 step_size = 0.2974 reward = 0.0000 fps = 15 mse_loss = 0.5863 
2022-07-08 09:25:29.358349 - gail/main.py:174 - [TRPO] iter = 23000 dist_mean = 0.2853 dist_std = 0.9025 vf_loss = 0.8804 grad_norm = 0.6642 nat_grad_norm = 0.4925 cg_residual = 0.0006 step_size = 0.3395 reward = -0.0000 fps = 10 mse_loss = 0.6140 
2022-07-08 09:25:57.569833 - gail/main.py:174 - [TRPO] iter = 24000 dist_mean = 0.3323 dist_std = 0.8962 vf_loss = 1.1027 grad_norm = 0.5262 nat_grad_norm = 0.5396 cg_residual = 0.0015 step_size = 0.3989 reward = -0.0000 fps = 8 mse_loss = 0.6200 
2022-07-08 09:26:25.943568 - gail/main.py:174 - [TRPO] iter = 25000 dist_mean = 0.3372 dist_std = 0.9002 vf_loss = 1.2830 grad_norm = 0.5969 nat_grad_norm = 0.4806 cg_residual = 0.0016 step_size = 0.4140 reward = 0.0000 fps = 6 mse_loss = 0.6009 
2022-07-08 09:26:26.796026 - gail/main.py:201 - [Discriminator] iter = 25000 loss = -0.9189 grad_norm = 6.9237 grad_penalty = 0.4582 regularization = 0.0000 true_logits = -0.0008 fake_logits = -1.3779 true_prob = 0.5037 fake_prob = 0.2251 
2022-07-08 09:28:21.711556 - gail/main.py:142 - [Evaluate] iter = 25000 episode={ returns = 356.3630 lengths = 278 } discounted_episode={ returns = 309.0623 lengths = 312 } 
2022-07-08 09:28:50.175482 - gail/main.py:174 - [TRPO] iter = 26000 dist_mean = 0.3904 dist_std = 0.9010 vf_loss = 1.2270 grad_norm = 0.6293 nat_grad_norm = 0.5430 cg_residual = 0.0011 step_size = 0.3875 reward = -0.0000 fps = 6 mse_loss = 0.5930 
2022-07-08 09:29:18.488487 - gail/main.py:174 - [TRPO] iter = 27000 dist_mean = 0.4234 dist_std = 0.8976 vf_loss = 2.2162 grad_norm = 0.6075 nat_grad_norm = 0.5051 cg_residual = 0.0014 step_size = 0.4172 reward = 0.0000 fps = 5 mse_loss = 0.5861 
2022-07-08 09:29:46.392944 - gail/main.py:174 - [TRPO] iter = 28000 dist_mean = 0.4455 dist_std = 0.8995 vf_loss = 1.2191 grad_norm = 0.4596 nat_grad_norm = 0.4507 cg_residual = 0.0036 step_size = 0.4594 reward = 0.0000 fps = 5 mse_loss = 0.5880 
2022-07-08 09:30:13.297456 - gail/main.py:174 - [TRPO] iter = 29000 dist_mean = 0.4327 dist_std = 0.8974 vf_loss = 1.4023 grad_norm = 0.5944 nat_grad_norm = 0.4829 cg_residual = 0.0019 step_size = 0.4078 reward = -0.0000 fps = 4 mse_loss = 0.6391 
2022-07-08 09:30:40.561591 - gail/main.py:174 - [TRPO] iter = 30000 dist_mean = 0.5061 dist_std = 0.8915 vf_loss = 1.5225 grad_norm = 0.6181 nat_grad_norm = 0.4893 cg_residual = 0.0011 step_size = 0.4050 reward = -0.0000 fps = 3 mse_loss = 0.6539 
2022-07-08 09:30:41.334742 - gail/main.py:201 - [Discriminator] iter = 30000 loss = -0.7311 grad_norm = 5.6234 grad_penalty = 0.4035 regularization = 0.0000 true_logits = 0.0113 fake_logits = -1.1233 true_prob = 0.5076 fake_prob = 0.2685 
2022-07-08 09:32:06.896081 - gail/main.py:142 - [Evaluate] iter = 30000 episode={ returns = 345.6545 lengths = 221 } discounted_episode={ returns = 299.4411 lengths = 216 } 
2022-07-08 09:32:40.365770 - gail/main.py:174 - [TRPO] iter = 31000 dist_mean = 0.4956 dist_std = 0.8945 vf_loss = 1.4681 grad_norm = 0.4431 nat_grad_norm = 0.5517 cg_residual = 0.0048 step_size = 0.3886 reward = 0.0000 fps = 8 mse_loss = 0.6564 
2022-07-08 09:33:06.567625 - gail/main.py:174 - [TRPO] iter = 32000 dist_mean = 0.5008 dist_std = 0.8920 vf_loss = 0.7091 grad_norm = 0.4674 nat_grad_norm = 0.5449 cg_residual = 0.0050 step_size = 0.3870 reward = 0.0000 fps = 6 mse_loss = 0.6231 
2022-07-08 09:33:33.354380 - gail/main.py:174 - [TRPO] iter = 33000 dist_mean = 0.4631 dist_std = 0.8877 vf_loss = 1.3014 grad_norm = 0.3681 nat_grad_norm = 0.4218 cg_residual = 0.0029 step_size = 0.5123 reward = 0.0000 fps = 5 mse_loss = 0.6776 
2022-07-08 09:34:00.162970 - gail/main.py:174 - [TRPO] iter = 34000 dist_mean = 0.5168 dist_std = 0.8865 vf_loss = 1.1920 grad_norm = 0.4168 nat_grad_norm = 0.4866 cg_residual = 0.0017 step_size = 0.4344 reward = -0.0000 fps = 5 mse_loss = 0.6361 
2022-07-08 09:34:26.006533 - gail/main.py:174 - [TRPO] iter = 35000 dist_mean = 0.5479 dist_std = 0.8825 vf_loss = 1.3578 grad_norm = 0.5746 nat_grad_norm = 0.3938 cg_residual = 0.0015 step_size = 0.4698 reward = 0.0000 fps = 4 mse_loss = 0.6673 
2022-07-08 09:34:26.735447 - gail/main.py:201 - [Discriminator] iter = 35000 loss = -1.1371 grad_norm = 5.3012 grad_penalty = 0.3370 regularization = 0.0000 true_logits = 0.0255 fake_logits = -1.4485 true_prob = 0.5119 fake_prob = 0.2057 
2022-07-08 09:35:57.836835 - gail/main.py:142 - [Evaluate] iter = 35000 episode={ returns = 197.5802 lengths = 256 } discounted_episode={ returns = 153.7397 lengths = 227 } 
2022-07-08 09:36:24.821055 - gail/main.py:174 - [TRPO] iter = 36000 dist_mean = 0.5523 dist_std = 0.8809 vf_loss = 1.1925 grad_norm = 0.3310 nat_grad_norm = 0.4971 cg_residual = 0.0024 step_size = 0.4593 reward = 0.0000 fps = 8 mse_loss = 0.6698 
2022-07-08 09:36:50.532305 - gail/main.py:174 - [TRPO] iter = 37000 dist_mean = 0.5407 dist_std = 0.8797 vf_loss = 0.9599 grad_norm = 0.3667 nat_grad_norm = 0.5241 cg_residual = 0.0051 step_size = 0.4056 reward = -0.0000 fps = 6 mse_loss = 0.7549 
2022-07-08 09:37:16.391395 - gail/main.py:174 - [TRPO] iter = 38000 dist_mean = 0.5243 dist_std = 0.8764 vf_loss = 1.1924 grad_norm = 0.3451 nat_grad_norm = 0.4073 cg_residual = 0.0010 step_size = 0.5005 reward = -0.0000 fps = 5 mse_loss = 0.7114 
2022-07-08 09:37:42.508904 - gail/main.py:174 - [TRPO] iter = 39000 dist_mean = 0.5360 dist_std = 0.8758 vf_loss = 0.5598 grad_norm = 0.4667 nat_grad_norm = 0.5638 cg_residual = 0.0031 step_size = 0.3874 reward = 0.0000 fps = 5 mse_loss = 0.7654 
2022-07-08 09:38:08.612667 - gail/main.py:174 - [TRPO] iter = 40000 dist_mean = 0.5408 dist_std = 0.8725 vf_loss = 0.5603 grad_norm = 0.4932 nat_grad_norm = 0.5147 cg_residual = 0.0056 step_size = 0.4248 reward = 0.0000 fps = 4 mse_loss = 0.7359 
2022-07-08 09:38:09.391109 - gail/main.py:201 - [Discriminator] iter = 40000 loss = -1.4575 grad_norm = 5.5958 grad_penalty = 0.3332 regularization = 0.0000 true_logits = 0.0472 fake_logits = -1.7434 true_prob = 0.5156 fake_prob = 0.1658 
2022-07-08 09:39:52.510146 - gail/main.py:142 - [Evaluate] iter = 40000 episode={ returns = 376.5583 lengths = 263 } discounted_episode={ returns = 327.4839 lengths = 298 } 
2022-07-08 09:40:20.579538 - gail/main.py:174 - [TRPO] iter = 41000 dist_mean = 0.5441 dist_std = 0.8696 vf_loss = 0.4442 grad_norm = 0.5550 nat_grad_norm = 0.5015 cg_residual = 0.0024 step_size = 0.4177 reward = -0.0000 fps = 7 mse_loss = 0.7796 
2022-07-08 09:40:46.880549 - gail/main.py:174 - [TRPO] iter = 42000 dist_mean = 0.5322 dist_std = 0.8742 vf_loss = 0.7694 grad_norm = 0.3754 nat_grad_norm = 0.4664 cg_residual = 0.0037 step_size = 0.4565 reward = -0.0000 fps = 6 mse_loss = 0.7565 
2022-07-08 09:41:12.467408 - gail/main.py:174 - [TRPO] iter = 43000 dist_mean = 0.5533 dist_std = 0.8749 vf_loss = 0.9244 grad_norm = 0.4676 nat_grad_norm = 0.4584 cg_residual = 0.0031 step_size = 0.4171 reward = -0.0000 fps = 5 mse_loss = 0.7778 
2022-07-08 09:41:39.151887 - gail/main.py:174 - [TRPO] iter = 44000 dist_mean = 0.5395 dist_std = 0.8744 vf_loss = 0.5331 grad_norm = 0.3696 nat_grad_norm = 0.4801 cg_residual = 0.0045 step_size = 0.4248 reward = 0.0000 fps = 4 mse_loss = 0.7346 
2022-07-08 09:42:07.483973 - gail/main.py:174 - [TRPO] iter = 45000 dist_mean = 0.5494 dist_std = 0.8778 vf_loss = 0.5165 grad_norm = 0.4287 nat_grad_norm = 0.5094 cg_residual = 0.0050 step_size = 0.3780 reward = -0.0000 fps = 4 mse_loss = 0.7646 
2022-07-08 09:42:08.749498 - gail/main.py:201 - [Discriminator] iter = 45000 loss = -2.1551 grad_norm = 6.0369 grad_penalty = 0.3110 regularization = 0.0000 true_logits = 0.0646 fake_logits = -2.4015 true_prob = 0.5271 fake_prob = 0.0994 
2022-07-08 09:43:38.233513 - gail/main.py:142 - [Evaluate] iter = 45000 episode={ returns = 184.5824 lengths = 243 } discounted_episode={ returns = 215.0765 lengths = 251 } 
2022-07-08 09:44:03.708008 - gail/main.py:174 - [TRPO] iter = 46000 dist_mean = 0.5543 dist_std = 0.8788 vf_loss = 0.8865 grad_norm = 0.4372 nat_grad_norm = 0.4532 cg_residual = 0.0040 step_size = 0.4335 reward = 0.0000 fps = 8 mse_loss = 0.8600 
2022-07-08 09:44:29.747387 - gail/main.py:174 - [TRPO] iter = 47000 dist_mean = 0.5588 dist_std = 0.8773 vf_loss = 1.0516 grad_norm = 0.4372 nat_grad_norm = 0.4757 cg_residual = 0.0046 step_size = 0.4414 reward = 0.0000 fps = 7 mse_loss = 0.7574 
2022-07-08 09:44:55.123350 - gail/main.py:174 - [TRPO] iter = 48000 dist_mean = 0.5352 dist_std = 0.8738 vf_loss = 1.0619 grad_norm = 0.3008 nat_grad_norm = 0.4572 cg_residual = 0.0020 step_size = 0.4727 reward = -0.0000 fps = 6 mse_loss = 0.7273 
2022-07-08 09:45:20.017026 - gail/main.py:174 - [TRPO] iter = 49000 dist_mean = 0.5291 dist_std = 0.8761 vf_loss = 1.4219 grad_norm = 0.3256 nat_grad_norm = 0.5154 cg_residual = 0.0019 step_size = 0.4516 reward = -0.0000 fps = 5 mse_loss = 0.8115 
2022-07-08 09:45:45.404452 - gail/main.py:174 - [TRPO] iter = 50000 dist_mean = 0.5277 dist_std = 0.8734 vf_loss = 0.8625 grad_norm = 0.3659 nat_grad_norm = 0.5064 cg_residual = 0.0023 step_size = 0.4731 reward = 0.0000 fps = 4 mse_loss = 0.7569 
2022-07-08 09:45:46.134764 - gail/main.py:201 - [Discriminator] iter = 50000 loss = -2.7009 grad_norm = 5.7393 grad_penalty = 0.3253 regularization = 0.0000 true_logits = 0.0924 fake_logits = -2.9339 true_prob = 0.5361 fake_prob = 0.0660 
2022-07-08 09:46:58.584068 - gail/main.py:142 - [Evaluate] iter = 50000 episode={ returns = 354.1514 lengths = 202 } discounted_episode={ returns = 319.9417 lengths = 212 } 
2022-07-08 09:47:24.340834 - gail/main.py:174 - [TRPO] iter = 51000 dist_mean = 0.4977 dist_std = 0.8745 vf_loss = 1.0924 grad_norm = 0.3565 nat_grad_norm = 0.5134 cg_residual = 0.0019 step_size = 0.4348 reward = -0.0000 fps = 10 mse_loss = 0.8384 
2022-07-08 09:47:49.788282 - gail/main.py:174 - [TRPO] iter = 52000 dist_mean = 0.4706 dist_std = 0.8727 vf_loss = 0.4795 grad_norm = 0.4167 nat_grad_norm = 0.6957 cg_residual = 0.0037 step_size = 0.3558 reward = -0.0000 fps = 8 mse_loss = 0.7694 
2022-07-08 09:48:15.405151 - gail/main.py:174 - [TRPO] iter = 53000 dist_mean = 0.4937 dist_std = 0.8795 vf_loss = 1.1385 grad_norm = 0.4277 nat_grad_norm = 0.5688 cg_residual = 0.0016 step_size = 0.4026 reward = 0.0000 fps = 6 mse_loss = 0.8234 
2022-07-08 09:48:40.451687 - gail/main.py:174 - [TRPO] iter = 54000 dist_mean = 0.5019 dist_std = 0.8752 vf_loss = 0.8163 grad_norm = 0.4152 nat_grad_norm = 0.5503 cg_residual = 0.0023 step_size = 0.3944 reward = 0.0000 fps = 5 mse_loss = 0.8839 
2022-07-08 09:49:06.132357 - gail/main.py:174 - [TRPO] iter = 55000 dist_mean = 0.4770 dist_std = 0.8719 vf_loss = 0.5707 grad_norm = 0.4645 nat_grad_norm = 0.6086 cg_residual = 0.0048 step_size = 0.3459 reward = -0.0000 fps = 5 mse_loss = 0.8885 
2022-07-08 09:49:06.862607 - gail/main.py:201 - [Discriminator] iter = 55000 loss = -3.0481 grad_norm = 5.7770 grad_penalty = 0.4309 regularization = 0.0000 true_logits = 0.1140 fake_logits = -3.3650 true_prob = 0.5391 fake_prob = 0.0506 
2022-07-08 09:50:05.464052 - gail/main.py:142 - [Evaluate] iter = 55000 episode={ returns = 327.6200 lengths = 156 } discounted_episode={ returns = 305.8889 lengths = 163 } 
2022-07-08 09:50:32.646446 - gail/main.py:174 - [TRPO] iter = 56000 dist_mean = 0.5051 dist_std = 0.8705 vf_loss = 0.5667 grad_norm = 0.3618 nat_grad_norm = 0.4875 cg_residual = 0.0026 step_size = 0.4625 reward = -0.0000 fps = 11 mse_loss = 0.9290 
2022-07-08 09:50:57.982733 - gail/main.py:174 - [TRPO] iter = 57000 dist_mean = 0.4943 dist_std = 0.8735 vf_loss = 0.8267 grad_norm = 0.5136 nat_grad_norm = 0.5452 cg_residual = 0.0039 step_size = 0.3602 reward = -0.0000 fps = 9 mse_loss = 0.8969 
2022-07-08 09:51:22.619704 - gail/main.py:174 - [TRPO] iter = 58000 dist_mean = 0.4931 dist_std = 0.8744 vf_loss = 0.9858 grad_norm = 0.4953 nat_grad_norm = 0.5514 cg_residual = 0.0023 step_size = 0.3860 reward = 0.0000 fps = 7 mse_loss = 0.9033 
2022-07-08 09:51:47.702954 - gail/main.py:174 - [TRPO] iter = 59000 dist_mean = 0.4513 dist_std = 0.8729 vf_loss = 1.0255 grad_norm = 0.4754 nat_grad_norm = 0.6582 cg_residual = 0.0038 step_size = 0.3607 reward = 0.0000 fps = 6 mse_loss = 1.0043 
2022-07-08 09:52:20.885515 - gail/main.py:174 - [TRPO] iter = 60000 dist_mean = 0.4769 dist_std = 0.8684 vf_loss = 0.5689 grad_norm = 0.5661 nat_grad_norm = 0.7743 cg_residual = 0.0078 step_size = 0.3256 reward = 0.0000 fps = 5 mse_loss = 0.9001 
2022-07-08 09:52:21.629720 - gail/main.py:201 - [Discriminator] iter = 60000 loss = -3.3062 grad_norm = 6.2913 grad_penalty = 0.4275 regularization = 0.0000 true_logits = 0.1760 fake_logits = -3.5577 true_prob = 0.5531 fake_prob = 0.0434 
2022-07-08 09:53:12.437076 - gail/main.py:142 - [Evaluate] iter = 60000 episode={ returns = 322.4545 lengths = 148 } discounted_episode={ returns = 288.8502 lengths = 144 } 
2022-07-08 09:53:39.542566 - gail/main.py:174 - [TRPO] iter = 61000 dist_mean = 0.4839 dist_std = 0.8712 vf_loss = 0.7346 grad_norm = 0.4935 nat_grad_norm = 0.5692 cg_residual = 0.0029 step_size = 0.3792 reward = -0.0000 fps = 12 mse_loss = 0.9061 
2022-07-08 09:54:06.555174 - gail/main.py:174 - [TRPO] iter = 62000 dist_mean = 0.4693 dist_std = 0.8667 vf_loss = 0.8488 grad_norm = 0.4512 nat_grad_norm = 0.4867 cg_residual = 0.0024 step_size = 0.4168 reward = 0.0000 fps = 9 mse_loss = 0.9081 
2022-07-08 09:54:33.453391 - gail/main.py:174 - [TRPO] iter = 63000 dist_mean = 0.4280 dist_std = 0.8690 vf_loss = 0.6738 grad_norm = 0.3356 nat_grad_norm = 0.5515 cg_residual = 0.0022 step_size = 0.4465 reward = 0.0000 fps = 7 mse_loss = 0.9893 
2022-07-08 09:54:59.445173 - gail/main.py:174 - [TRPO] iter = 64000 dist_mean = 0.4558 dist_std = 0.8679 vf_loss = 0.4511 grad_norm = 0.4813 nat_grad_norm = 0.6695 cg_residual = 0.0058 step_size = 0.3386 reward = 0.0000 fps = 6 mse_loss = 1.0405 
2022-07-08 09:55:24.049966 - gail/main.py:174 - [TRPO] iter = 65000 dist_mean = 0.4557 dist_std = 0.8594 vf_loss = 0.6763 grad_norm = 0.4467 nat_grad_norm = 0.5838 cg_residual = 0.0043 step_size = 0.3904 reward = -0.0000 fps = 5 mse_loss = 0.8817 
2022-07-08 09:55:24.765747 - gail/main.py:201 - [Discriminator] iter = 65000 loss = -3.5022 grad_norm = 5.1984 grad_penalty = 0.3992 regularization = 0.0000 true_logits = 0.0851 fake_logits = -3.8163 true_prob = 0.5393 fake_prob = 0.0381 
2022-07-08 09:56:10.647339 - gail/main.py:142 - [Evaluate] iter = 65000 episode={ returns = 308.1736 lengths = 144 } discounted_episode={ returns = 283.0464 lengths = 144 } 
2022-07-08 09:56:34.880808 - gail/main.py:174 - [TRPO] iter = 66000 dist_mean = 0.4535 dist_std = 0.8562 vf_loss = 0.8283 grad_norm = 0.5492 nat_grad_norm = 0.7310 cg_residual = 0.0037 step_size = 0.3361 reward = 0.0000 fps = 14 mse_loss = 0.8423 
2022-07-08 09:56:58.830893 - gail/main.py:174 - [TRPO] iter = 67000 dist_mean = 0.4541 dist_std = 0.8560 vf_loss = 1.0429 grad_norm = 0.4522 nat_grad_norm = 0.6028 cg_residual = 0.0024 step_size = 0.3852 reward = -0.0000 fps = 10 mse_loss = 0.8897 
2022-07-08 09:57:24.009633 - gail/main.py:174 - [TRPO] iter = 68000 dist_mean = 0.4498 dist_std = 0.8564 vf_loss = 1.1628 grad_norm = 0.4221 nat_grad_norm = 0.6344 cg_residual = 0.0031 step_size = 0.3906 reward = -0.0000 fps = 8 mse_loss = 0.8583 
2022-07-08 09:57:48.634957 - gail/main.py:174 - [TRPO] iter = 69000 dist_mean = 0.4092 dist_std = 0.8567 vf_loss = 0.5591 grad_norm = 0.5262 nat_grad_norm = 0.6801 cg_residual = 0.0033 step_size = 0.3582 reward = 0.0000 fps = 6 mse_loss = 0.8277 
2022-07-08 09:58:13.351398 - gail/main.py:174 - [TRPO] iter = 70000 dist_mean = 0.4281 dist_std = 0.8570 vf_loss = 0.8077 grad_norm = 0.4585 nat_grad_norm = 0.6082 cg_residual = 0.0045 step_size = 0.3888 reward = -0.0000 fps = 5 mse_loss = 0.7592 
2022-07-08 09:58:14.088252 - gail/main.py:201 - [Discriminator] iter = 70000 loss = -3.6418 grad_norm = 4.9246 grad_penalty = 0.4266 regularization = 0.0000 true_logits = 0.0416 fake_logits = -4.0269 true_prob = 0.5335 fake_prob = 0.0300 
2022-07-08 09:59:06.373733 - gail/main.py:142 - [Evaluate] iter = 70000 episode={ returns = 319.4675 lengths = 148 } discounted_episode={ returns = 293.4766 lengths = 148 } 
2022-07-08 09:59:32.785253 - gail/main.py:174 - [TRPO] iter = 71000 dist_mean = 0.3980 dist_std = 0.8544 vf_loss = 0.6524 grad_norm = 0.5850 nat_grad_norm = 0.7579 cg_residual = 0.0057 step_size = 0.3468 reward = -0.0000 fps = 12 mse_loss = 0.8429 
2022-07-08 09:59:59.046975 - gail/main.py:174 - [TRPO] iter = 72000 dist_mean = 0.4065 dist_std = 0.8533 vf_loss = 0.5975 grad_norm = 0.5125 nat_grad_norm = 0.7410 cg_residual = 0.0041 step_size = 0.3303 reward = -0.0000 fps = 9 mse_loss = 0.8035 
2022-07-08 10:00:25.423583 - gail/main.py:174 - [TRPO] iter = 73000 dist_mean = 0.3765 dist_std = 0.8555 vf_loss = 0.7057 grad_norm = 0.5045 nat_grad_norm = 0.6654 cg_residual = 0.0050 step_size = 0.3428 reward = -0.0000 fps = 7 mse_loss = 0.9594 
2022-07-08 10:00:52.427696 - gail/main.py:174 - [TRPO] iter = 74000 dist_mean = 0.3615 dist_std = 0.8518 vf_loss = 0.3156 grad_norm = 0.5927 nat_grad_norm = 0.6723 cg_residual = 0.0098 step_size = 0.3365 reward = -0.0000 fps = 6 mse_loss = 0.8652 
2022-07-08 10:01:18.644528 - gail/main.py:174 - [TRPO] iter = 75000 dist_mean = 0.3907 dist_std = 0.8526 vf_loss = 0.3904 grad_norm = 0.5053 nat_grad_norm = 0.5565 cg_residual = 0.0041 step_size = 0.4016 reward = -0.0000 fps = 5 mse_loss = 0.9356 
2022-07-08 10:01:19.451506 - gail/main.py:201 - [Discriminator] iter = 75000 loss = -3.4951 grad_norm = 4.6899 grad_penalty = 0.3954 regularization = 0.0000 true_logits = -0.0075 fake_logits = -3.8979 true_prob = 0.5218 fake_prob = 0.0320 
2022-07-08 10:02:24.633759 - gail/main.py:142 - [Evaluate] iter = 75000 episode={ returns = 325.1748 lengths = 153 } discounted_episode={ returns = 297.2537 lengths = 152 } 
2022-07-08 10:02:54.112123 - gail/main.py:174 - [TRPO] iter = 76000 dist_mean = 0.4051 dist_std = 0.8508 vf_loss = 0.4432 grad_norm = 0.4406 nat_grad_norm = 0.4660 cg_residual = 0.0028 step_size = 0.4443 reward = -0.0000 fps = 10 mse_loss = 0.9915 
2022-07-08 10:03:24.179507 - gail/main.py:174 - [TRPO] iter = 77000 dist_mean = 0.3826 dist_std = 0.8545 vf_loss = 0.4325 grad_norm = 0.5145 nat_grad_norm = 0.7634 cg_residual = 0.0064 step_size = 0.3284 reward = 0.0000 fps = 8 mse_loss = 0.8590 
2022-07-08 10:03:54.485247 - gail/main.py:174 - [TRPO] iter = 78000 dist_mean = 0.3717 dist_std = 0.8503 vf_loss = 0.4382 grad_norm = 0.5066 nat_grad_norm = 0.6218 cg_residual = 0.0035 step_size = 0.3892 reward = -0.0000 fps = 6 mse_loss = 0.9791 
2022-07-08 10:04:23.845126 - gail/main.py:174 - [TRPO] iter = 79000 dist_mean = 0.3786 dist_std = 0.8531 vf_loss = 0.6530 grad_norm = 0.4654 nat_grad_norm = 0.6224 cg_residual = 0.0048 step_size = 0.3785 reward = 0.0000 fps = 5 mse_loss = 0.9663 
2022-07-08 10:04:53.122589 - gail/main.py:174 - [TRPO] iter = 80000 dist_mean = 0.3822 dist_std = 0.8471 vf_loss = 0.4486 grad_norm = 0.6557 nat_grad_norm = 0.8528 cg_residual = 0.0121 step_size = 0.2907 reward = -0.0000 fps = 4 mse_loss = 1.0602 
2022-07-08 10:04:53.916796 - gail/main.py:201 - [Discriminator] iter = 80000 loss = -3.9395 grad_norm = 5.3941 grad_penalty = 0.3639 regularization = 0.0000 true_logits = -0.0706 fake_logits = -4.3739 true_prob = 0.5083 fake_prob = 0.0247 
2022-07-08 10:05:46.990702 - gail/main.py:142 - [Evaluate] iter = 80000 episode={ returns = 288.9293 lengths = 136 } discounted_episode={ returns = 265.7132 lengths = 134 } 
2022-07-08 10:06:16.158479 - gail/main.py:174 - [TRPO] iter = 81000 dist_mean = 0.4040 dist_std = 0.8444 vf_loss = 0.5422 grad_norm = 0.5127 nat_grad_norm = 0.7836 cg_residual = 0.0057 step_size = 0.3294 reward = 0.0000 fps = 12 mse_loss = 1.0387 
2022-07-08 10:06:45.657379 - gail/main.py:174 - [TRPO] iter = 82000 dist_mean = 0.3984 dist_std = 0.8414 vf_loss = 0.6360 grad_norm = 0.4431 nat_grad_norm = 0.6006 cg_residual = 0.0041 step_size = 0.4255 reward = -0.0000 fps = 8 mse_loss = 0.9577 
2022-07-08 10:07:15.051747 - gail/main.py:174 - [TRPO] iter = 83000 dist_mean = 0.3915 dist_std = 0.8398 vf_loss = 0.5421 grad_norm = 0.5083 nat_grad_norm = 0.6687 cg_residual = 0.0057 step_size = 0.3369 reward = -0.0000 fps = 7 mse_loss = 1.0125 
2022-07-08 10:07:43.292931 - gail/main.py:174 - [TRPO] iter = 84000 dist_mean = 0.3741 dist_std = 0.8422 vf_loss = 0.5194 grad_norm = 0.5439 nat_grad_norm = 0.5787 cg_residual = 0.0058 step_size = 0.3651 reward = -0.0000 fps = 5 mse_loss = 1.1409 
2022-07-08 10:08:11.412694 - gail/main.py:174 - [TRPO] iter = 85000 dist_mean = 0.3759 dist_std = 0.8390 vf_loss = 0.3797 grad_norm = 0.5963 nat_grad_norm = 0.6819 cg_residual = 0.0061 step_size = 0.3402 reward = 0.0000 fps = 5 mse_loss = 1.1723 
2022-07-08 10:08:12.174178 - gail/main.py:201 - [Discriminator] iter = 85000 loss = -3.6300 grad_norm = 4.7184 grad_penalty = 0.4326 regularization = 0.0000 true_logits = -0.2193 fake_logits = -4.2820 true_prob = 0.4790 fake_prob = 0.0240 
2022-07-08 10:09:03.035641 - gail/main.py:142 - [Evaluate] iter = 85000 episode={ returns = 282.1510 lengths = 129 } discounted_episode={ returns = 259.1484 lengths = 127 } 
2022-07-08 10:09:33.000842 - gail/main.py:174 - [TRPO] iter = 86000 dist_mean = 0.3949 dist_std = 0.8340 vf_loss = 0.2802 grad_norm = 0.5561 nat_grad_norm = 0.5640 cg_residual = 0.0062 step_size = 0.3728 reward = -0.0000 fps = 12 mse_loss = 1.0623 
2022-07-08 10:10:02.605706 - gail/main.py:174 - [TRPO] iter = 87000 dist_mean = 0.3343 dist_std = 0.8307 vf_loss = 0.3116 grad_norm = 0.5571 nat_grad_norm = 0.6498 cg_residual = 0.0082 step_size = 0.3691 reward = -0.0000 fps = 9 mse_loss = 1.0627 
2022-07-08 10:10:31.576288 - gail/main.py:174 - [TRPO] iter = 88000 dist_mean = 0.3663 dist_std = 0.8253 vf_loss = 0.2937 grad_norm = 0.5507 nat_grad_norm = 0.6803 cg_residual = 0.0146 step_size = 0.3512 reward = 0.0000 fps = 7 mse_loss = 1.0556 
2022-07-08 10:11:00.302536 - gail/main.py:174 - [TRPO] iter = 89000 dist_mean = 0.4074 dist_std = 0.8207 vf_loss = 0.4623 grad_norm = 0.4382 nat_grad_norm = 0.5160 cg_residual = 0.0094 step_size = 0.4316 reward = -0.0000 fps = 5 mse_loss = 1.0936 
2022-07-08 10:11:29.532101 - gail/main.py:174 - [TRPO] iter = 90000 dist_mean = 0.3710 dist_std = 0.8195 vf_loss = 0.2880 grad_norm = 0.5116 nat_grad_norm = 0.5607 cg_residual = 0.0071 step_size = 0.3806 reward = 0.0000 fps = 5 mse_loss = 1.1052 
2022-07-08 10:11:30.425106 - gail/main.py:201 - [Discriminator] iter = 90000 loss = -3.5486 grad_norm = 4.4431 grad_penalty = 0.3867 regularization = 0.0000 true_logits = -0.2341 fake_logits = -4.1694 true_prob = 0.4824 fake_prob = 0.0239 
2022-07-08 10:12:24.058512 - gail/main.py:142 - [Evaluate] iter = 90000 episode={ returns = 263.9015 lengths = 121 } discounted_episode={ returns = 249.6447 lengths = 121 } 
2022-07-08 10:12:53.780338 - gail/main.py:174 - [TRPO] iter = 91000 dist_mean = 0.4155 dist_std = 0.8172 vf_loss = 0.3260 grad_norm = 0.5654 nat_grad_norm = 0.6222 cg_residual = 0.0116 step_size = 0.3415 reward = 0.0000 fps = 12 mse_loss = 1.0892 
2022-07-08 10:13:22.030464 - gail/main.py:174 - [TRPO] iter = 92000 dist_mean = 0.3644 dist_std = 0.8120 vf_loss = 0.2981 grad_norm = 0.5024 nat_grad_norm = 0.6385 cg_residual = 0.0082 step_size = 0.3474 reward = 0.0000 fps = 8 mse_loss = 0.9732 
2022-07-08 10:13:50.429489 - gail/main.py:174 - [TRPO] iter = 93000 dist_mean = 0.4403 dist_std = 0.8088 vf_loss = 0.4913 grad_norm = 0.4402 nat_grad_norm = 0.5767 cg_residual = 0.0101 step_size = 0.4208 reward = 0.0000 fps = 7 mse_loss = 1.1400 
2022-07-08 10:14:18.820967 - gail/main.py:174 - [TRPO] iter = 94000 dist_mean = 0.3998 dist_std = 0.8051 vf_loss = 0.4249 grad_norm = 0.4701 nat_grad_norm = 0.5793 cg_residual = 0.0138 step_size = 0.3672 reward = -0.0000 fps = 5 mse_loss = 1.0146 
2022-07-08 10:14:47.362959 - gail/main.py:174 - [TRPO] iter = 95000 dist_mean = 0.3909 dist_std = 0.8033 vf_loss = 0.5157 grad_norm = 0.5560 nat_grad_norm = 0.7249 cg_residual = 0.0156 step_size = 0.3157 reward = 0.0000 fps = 5 mse_loss = 1.1111 
2022-07-08 10:14:48.185517 - gail/main.py:201 - [Discriminator] iter = 95000 loss = -3.6596 grad_norm = 3.7448 grad_penalty = 0.3556 regularization = 0.0000 true_logits = -0.1942 fake_logits = -4.2093 true_prob = 0.4851 fake_prob = 0.0250 
2022-07-08 10:15:35.541818 - gail/main.py:142 - [Evaluate] iter = 95000 episode={ returns = 279.9512 lengths = 122 } discounted_episode={ returns = 255.4356 lengths = 121 } 
2022-07-08 10:16:04.832759 - gail/main.py:174 - [TRPO] iter = 96000 dist_mean = 0.3598 dist_std = 0.8030 vf_loss = 0.3170 grad_norm = 0.4362 nat_grad_norm = 0.6227 cg_residual = 0.0099 step_size = 0.4115 reward = 0.0000 fps = 13 mse_loss = 0.9875 
2022-07-08 10:16:32.789290 - gail/main.py:174 - [TRPO] iter = 97000 dist_mean = 0.3800 dist_std = 0.8056 vf_loss = 0.3719 grad_norm = 0.5219 nat_grad_norm = 0.5984 cg_residual = 0.0152 step_size = 0.3512 reward = 0.0000 fps = 9 mse_loss = 1.0685 
2022-07-08 10:17:01.742644 - gail/main.py:174 - [TRPO] iter = 98000 dist_mean = 0.3888 dist_std = 0.7991 vf_loss = 0.5099 grad_norm = 0.6076 nat_grad_norm = 0.6038 cg_residual = 0.0137 step_size = 0.3856 reward = -0.0000 fps = 7 mse_loss = 0.9646 
2022-07-08 10:17:30.424215 - gail/main.py:174 - [TRPO] iter = 99000 dist_mean = 0.3842 dist_std = 0.7988 vf_loss = 0.6028 grad_norm = 0.4479 nat_grad_norm = 0.6067 cg_residual = 0.0108 step_size = 0.3910 reward = -0.0000 fps = 6 mse_loss = 0.9592 
2022-07-08 10:17:59.334142 - gail/main.py:174 - [TRPO] iter = 100000 dist_mean = 0.3828 dist_std = 0.7954 vf_loss = 0.4585 grad_norm = 0.4801 nat_grad_norm = 0.5617 cg_residual = 0.0101 step_size = 0.3999 reward = -0.0000 fps = 5 mse_loss = 0.8656 
2022-07-08 10:18:00.231167 - gail/main.py:201 - [Discriminator] iter = 100000 loss = -3.7043 grad_norm = 2.9938 grad_penalty = 0.3665 regularization = 0.0000 true_logits = -0.1722 fake_logits = -4.2429 true_prob = 0.4867 fake_prob = 0.0227 
2022-07-08 10:18:46.281948 - gail/main.py:142 - [Evaluate] iter = 100000 episode={ returns = 277.5413 lengths = 124 } discounted_episode={ returns = 256.1505 lengths = 124 } 
2022-07-08 10:19:13.859508 - gail/main.py:174 - [TRPO] iter = 101000 dist_mean = 0.3932 dist_std = 0.7972 vf_loss = 0.4475 grad_norm = 0.5065 nat_grad_norm = 0.6567 cg_residual = 0.0115 step_size = 0.3592 reward = 0.0000 fps = 13 mse_loss = 0.8962 
2022-07-08 10:19:42.645790 - gail/main.py:174 - [TRPO] iter = 102000 dist_mean = 0.4007 dist_std = 0.7954 vf_loss = 0.5827 grad_norm = 0.5572 nat_grad_norm = 0.6209 cg_residual = 0.0172 step_size = 0.3399 reward = -0.0000 fps = 9 mse_loss = 0.8459 
2022-07-08 10:20:10.667917 - gail/main.py:174 - [TRPO] iter = 103000 dist_mean = 0.3647 dist_std = 0.7948 vf_loss = 0.3772 grad_norm = 0.5004 nat_grad_norm = 0.5527 cg_residual = 0.0162 step_size = 0.3766 reward = -0.0000 fps = 7 mse_loss = 0.9704 
2022-07-08 10:20:40.320092 - gail/main.py:174 - [TRPO] iter = 104000 dist_mean = 0.3681 dist_std = 0.7933 vf_loss = 0.2565 grad_norm = 0.5325 nat_grad_norm = 0.5919 cg_residual = 0.0213 step_size = 0.3561 reward = -0.0000 fps = 6 mse_loss = 0.9322 
2022-07-08 10:21:09.018087 - gail/main.py:174 - [TRPO] iter = 105000 dist_mean = 0.3607 dist_std = 0.7912 vf_loss = 0.3027 grad_norm = 0.6102 nat_grad_norm = 0.6307 cg_residual = 0.0108 step_size = 0.3614 reward = -0.0000 fps = 5 mse_loss = 1.0290 
2022-07-08 10:21:09.852384 - gail/main.py:201 - [Discriminator] iter = 105000 loss = -3.8504 grad_norm = 3.9019 grad_penalty = 0.3683 regularization = 0.0000 true_logits = -0.1343 fake_logits = -4.3530 true_prob = 0.4997 fake_prob = 0.0197 
2022-07-08 10:22:01.127972 - gail/main.py:142 - [Evaluate] iter = 105000 episode={ returns = 273.7188 lengths = 123 } discounted_episode={ returns = 256.5482 lengths = 123 } 
2022-07-08 10:22:28.627524 - gail/main.py:174 - [TRPO] iter = 106000 dist_mean = 0.3645 dist_std = 0.7894 vf_loss = 0.3978 grad_norm = 0.5359 nat_grad_norm = 0.5611 cg_residual = 0.0129 step_size = 0.4047 reward = 0.0000 fps = 12 mse_loss = 1.0240 
2022-07-08 10:22:57.283372 - gail/main.py:174 - [TRPO] iter = 107000 dist_mean = 0.3998 dist_std = 0.7887 vf_loss = 0.4013 grad_norm = 0.5380 nat_grad_norm = 0.5906 cg_residual = 0.0126 step_size = 0.3835 reward = 0.0000 fps = 9 mse_loss = 0.9889 
2022-07-08 10:23:24.497025 - gail/main.py:174 - [TRPO] iter = 108000 dist_mean = 0.3699 dist_std = 0.7891 vf_loss = 0.2961 grad_norm = 0.6253 nat_grad_norm = 0.6726 cg_residual = 0.0190 step_size = 0.3350 reward = 0.0000 fps = 7 mse_loss = 0.9441 
2022-07-08 10:23:52.765912 - gail/main.py:174 - [TRPO] iter = 109000 dist_mean = 0.3471 dist_std = 0.7866 vf_loss = 0.2992 grad_norm = 0.4743 nat_grad_norm = 0.6514 cg_residual = 0.0190 step_size = 0.3674 reward = -0.0000 fps = 6 mse_loss = 0.9298 
2022-07-08 10:24:21.754464 - gail/main.py:174 - [TRPO] iter = 110000 dist_mean = 0.3533 dist_std = 0.7853 vf_loss = 0.2101 grad_norm = 0.4220 nat_grad_norm = 0.5384 cg_residual = 0.0154 step_size = 0.4296 reward = -0.0000 fps = 5 mse_loss = 0.9576 
2022-07-08 10:24:22.607505 - gail/main.py:201 - [Discriminator] iter = 110000 loss = -3.7392 grad_norm = 2.9893 grad_penalty = 0.3612 regularization = 0.0000 true_logits = -0.0683 fake_logits = -4.1687 true_prob = 0.5043 fake_prob = 0.0234 
2022-07-08 10:25:08.896199 - gail/main.py:142 - [Evaluate] iter = 110000 episode={ returns = 302.2715 lengths = 123 } discounted_episode={ returns = 281.6681 lengths = 123 } 
2022-07-08 10:25:37.549205 - gail/main.py:174 - [TRPO] iter = 111000 dist_mean = 0.3610 dist_std = 0.7880 vf_loss = 0.4796 grad_norm = 0.6177 nat_grad_norm = 0.4732 cg_residual = 0.0135 step_size = 0.4090 reward = 0.0000 fps = 13 mse_loss = 0.9339 
2022-07-08 10:26:05.273372 - gail/main.py:174 - [TRPO] iter = 112000 dist_mean = 0.3671 dist_std = 0.7848 vf_loss = 0.6359 grad_norm = 0.5827 nat_grad_norm = 0.5773 cg_residual = 0.0128 step_size = 0.3951 reward = 0.0000 fps = 9 mse_loss = 0.9412 
2022-07-08 10:26:35.371860 - gail/main.py:174 - [TRPO] iter = 113000 dist_mean = 0.3337 dist_std = 0.7867 vf_loss = 0.2995 grad_norm = 0.6671 nat_grad_norm = 0.5601 cg_residual = 0.0172 step_size = 0.3422 reward = -0.0000 fps = 7 mse_loss = 0.9780 
2022-07-08 10:27:04.027984 - gail/main.py:174 - [TRPO] iter = 114000 dist_mean = 0.3524 dist_std = 0.7867 vf_loss = 0.2808 grad_norm = 0.6072 nat_grad_norm = 0.5344 cg_residual = 0.0176 step_size = 0.3793 reward = -0.0000 fps = 6 mse_loss = 1.1750 
2022-07-08 10:27:32.373416 - gail/main.py:174 - [TRPO] iter = 115000 dist_mean = 0.3044 dist_std = 0.7837 vf_loss = 0.2490 grad_norm = 0.4879 nat_grad_norm = 0.4987 cg_residual = 0.0149 step_size = 0.4254 reward = -0.0000 fps = 5 mse_loss = 1.0038 
2022-07-08 10:27:33.115335 - gail/main.py:201 - [Discriminator] iter = 115000 loss = -3.7081 grad_norm = 3.0084 grad_penalty = 0.3379 regularization = 0.0000 true_logits = -0.0430 fake_logits = -4.0889 true_prob = 0.5123 fake_prob = 0.0261 
2022-07-08 10:28:22.248091 - gail/main.py:142 - [Evaluate] iter = 115000 episode={ returns = 312.8352 lengths = 123 } discounted_episode={ returns = 289.0721 lengths = 123 } 
2022-07-08 10:28:51.975813 - gail/main.py:174 - [TRPO] iter = 116000 dist_mean = 0.4144 dist_std = 0.7827 vf_loss = 0.6487 grad_norm = 0.5815 nat_grad_norm = 0.7204 cg_residual = 0.0232 step_size = 0.3333 reward = -0.0000 fps = 12 mse_loss = 1.1099 
2022-07-08 10:29:21.450096 - gail/main.py:174 - [TRPO] iter = 117000 dist_mean = 0.3356 dist_std = 0.7829 vf_loss = 0.2853 grad_norm = 0.6045 nat_grad_norm = 0.5287 cg_residual = 0.0329 step_size = 0.3518 reward = -0.0000 fps = 9 mse_loss = 1.0327 
2022-07-08 10:29:51.091766 - gail/main.py:174 - [TRPO] iter = 118000 dist_mean = 0.3177 dist_std = 0.7843 vf_loss = 0.3298 grad_norm = 0.4958 nat_grad_norm = 0.5126 cg_residual = 0.0140 step_size = 0.3952 reward = -0.0000 fps = 7 mse_loss = 1.0043 
2022-07-08 10:30:20.353274 - gail/main.py:174 - [TRPO] iter = 119000 dist_mean = 0.3512 dist_std = 0.7844 vf_loss = 0.3870 grad_norm = 0.6867 nat_grad_norm = 0.7193 cg_residual = 0.0277 step_size = 0.3192 reward = -0.0000 fps = 5 mse_loss = 0.9754 
2022-07-08 10:30:49.969499 - gail/main.py:174 - [TRPO] iter = 120000 dist_mean = 0.3725 dist_std = 0.7837 vf_loss = 0.4328 grad_norm = 0.5185 nat_grad_norm = 0.6190 cg_residual = 0.0202 step_size = 0.3681 reward = -0.0000 fps = 5 mse_loss = 1.0229 
2022-07-08 10:30:50.774501 - gail/main.py:201 - [Discriminator] iter = 120000 loss = -3.8652 grad_norm = 3.0519 grad_penalty = 0.3635 regularization = 0.0000 true_logits = -0.0433 fake_logits = -4.2719 true_prob = 0.5139 fake_prob = 0.0206 
2022-07-08 10:31:45.853441 - gail/main.py:142 - [Evaluate] iter = 120000 episode={ returns = 301.6089 lengths = 123 } discounted_episode={ returns = 278.8715 lengths = 123 } 
