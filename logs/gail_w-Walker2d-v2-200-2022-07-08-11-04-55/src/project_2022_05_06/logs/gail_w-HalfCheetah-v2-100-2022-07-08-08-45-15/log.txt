2022-07-08 08:45:15.296573 - utils/flags.py:257 - log_dir = logs/gail_w-HalfCheetah-v2-100-2022-07-08-08-45-15
2022-07-08 08:46:24.420655 - gail/utils/replay_buffer.py:86 - Load dataset from /content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/HalfCheetah-v2
2022-07-08 08:46:33.222805 - gail/main.py:80 - Expert Reward 6594.485577
2022-07-08 08:46:33.611953 - gail/main.py:84 - Original dataset size 3000
2022-07-08 08:46:33.656876 - gail/main.py:86 - Subsampled dataset size 3000
2022-07-08 08:46:33.659703 - gail/main.py:87 - np random: 864 random : 786
2022-07-08 08:46:33.664576 - gail/main.py:91 - Sampled obs: 0.6422, acs: 0.2570
2022-07-08 08:46:34.706233 - lunzi/nn/flat_param.py:17 - Enabling flattening... ['GaussianMLPPolicy_1/log_std:0', 'GaussianMLPPolicy_1/Variable:0', 'GaussianMLPPolicy_1/Variable_1:0', 'GaussianMLPPolicy_1/Variable_2:0', 'GaussianMLPPolicy_1/Variable_3:0', 'GaussianMLPPolicy_1/Variable_4:0', 'GaussianMLPPolicy_1/Variable_5:0']
2022-07-08 08:46:41.824907 - gail/discriminator/discriminator.py:30 - Use predefined normalization.
2022-07-08 08:46:41.826598 - gail/discriminator/discriminator.py:33 - Normalizer loc:[[-1.0330443e-01 -9.3026832e-03  2.9791275e-01  8.1040427e-02
   1.7489190e-01  4.0859219e-01 -3.0232381e-02 -9.5055901e-02
   7.1118622e+00  5.5424552e-03 -1.8441556e-01  3.1117105e-01
  -2.7393317e-01  2.6658252e-01  4.0665963e-01 -2.8198040e-01
   2.5822452e-01]] 
 scale:[[ 0.03102476  0.07177044  0.37663096  0.5276757   0.46577376  0.12057849
   0.35446092  0.3551628   1.290817    0.4731574   1.4939526  12.314076
  14.637607   14.150712    3.6817648  11.124868    9.962843  ]]
2022-07-08 08:46:45.373051 - gail/discriminator/discriminator.py:67 - Discriminator uses Wasserstein distance.
2022-07-08 08:46:45.375742 - gail/discriminator/discriminator.py:68 - [<tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable:0' shape=(23, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_1:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_2:0' shape=(100, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_3:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_4:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_5:0' shape=(1,) dtype=float32_ref>]
2022-07-08 08:46:45.377448 - gail/discriminator/discriminator.py:69 - Use gradient penalty regularization (coef = 10.000000)
2022-07-08 08:46:46.170836 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions_mean]
2022-07-08 08:47:55.415614 - gail/main.py:142 - [Evaluate] iter = 0 episode={ returns = -0.4623 lengths = 1000 } discounted_episode={ returns = -0.4535 lengths = 1000 } 
2022-07-08 08:47:55.417692 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions]
2022-07-08 08:48:03.196731 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [fake_states fake_actions] => [scaled_neural_reward]
2022-07-08 08:48:03.495876 - lunzi/nn/module.py:71 - [MLPVFunction] is making TensorFlow callables, key = [states] => [values]
2022-07-08 08:48:04.018956 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [] => [sync_old]
2022-07-08 08:48:04.266101 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss flat_grad dist_std mean_kl dist_mean]
2022-07-08 08:48:05.590208 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states tangents actions] => [hessian_vec_prod]
2022-07-08 08:48:08.451525 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [] => [get_flat]
2022-07-08 08:48:08.817603 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [feed_flat] => [set_flat]
2022-07-08 08:48:09.202753 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss mean_kl]
2022-07-08 08:48:09.701086 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [train_vf vf_loss]
2022-07-08 08:48:10.506483 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [vf_loss]
2022-07-08 08:48:10.779440 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states actions_] => [mse_loss]
2022-07-08 08:48:11.116491 - gail/main.py:174 - [TRPO] iter = 1000 dist_mean = -0.0000 dist_std = 1.0000 vf_loss = 0.6155 grad_norm = 0.2855 nat_grad_norm = 0.5289 cg_residual = 0.0000 step_size = 0.4203 reward = 0.0000 fps = 11 mse_loss = 0.9499 
2022-07-08 08:48:17.098863 - gail/main.py:174 - [TRPO] iter = 2000 dist_mean = -0.0088 dist_std = 1.0052 vf_loss = 1.3476 grad_norm = 0.2954 nat_grad_norm = 0.5110 cg_residual = 0.0000 step_size = 0.4162 reward = 0.0000 fps = 10 mse_loss = 0.8927 
2022-07-08 08:48:23.017151 - gail/main.py:174 - [TRPO] iter = 3000 dist_mean = 0.0005 dist_std = 1.0109 vf_loss = 0.6947 grad_norm = 0.3094 nat_grad_norm = 0.5694 cg_residual = 0.0000 step_size = 0.3867 reward = 0.0000 fps = 10 mse_loss = 0.9085 
2022-07-08 08:48:29.124114 - gail/main.py:174 - [TRPO] iter = 4000 dist_mean = 0.0034 dist_std = 1.0135 vf_loss = 0.7183 grad_norm = 0.2627 nat_grad_norm = 0.4766 cg_residual = 0.0000 step_size = 0.4691 reward = -0.0000 fps = 9 mse_loss = 0.9739 
2022-07-08 08:48:35.326437 - gail/main.py:174 - [TRPO] iter = 5000 dist_mean = 0.0106 dist_std = 1.0138 vf_loss = 0.8461 grad_norm = 0.3189 nat_grad_norm = 0.5651 cg_residual = 0.0000 step_size = 0.3892 reward = -0.0000 fps = 9 mse_loss = 1.0079 
2022-07-08 08:48:35.329144 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [true_states true_actions fake_states fake_actions true_masks] => [train loss true_logits fake_logits true_prob fake_prob grad_norm grad_penalty regularization]
2022-07-08 08:48:37.574510 - gail/main.py:201 - [Discriminator] iter = 5000 loss = 0.3933 grad_norm = 38.9849 grad_penalty = 1.8469 regularization = 0.0000 true_logits = -0.0813 fake_logits = -1.5349 true_prob = 0.4797 fake_prob = 0.2288 
2022-07-08 08:49:50.163070 - gail/main.py:142 - [Evaluate] iter = 5000 episode={ returns = -4.0572 lengths = 1000 } discounted_episode={ returns = -2.6743 lengths = 1000 } 
2022-07-08 08:49:56.066467 - gail/main.py:174 - [TRPO] iter = 6000 dist_mean = 0.0169 dist_std = 1.0109 vf_loss = 1.0888 grad_norm = 0.3653 nat_grad_norm = 0.7862 cg_residual = 0.0000 step_size = 0.3024 reward = -0.0000 fps = 12 mse_loss = 0.9676 
2022-07-08 08:50:03.028629 - gail/main.py:174 - [TRPO] iter = 7000 dist_mean = 0.0231 dist_std = 1.0130 vf_loss = 3.8571 grad_norm = 0.2787 nat_grad_norm = 0.5615 cg_residual = 0.0001 step_size = 0.4153 reward = 0.0000 fps = 11 mse_loss = 1.0235 
2022-07-08 08:50:13.179517 - gail/main.py:174 - [TRPO] iter = 8000 dist_mean = 0.0182 dist_std = 1.0125 vf_loss = 0.9335 grad_norm = 0.4036 nat_grad_norm = 0.5168 cg_residual = 0.0001 step_size = 0.3964 reward = -0.0000 fps = 10 mse_loss = 1.0459 
2022-07-08 08:50:19.188010 - gail/main.py:174 - [TRPO] iter = 9000 dist_mean = 0.0192 dist_std = 1.0068 vf_loss = 1.6190 grad_norm = 0.3112 nat_grad_norm = 0.5929 cg_residual = 0.0000 step_size = 0.3840 reward = 0.0000 fps = 9 mse_loss = 1.0027 
2022-07-08 08:50:25.471126 - gail/main.py:174 - [TRPO] iter = 10000 dist_mean = 0.0214 dist_std = 1.0072 vf_loss = 3.6663 grad_norm = 0.3028 nat_grad_norm = 0.5128 cg_residual = 0.0000 step_size = 0.4310 reward = 0.0000 fps = 9 mse_loss = 0.9865 
2022-07-08 08:50:25.709810 - gail/main.py:201 - [Discriminator] iter = 10000 loss = -0.3619 grad_norm = 34.4331 grad_penalty = 1.9658 regularization = 0.0000 true_logits = -0.0334 fake_logits = -2.3611 true_prob = 0.4917 fake_prob = 0.1517 
2022-07-08 08:51:34.857917 - gail/main.py:142 - [Evaluate] iter = 10000 episode={ returns = -10.1127 lengths = 1000 } discounted_episode={ returns = -6.7216 lengths = 1000 } 
2022-07-08 08:51:40.758616 - gail/main.py:174 - [TRPO] iter = 11000 dist_mean = -0.0233 dist_std = 1.0061 vf_loss = 1.3460 grad_norm = 0.3120 nat_grad_norm = 0.5757 cg_residual = 0.0001 step_size = 0.3977 reward = -0.0000 fps = 13 mse_loss = 0.9906 
2022-07-08 08:51:47.059593 - gail/main.py:174 - [TRPO] iter = 12000 dist_mean = -0.0152 dist_std = 1.0105 vf_loss = 2.8153 grad_norm = 0.3072 nat_grad_norm = 0.4712 cg_residual = 0.0000 step_size = 0.4671 reward = -0.0000 fps = 12 mse_loss = 0.9661 
2022-07-08 08:51:52.946404 - gail/main.py:174 - [TRPO] iter = 13000 dist_mean = 0.0180 dist_std = 1.0171 vf_loss = 1.9099 grad_norm = 0.2819 nat_grad_norm = 0.5699 cg_residual = 0.0000 step_size = 0.4182 reward = -0.0000 fps = 11 mse_loss = 0.9853 
2022-07-08 08:51:58.977910 - gail/main.py:174 - [TRPO] iter = 14000 dist_mean = 0.0236 dist_std = 1.0202 vf_loss = 4.0901 grad_norm = 0.2811 nat_grad_norm = 0.5132 cg_residual = 0.0000 step_size = 0.4333 reward = -0.0000 fps = 10 mse_loss = 1.0281 
2022-07-08 08:52:04.893656 - gail/main.py:174 - [TRPO] iter = 15000 dist_mean = 0.0223 dist_std = 1.0182 vf_loss = 2.5975 grad_norm = 0.3162 nat_grad_norm = 0.6553 cg_residual = 0.0000 step_size = 0.3637 reward = -0.0000 fps = 10 mse_loss = 0.9883 
2022-07-08 08:52:05.142373 - gail/main.py:201 - [Discriminator] iter = 15000 loss = -2.5453 grad_norm = 22.1433 grad_penalty = 0.8238 regularization = 0.0000 true_logits = 0.0332 fake_logits = -3.3359 true_prob = 0.5083 fake_prob = 0.0509 
2022-07-08 08:53:13.340206 - gail/main.py:142 - [Evaluate] iter = 15000 episode={ returns = -10.6767 lengths = 1000 } discounted_episode={ returns = -6.7731 lengths = 1000 } 
2022-07-08 08:53:19.304852 - gail/main.py:174 - [TRPO] iter = 16000 dist_mean = 0.0285 dist_std = 1.0198 vf_loss = 2.7145 grad_norm = 0.3407 nat_grad_norm = 0.6946 cg_residual = 0.0000 step_size = 0.3396 reward = 0.0000 fps = 13 mse_loss = 0.9980 
2022-07-08 08:53:25.631912 - gail/main.py:174 - [TRPO] iter = 17000 dist_mean = 0.0648 dist_std = 1.0211 vf_loss = 4.6963 grad_norm = 0.2955 nat_grad_norm = 0.5285 cg_residual = 0.0000 step_size = 0.4272 reward = 0.0000 fps = 12 mse_loss = 0.9699 
2022-07-08 08:53:31.709452 - gail/main.py:174 - [TRPO] iter = 18000 dist_mean = 0.0423 dist_std = 1.0226 vf_loss = 6.8839 grad_norm = 0.2906 nat_grad_norm = 0.5204 cg_residual = 0.0000 step_size = 0.4403 reward = 0.0000 fps = 11 mse_loss = 1.0002 
2022-07-08 08:53:38.019638 - gail/main.py:174 - [TRPO] iter = 19000 dist_mean = 0.0432 dist_std = 1.0162 vf_loss = 6.2110 grad_norm = 0.2903 nat_grad_norm = 0.4909 cg_residual = 0.0000 step_size = 0.4459 reward = -0.0000 fps = 10 mse_loss = 1.0264 
2022-07-08 08:53:43.870953 - gail/main.py:174 - [TRPO] iter = 20000 dist_mean = 0.0545 dist_std = 1.0193 vf_loss = 4.9877 grad_norm = 0.2868 nat_grad_norm = 0.5528 cg_residual = 0.0000 step_size = 0.4364 reward = 0.0000 fps = 10 mse_loss = 1.0043 
2022-07-08 08:53:44.129211 - gail/main.py:201 - [Discriminator] iter = 20000 loss = 0.8056 grad_norm = 25.9049 grad_penalty = 1.5739 regularization = 0.0000 true_logits = 0.1060 fake_logits = -0.6623 true_prob = 0.5264 fake_prob = 0.3449 
2022-07-08 08:54:51.261167 - gail/main.py:142 - [Evaluate] iter = 20000 episode={ returns = -4.9802 lengths = 1000 } discounted_episode={ returns = -3.9296 lengths = 1000 } 
2022-07-08 08:54:57.720817 - gail/main.py:174 - [TRPO] iter = 21000 dist_mean = 0.0611 dist_std = 1.0222 vf_loss = 3.5499 grad_norm = 0.3057 nat_grad_norm = 0.4804 cg_residual = 0.0000 step_size = 0.4698 reward = 0.0000 fps = 13 mse_loss = 0.9638 
2022-07-08 08:55:04.634781 - gail/main.py:174 - [TRPO] iter = 22000 dist_mean = 0.0702 dist_std = 1.0230 vf_loss = 1.2693 grad_norm = 0.3479 nat_grad_norm = 0.5611 cg_residual = 0.0000 step_size = 0.4183 reward = -0.0000 fps = 12 mse_loss = 1.0051 
2022-07-08 08:55:11.437686 - gail/main.py:174 - [TRPO] iter = 23000 dist_mean = 0.0795 dist_std = 1.0216 vf_loss = 3.1766 grad_norm = 0.2692 nat_grad_norm = 0.4861 cg_residual = 0.0000 step_size = 0.4772 reward = 0.0000 fps = 11 mse_loss = 0.9742 
2022-07-08 08:55:17.281586 - gail/main.py:174 - [TRPO] iter = 24000 dist_mean = 0.1043 dist_std = 1.0218 vf_loss = 1.7529 grad_norm = 0.3681 nat_grad_norm = 0.5345 cg_residual = 0.0000 step_size = 0.4106 reward = -0.0000 fps = 10 mse_loss = 1.0032 
2022-07-08 08:55:22.980534 - gail/main.py:174 - [TRPO] iter = 25000 dist_mean = 0.0934 dist_std = 1.0229 vf_loss = 2.2879 grad_norm = 0.2642 nat_grad_norm = 0.5454 cg_residual = 0.0000 step_size = 0.4479 reward = -0.0000 fps = 10 mse_loss = 1.0384 
2022-07-08 08:55:23.225930 - gail/main.py:201 - [Discriminator] iter = 25000 loss = -2.9322 grad_norm = 25.2504 grad_penalty = 0.7118 regularization = 0.0000 true_logits = 0.1558 fake_logits = -3.4881 true_prob = 0.5388 fake_prob = 0.0518 
2022-07-08 08:56:31.033045 - gail/main.py:142 - [Evaluate] iter = 25000 episode={ returns = -19.0641 lengths = 1000 } discounted_episode={ returns = -12.3346 lengths = 1000 } 
2022-07-08 08:56:36.989424 - gail/main.py:174 - [TRPO] iter = 26000 dist_mean = 0.1373 dist_std = 1.0203 vf_loss = 3.3495 grad_norm = 0.3536 nat_grad_norm = 0.5319 cg_residual = 0.0001 step_size = 0.4062 reward = -0.0000 fps = 13 mse_loss = 1.0386 
2022-07-08 08:56:43.271615 - gail/main.py:174 - [TRPO] iter = 27000 dist_mean = 0.1020 dist_std = 1.0126 vf_loss = 2.3186 grad_norm = 0.3468 nat_grad_norm = 0.6294 cg_residual = 0.0000 step_size = 0.3678 reward = 0.0000 fps = 12 mse_loss = 1.0199 
2022-07-08 08:56:49.327160 - gail/main.py:174 - [TRPO] iter = 28000 dist_mean = 0.1620 dist_std = 1.0150 vf_loss = 5.5702 grad_norm = 0.3095 nat_grad_norm = 0.5669 cg_residual = 0.0001 step_size = 0.4194 reward = -0.0000 fps = 11 mse_loss = 1.0362 
2022-07-08 08:56:55.170710 - gail/main.py:174 - [TRPO] iter = 29000 dist_mean = 0.1456 dist_std = 1.0159 vf_loss = 2.4632 grad_norm = 0.3267 nat_grad_norm = 0.5690 cg_residual = 0.0000 step_size = 0.4081 reward = -0.0000 fps = 10 mse_loss = 1.1037 
2022-07-08 08:57:01.079559 - gail/main.py:174 - [TRPO] iter = 30000 dist_mean = 0.1499 dist_std = 1.0090 vf_loss = 1.1807 grad_norm = 0.3343 nat_grad_norm = 0.5407 cg_residual = 0.0000 step_size = 0.4143 reward = 0.0000 fps = 10 mse_loss = 1.0784 
2022-07-08 08:57:01.289932 - gail/main.py:201 - [Discriminator] iter = 30000 loss = -0.6850 grad_norm = 29.3487 grad_penalty = 1.2840 regularization = 0.0000 true_logits = 0.1955 fake_logits = -1.7735 true_prob = 0.5485 fake_prob = 0.1855 
2022-07-08 08:58:08.320936 - gail/main.py:142 - [Evaluate] iter = 30000 episode={ returns = -33.0873 lengths = 1000 } discounted_episode={ returns = -21.6265 lengths = 1000 } 
2022-07-08 08:58:13.900837 - gail/main.py:174 - [TRPO] iter = 31000 dist_mean = 0.1840 dist_std = 1.0074 vf_loss = 4.3252 grad_norm = 0.3763 nat_grad_norm = 0.5895 cg_residual = 0.0000 step_size = 0.3706 reward = 0.0000 fps = 13 mse_loss = 1.0875 
2022-07-08 08:58:19.788864 - gail/main.py:174 - [TRPO] iter = 32000 dist_mean = 0.2019 dist_std = 1.0050 vf_loss = 1.9712 grad_norm = 0.3304 nat_grad_norm = 0.5065 cg_residual = 0.0000 step_size = 0.4483 reward = 0.0000 fps = 12 mse_loss = 1.1095 
2022-07-08 08:58:25.644866 - gail/main.py:174 - [TRPO] iter = 33000 dist_mean = 0.2106 dist_std = 0.9996 vf_loss = 1.7871 grad_norm = 0.3570 nat_grad_norm = 0.5647 cg_residual = 0.0001 step_size = 0.3897 reward = 0.0000 fps = 11 mse_loss = 1.0805 
2022-07-08 08:58:31.533649 - gail/main.py:174 - [TRPO] iter = 34000 dist_mean = 0.2097 dist_std = 0.9953 vf_loss = 0.7664 grad_norm = 0.4186 nat_grad_norm = 0.5359 cg_residual = 0.0001 step_size = 0.3817 reward = 0.0000 fps = 11 mse_loss = 0.9805 
2022-07-08 08:58:37.466970 - gail/main.py:174 - [TRPO] iter = 35000 dist_mean = 0.2188 dist_std = 0.9960 vf_loss = 0.8671 grad_norm = 0.3630 nat_grad_norm = 0.5353 cg_residual = 0.0001 step_size = 0.4062 reward = -0.0000 fps = 10 mse_loss = 0.9529 
2022-07-08 08:58:37.737251 - gail/main.py:201 - [Discriminator] iter = 35000 loss = -0.2401 grad_norm = 15.9093 grad_penalty = 1.0870 regularization = 0.0000 true_logits = 0.2522 fake_logits = -1.0749 true_prob = 0.5623 fake_prob = 0.2610 
2022-07-08 08:59:46.166892 - gail/main.py:142 - [Evaluate] iter = 35000 episode={ returns = -40.0057 lengths = 1000 } discounted_episode={ returns = -25.7558 lengths = 1000 } 
2022-07-08 08:59:52.073988 - gail/main.py:174 - [TRPO] iter = 36000 dist_mean = 0.2294 dist_std = 0.9887 vf_loss = 1.2824 grad_norm = 0.3744 nat_grad_norm = 0.5772 cg_residual = 0.0001 step_size = 0.3803 reward = 0.0000 fps = 13 mse_loss = 0.9582 
2022-07-08 08:59:57.879423 - gail/main.py:174 - [TRPO] iter = 37000 dist_mean = 0.1818 dist_std = 0.9886 vf_loss = 1.4700 grad_norm = 0.3455 nat_grad_norm = 0.5389 cg_residual = 0.0000 step_size = 0.4118 reward = -0.0000 fps = 12 mse_loss = 0.9878 
2022-07-08 09:00:04.365136 - gail/main.py:174 - [TRPO] iter = 38000 dist_mean = 0.2313 dist_std = 0.9939 vf_loss = 1.7274 grad_norm = 0.3765 nat_grad_norm = 0.5776 cg_residual = 0.0001 step_size = 0.3851 reward = 0.0000 fps = 11 mse_loss = 0.9820 
2022-07-08 09:00:10.967215 - gail/main.py:174 - [TRPO] iter = 39000 dist_mean = 0.1931 dist_std = 0.9967 vf_loss = 2.8112 grad_norm = 0.3735 nat_grad_norm = 0.5898 cg_residual = 0.0001 step_size = 0.3735 reward = -0.0000 fps = 10 mse_loss = 0.9864 
2022-07-08 09:00:17.619719 - gail/main.py:174 - [TRPO] iter = 40000 dist_mean = 0.2457 dist_std = 0.9922 vf_loss = 1.2412 grad_norm = 0.3372 nat_grad_norm = 0.5323 cg_residual = 0.0001 step_size = 0.4345 reward = -0.0000 fps = 10 mse_loss = 1.0130 
2022-07-08 09:00:17.889334 - gail/main.py:201 - [Discriminator] iter = 40000 loss = -0.6912 grad_norm = 15.3772 grad_penalty = 0.9456 regularization = 0.0000 true_logits = 0.3220 fake_logits = -1.3148 true_prob = 0.5790 fake_prob = 0.2198 
2022-07-08 09:01:31.876849 - gail/main.py:142 - [Evaluate] iter = 40000 episode={ returns = -41.6862 lengths = 1000 } discounted_episode={ returns = -27.6150 lengths = 1000 } 
2022-07-08 09:01:37.535164 - gail/main.py:174 - [TRPO] iter = 41000 dist_mean = 0.2145 dist_std = 0.9847 vf_loss = 4.4396 grad_norm = 0.3390 nat_grad_norm = 0.5778 cg_residual = 0.0000 step_size = 0.4000 reward = -0.0000 fps = 12 mse_loss = 0.9533 
2022-07-08 09:01:43.330028 - gail/main.py:174 - [TRPO] iter = 42000 dist_mean = 0.2434 dist_std = 0.9871 vf_loss = 1.1560 grad_norm = 0.3954 nat_grad_norm = 0.5742 cg_residual = 0.0001 step_size = 0.3898 reward = 0.0000 fps = 11 mse_loss = 0.9623 
2022-07-08 09:01:49.255416 - gail/main.py:174 - [TRPO] iter = 43000 dist_mean = 0.2592 dist_std = 0.9898 vf_loss = 0.9575 grad_norm = 0.3553 nat_grad_norm = 0.5648 cg_residual = 0.0001 step_size = 0.4177 reward = 0.0000 fps = 10 mse_loss = 0.9173 
2022-07-08 09:01:55.067271 - gail/main.py:174 - [TRPO] iter = 44000 dist_mean = 0.2566 dist_std = 0.9876 vf_loss = 1.0315 grad_norm = 0.3742 nat_grad_norm = 0.5285 cg_residual = 0.0001 step_size = 0.4225 reward = -0.0000 fps = 10 mse_loss = 0.8705 
2022-07-08 09:02:00.758515 - gail/main.py:174 - [TRPO] iter = 45000 dist_mean = 0.2648 dist_std = 0.9785 vf_loss = 0.5719 grad_norm = 0.3688 nat_grad_norm = 0.5008 cg_residual = 0.0001 step_size = 0.4497 reward = 0.0000 fps = 9 mse_loss = 0.8755 
2022-07-08 09:02:01.008330 - gail/main.py:201 - [Discriminator] iter = 45000 loss = -0.9098 grad_norm = 14.3278 grad_penalty = 0.8616 regularization = 0.0000 true_logits = 0.3857 fake_logits = -1.3857 true_prob = 0.5939 fake_prob = 0.2081 
2022-07-08 09:03:10.075532 - gail/main.py:142 - [Evaluate] iter = 45000 episode={ returns = -67.8794 lengths = 1000 } discounted_episode={ returns = -43.3818 lengths = 1000 } 
2022-07-08 09:03:15.768575 - gail/main.py:174 - [TRPO] iter = 46000 dist_mean = 0.2727 dist_std = 0.9836 vf_loss = 0.7996 grad_norm = 0.4401 nat_grad_norm = 0.5868 cg_residual = 0.0002 step_size = 0.3582 reward = 0.0000 fps = 13 mse_loss = 0.8581 
2022-07-08 09:03:21.857976 - gail/main.py:174 - [TRPO] iter = 47000 dist_mean = 0.2869 dist_std = 0.9855 vf_loss = 0.7223 grad_norm = 0.4446 nat_grad_norm = 0.5807 cg_residual = 0.0001 step_size = 0.3743 reward = -0.0000 fps = 12 mse_loss = 0.8115 
2022-07-08 09:03:27.761014 - gail/main.py:174 - [TRPO] iter = 48000 dist_mean = 0.2281 dist_std = 0.9811 vf_loss = 3.9300 grad_norm = 0.3458 nat_grad_norm = 0.5534 cg_residual = 0.0001 step_size = 0.4267 reward = -0.0000 fps = 11 mse_loss = 0.8097 
2022-07-08 09:03:34.220325 - gail/main.py:174 - [TRPO] iter = 49000 dist_mean = 0.3007 dist_std = 0.9816 vf_loss = 1.0749 grad_norm = 0.3848 nat_grad_norm = 0.6540 cg_residual = 0.0002 step_size = 0.3836 reward = -0.0000 fps = 10 mse_loss = 0.8127 
2022-07-08 09:03:40.162836 - gail/main.py:174 - [TRPO] iter = 50000 dist_mean = 0.2519 dist_std = 0.9831 vf_loss = 4.8162 grad_norm = 0.3937 nat_grad_norm = 0.6848 cg_residual = 0.0001 step_size = 0.3581 reward = 0.0000 fps = 10 mse_loss = 0.8065 
2022-07-08 09:03:40.388384 - gail/main.py:201 - [Discriminator] iter = 50000 loss = -1.7827 grad_norm = 16.4631 grad_penalty = 0.9549 regularization = 0.0000 true_logits = 0.4618 fake_logits = -2.2758 true_prob = 0.6113 fake_prob = 0.1314 
2022-07-08 09:05:20.994675 - gail/main.py:142 - [Evaluate] iter = 50000 episode={ returns = -80.5169 lengths = 1000 } discounted_episode={ returns = -52.2835 lengths = 1000 } 
2022-07-08 09:05:35.526011 - gail/main.py:174 - [TRPO] iter = 51000 dist_mean = 0.2989 dist_std = 0.9809 vf_loss = 0.5899 grad_norm = 0.3864 nat_grad_norm = 0.5476 cg_residual = 0.0002 step_size = 0.4167 reward = 0.0000 fps = 8 mse_loss = 0.8329 
2022-07-08 09:05:49.289866 - gail/main.py:174 - [TRPO] iter = 52000 dist_mean = 0.3184 dist_std = 0.9788 vf_loss = 0.7270 grad_norm = 0.4184 nat_grad_norm = 0.6256 cg_residual = 0.0002 step_size = 0.3744 reward = 0.0000 fps = 7 mse_loss = 0.8409 
2022-07-08 09:06:01.920113 - gail/main.py:174 - [TRPO] iter = 53000 dist_mean = 0.3185 dist_std = 0.9784 vf_loss = 0.5538 grad_norm = 0.4103 nat_grad_norm = 0.5768 cg_residual = 0.0003 step_size = 0.3961 reward = -0.0000 fps = 7 mse_loss = 0.8436 
2022-07-08 09:06:14.889709 - gail/main.py:174 - [TRPO] iter = 54000 dist_mean = 0.3196 dist_std = 0.9759 vf_loss = 0.6255 grad_norm = 0.4390 nat_grad_norm = 0.6630 cg_residual = 0.0003 step_size = 0.3564 reward = 0.0000 fps = 6 mse_loss = 0.7917 
2022-07-08 09:06:27.329565 - gail/main.py:174 - [TRPO] iter = 55000 dist_mean = 0.3331 dist_std = 0.9718 vf_loss = 0.5712 grad_norm = 0.3950 nat_grad_norm = 0.6249 cg_residual = 0.0003 step_size = 0.3757 reward = -0.0000 fps = 5 mse_loss = 0.7591 
2022-07-08 09:06:27.888136 - gail/main.py:201 - [Discriminator] iter = 55000 loss = -1.5867 grad_norm = 13.2073 grad_penalty = 0.7605 regularization = 0.0000 true_logits = 0.5543 fake_logits = -1.7930 true_prob = 0.6313 fake_prob = 0.1512 
2022-07-08 09:09:09.151748 - gail/main.py:142 - [Evaluate] iter = 55000 episode={ returns = -109.6824 lengths = 1000 } discounted_episode={ returns = -70.0702 lengths = 1000 } 
2022-07-08 09:09:29.196772 - gail/main.py:174 - [TRPO] iter = 56000 dist_mean = 0.3424 dist_std = 0.9687 vf_loss = 0.4623 grad_norm = 0.4570 nat_grad_norm = 0.5961 cg_residual = 0.0003 step_size = 0.3760 reward = -0.0000 fps = 5 mse_loss = 0.7392 
2022-07-08 09:09:46.684120 - gail/main.py:174 - [TRPO] iter = 57000 dist_mean = 0.3395 dist_std = 0.9645 vf_loss = 0.3427 grad_norm = 0.3936 nat_grad_norm = 0.6016 cg_residual = 0.0002 step_size = 0.3902 reward = 0.0000 fps = 5 mse_loss = 0.6862 
2022-07-08 09:10:03.794285 - gail/main.py:174 - [TRPO] iter = 58000 dist_mean = 0.3570 dist_std = 0.9632 vf_loss = 0.4734 grad_norm = 0.5798 nat_grad_norm = 0.5998 cg_residual = 0.0003 step_size = 0.3433 reward = 0.0000 fps = 4 mse_loss = 0.7088 
2022-07-08 09:10:25.063963 - gail/main.py:174 - [TRPO] iter = 59000 dist_mean = 0.3552 dist_std = 0.9646 vf_loss = 0.3398 grad_norm = 0.3908 nat_grad_norm = 0.5594 cg_residual = 0.0003 step_size = 0.4167 reward = -0.0000 fps = 4 mse_loss = 0.6759 
2022-07-08 09:10:45.688935 - gail/main.py:174 - [TRPO] iter = 60000 dist_mean = 0.3589 dist_std = 0.9545 vf_loss = 0.2731 grad_norm = 0.4412 nat_grad_norm = 0.6127 cg_residual = 0.0007 step_size = 0.3868 reward = -0.0000 fps = 3 mse_loss = 0.6614 
2022-07-08 09:10:46.391862 - gail/main.py:201 - [Discriminator] iter = 60000 loss = -1.9706 grad_norm = 13.4872 grad_penalty = 0.7744 regularization = 0.0000 true_logits = 0.6622 fake_logits = -2.0828 true_prob = 0.6543 fake_prob = 0.1186 
2022-07-08 09:15:32.655916 - gail/main.py:142 - [Evaluate] iter = 60000 episode={ returns = -131.2378 lengths = 1000 } discounted_episode={ returns = -82.9888 lengths = 1000 } 
2022-07-08 09:15:56.095984 - gail/main.py:174 - [TRPO] iter = 61000 dist_mean = 0.3627 dist_std = 0.9549 vf_loss = 0.3436 grad_norm = 0.4162 nat_grad_norm = 0.5845 cg_residual = 0.0004 step_size = 0.3939 reward = 0.0000 fps = 3 mse_loss = 0.6805 
2022-07-08 09:16:18.303927 - gail/main.py:174 - [TRPO] iter = 62000 dist_mean = 0.3645 dist_std = 0.9565 vf_loss = 0.4182 grad_norm = 0.4624 nat_grad_norm = 0.6106 cg_residual = 0.0005 step_size = 0.3690 reward = -0.0000 fps = 3 mse_loss = 0.6549 
2022-07-08 09:16:41.923314 - gail/main.py:174 - [TRPO] iter = 63000 dist_mean = 0.3861 dist_std = 0.9569 vf_loss = 0.3498 grad_norm = 0.4478 nat_grad_norm = 0.5736 cg_residual = 0.0005 step_size = 0.3961 reward = -0.0000 fps = 2 mse_loss = 0.6414 
2022-07-08 09:17:05.586110 - gail/main.py:174 - [TRPO] iter = 64000 dist_mean = 0.3660 dist_std = 0.9526 vf_loss = 0.3343 grad_norm = 0.4523 nat_grad_norm = 0.6070 cg_residual = 0.0007 step_size = 0.3743 reward = -0.0000 fps = 2 mse_loss = 0.6448 
2022-07-08 09:17:28.963459 - gail/main.py:174 - [TRPO] iter = 65000 dist_mean = 0.3809 dist_std = 0.9526 vf_loss = 0.4780 grad_norm = 0.4492 nat_grad_norm = 0.6018 cg_residual = 0.0007 step_size = 0.3682 reward = 0.0000 fps = 2 mse_loss = 0.6049 
2022-07-08 09:17:29.758198 - gail/main.py:201 - [Discriminator] iter = 65000 loss = -2.5229 grad_norm = 14.2517 grad_penalty = 0.6811 regularization = 0.0000 true_logits = 0.6876 fake_logits = -2.5164 true_prob = 0.6589 fake_prob = 0.0817 
2022-07-08 09:22:08.524107 - gail/main.py:142 - [Evaluate] iter = 65000 episode={ returns = -160.4824 lengths = 1000 } discounted_episode={ returns = -101.4913 lengths = 1000 } 
2022-07-08 09:22:31.391164 - gail/main.py:174 - [TRPO] iter = 66000 dist_mean = 0.3856 dist_std = 0.9493 vf_loss = 0.3558 grad_norm = 0.4826 nat_grad_norm = 0.6158 cg_residual = 0.0009 step_size = 0.3650 reward = -0.0000 fps = 3 mse_loss = 0.5936 
2022-07-08 09:22:54.621815 - gail/main.py:174 - [TRPO] iter = 67000 dist_mean = 0.3897 dist_std = 0.9483 vf_loss = 0.5946 grad_norm = 0.3833 nat_grad_norm = 0.5782 cg_residual = 0.0004 step_size = 0.4066 reward = -0.0000 fps = 3 mse_loss = 0.5885 
2022-07-08 09:23:17.813011 - gail/main.py:174 - [TRPO] iter = 68000 dist_mean = 0.3850 dist_std = 0.9437 vf_loss = 0.4756 grad_norm = 0.4625 nat_grad_norm = 0.5961 cg_residual = 0.0007 step_size = 0.3654 reward = 0.0000 fps = 2 mse_loss = 0.6103 
2022-07-08 09:23:41.042069 - gail/main.py:174 - [TRPO] iter = 69000 dist_mean = 0.3741 dist_std = 0.9416 vf_loss = 0.5862 grad_norm = 0.4793 nat_grad_norm = 0.6965 cg_residual = 0.0013 step_size = 0.3499 reward = -0.0000 fps = 2 mse_loss = 0.5952 
2022-07-08 09:24:03.232404 - gail/main.py:174 - [TRPO] iter = 70000 dist_mean = 0.3795 dist_std = 0.9386 vf_loss = 0.9139 grad_norm = 0.4592 nat_grad_norm = 0.5714 cg_residual = 0.0009 step_size = 0.3915 reward = -0.0000 fps = 2 mse_loss = 0.6220 
2022-07-08 09:24:04.092804 - gail/main.py:201 - [Discriminator] iter = 70000 loss = -2.9881 grad_norm = 16.2178 grad_penalty = 0.6705 regularization = 0.0000 true_logits = 0.7608 fake_logits = -2.8978 true_prob = 0.6717 fake_prob = 0.0590 
2022-07-08 09:28:24.230643 - gail/main.py:142 - [Evaluate] iter = 70000 episode={ returns = -176.6438 lengths = 1000 } discounted_episode={ returns = -110.7647 lengths = 1000 } 
2022-07-08 09:28:46.430651 - gail/main.py:174 - [TRPO] iter = 71000 dist_mean = 0.3865 dist_std = 0.9378 vf_loss = 0.5845 grad_norm = 0.4631 nat_grad_norm = 0.6208 cg_residual = 0.0012 step_size = 0.3575 reward = -0.0000 fps = 3 mse_loss = 0.6145 
2022-07-08 09:29:08.878245 - gail/main.py:174 - [TRPO] iter = 72000 dist_mean = 0.3868 dist_std = 0.9385 vf_loss = 0.6043 grad_norm = 0.4905 nat_grad_norm = 0.6473 cg_residual = 0.0020 step_size = 0.3447 reward = 0.0000 fps = 3 mse_loss = 0.6468 
2022-07-08 09:29:30.659222 - gail/main.py:174 - [TRPO] iter = 73000 dist_mean = 0.3829 dist_std = 0.9365 vf_loss = 0.4939 grad_norm = 0.4359 nat_grad_norm = 0.6098 cg_residual = 0.0017 step_size = 0.3909 reward = -0.0000 fps = 3 mse_loss = 0.6366 
2022-07-08 09:29:51.771629 - gail/main.py:174 - [TRPO] iter = 74000 dist_mean = 0.3789 dist_std = 0.9353 vf_loss = 0.7082 grad_norm = 0.4351 nat_grad_norm = 0.6296 cg_residual = 0.0016 step_size = 0.4089 reward = -0.0000 fps = 2 mse_loss = 0.5657 
2022-07-08 09:30:13.370622 - gail/main.py:174 - [TRPO] iter = 75000 dist_mean = 0.3846 dist_std = 0.9375 vf_loss = 0.6037 grad_norm = 0.4322 nat_grad_norm = 0.6509 cg_residual = 0.0019 step_size = 0.3846 reward = -0.0000 fps = 2 mse_loss = 0.5858 
2022-07-08 09:30:14.100641 - gail/main.py:201 - [Discriminator] iter = 75000 loss = -3.6032 grad_norm = 17.1880 grad_penalty = 0.6099 regularization = 0.0000 true_logits = 0.8183 fake_logits = -3.3947 true_prob = 0.6823 fake_prob = 0.0378 
2022-07-08 09:34:33.220390 - gail/main.py:142 - [Evaluate] iter = 75000 episode={ returns = -185.8761 lengths = 1000 } discounted_episode={ returns = -117.1992 lengths = 1000 } 
2022-07-08 09:34:53.970562 - gail/main.py:174 - [TRPO] iter = 76000 dist_mean = 0.3779 dist_std = 0.9334 vf_loss = 0.5113 grad_norm = 0.4649 nat_grad_norm = 0.6612 cg_residual = 0.0017 step_size = 0.3533 reward = 0.0000 fps = 3 mse_loss = 0.5836 
2022-07-08 09:35:14.478807 - gail/main.py:174 - [TRPO] iter = 77000 dist_mean = 0.3878 dist_std = 0.9286 vf_loss = 0.4110 grad_norm = 0.4111 nat_grad_norm = 0.6314 cg_residual = 0.0020 step_size = 0.3883 reward = 0.0000 fps = 3 mse_loss = 0.5876 
2022-07-08 09:35:35.191233 - gail/main.py:174 - [TRPO] iter = 78000 dist_mean = 0.3778 dist_std = 0.9247 vf_loss = 0.4810 grad_norm = 0.4236 nat_grad_norm = 0.5906 cg_residual = 0.0029 step_size = 0.3756 reward = 0.0000 fps = 3 mse_loss = 0.6192 
2022-07-08 09:35:55.835471 - gail/main.py:174 - [TRPO] iter = 79000 dist_mean = 0.3723 dist_std = 0.9271 vf_loss = 0.7643 grad_norm = 0.5017 nat_grad_norm = 0.6332 cg_residual = 0.0027 step_size = 0.3637 reward = -0.0000 fps = 2 mse_loss = 0.6483 
2022-07-08 09:36:16.740506 - gail/main.py:174 - [TRPO] iter = 80000 dist_mean = 0.3839 dist_std = 0.9243 vf_loss = 0.6008 grad_norm = 0.4276 nat_grad_norm = 0.6070 cg_residual = 0.0033 step_size = 0.3845 reward = 0.0000 fps = 2 mse_loss = 0.6161 
2022-07-08 09:36:17.516626 - gail/main.py:201 - [Discriminator] iter = 80000 loss = -4.2159 grad_norm = 13.8376 grad_penalty = 0.6083 regularization = 0.0000 true_logits = 0.9228 fake_logits = -3.9014 true_prob = 0.7005 fake_prob = 0.0241 
2022-07-08 09:40:26.396643 - gail/main.py:142 - [Evaluate] iter = 80000 episode={ returns = -161.7729 lengths = 1000 } discounted_episode={ returns = -100.7222 lengths = 1000 } 
2022-07-08 09:40:47.257321 - gail/main.py:174 - [TRPO] iter = 81000 dist_mean = 0.3788 dist_std = 0.9233 vf_loss = 0.6115 grad_norm = 0.4434 nat_grad_norm = 0.6540 cg_residual = 0.0031 step_size = 0.3887 reward = -0.0000 fps = 3 mse_loss = 0.6493 
2022-07-08 09:41:06.864098 - gail/main.py:174 - [TRPO] iter = 82000 dist_mean = 0.3597 dist_std = 0.9210 vf_loss = 0.5799 grad_norm = 0.5542 nat_grad_norm = 0.6418 cg_residual = 0.0031 step_size = 0.3595 reward = -0.0000 fps = 3 mse_loss = 0.6569 
2022-07-08 09:41:27.734823 - gail/main.py:174 - [TRPO] iter = 83000 dist_mean = 0.3561 dist_std = 0.9197 vf_loss = 0.6995 grad_norm = 0.4016 nat_grad_norm = 0.6445 cg_residual = 0.0043 step_size = 0.4190 reward = 0.0000 fps = 3 mse_loss = 0.6791 
2022-07-08 09:41:47.950733 - gail/main.py:174 - [TRPO] iter = 84000 dist_mean = 0.3511 dist_std = 0.9193 vf_loss = 1.0032 grad_norm = 0.5498 nat_grad_norm = 0.6247 cg_residual = 0.0042 step_size = 0.3622 reward = 0.0000 fps = 3 mse_loss = 0.6241 
2022-07-08 09:42:13.018676 - gail/main.py:174 - [TRPO] iter = 85000 dist_mean = 0.3472 dist_std = 0.9208 vf_loss = 0.7596 grad_norm = 0.4035 nat_grad_norm = 0.6512 cg_residual = 0.0048 step_size = 0.3935 reward = 0.0000 fps = 2 mse_loss = 0.6419 
2022-07-08 09:42:14.226725 - gail/main.py:201 - [Discriminator] iter = 85000 loss = -4.6026 grad_norm = 19.0754 grad_penalty = 0.7637 regularization = 0.0000 true_logits = 0.9111 fake_logits = -4.4551 true_prob = 0.6972 fake_prob = 0.0149 
2022-07-08 09:46:04.299640 - gail/main.py:142 - [Evaluate] iter = 85000 episode={ returns = 549.1790 lengths = 1000 } discounted_episode={ returns = 361.3587 lengths = 1000 } 
2022-07-08 09:46:23.919109 - gail/main.py:174 - [TRPO] iter = 86000 dist_mean = 0.3384 dist_std = 0.9205 vf_loss = 0.6257 grad_norm = 0.5027 nat_grad_norm = 0.6631 cg_residual = 0.0037 step_size = 0.3665 reward = -0.0000 fps = 4 mse_loss = 0.6409 
2022-07-08 09:46:44.260425 - gail/main.py:174 - [TRPO] iter = 87000 dist_mean = 0.3380 dist_std = 0.9170 vf_loss = 0.6395 grad_norm = 0.5314 nat_grad_norm = 0.6157 cg_residual = 0.0027 step_size = 0.3646 reward = -0.0000 fps = 3 mse_loss = 0.6266 
2022-07-08 09:47:03.608418 - gail/main.py:174 - [TRPO] iter = 88000 dist_mean = 0.3361 dist_std = 0.9135 vf_loss = 0.9019 grad_norm = 0.5255 nat_grad_norm = 0.6760 cg_residual = 0.0046 step_size = 0.3589 reward = 0.0000 fps = 3 mse_loss = 0.6333 
2022-07-08 09:47:23.643669 - gail/main.py:174 - [TRPO] iter = 89000 dist_mean = 0.3283 dist_std = 0.9142 vf_loss = 0.9952 grad_norm = 0.4828 nat_grad_norm = 0.6766 cg_residual = 0.0052 step_size = 0.3781 reward = 0.0000 fps = 3 mse_loss = 0.6218 
2022-07-08 09:47:43.861044 - gail/main.py:174 - [TRPO] iter = 90000 dist_mean = 0.3305 dist_std = 0.9095 vf_loss = 0.8401 grad_norm = 0.5486 nat_grad_norm = 0.6426 cg_residual = 0.0083 step_size = 0.3520 reward = -0.0000 fps = 3 mse_loss = 0.6276 
2022-07-08 09:47:44.642529 - gail/main.py:201 - [Discriminator] iter = 90000 loss = -5.3341 grad_norm = 19.7934 grad_penalty = 0.7333 regularization = 0.0000 true_logits = 0.9322 fake_logits = -5.1352 true_prob = 0.6997 fake_prob = 0.0080 
2022-07-08 09:51:35.337089 - gail/main.py:142 - [Evaluate] iter = 90000 episode={ returns = 1242.7882 lengths = 1000 } discounted_episode={ returns = 763.8893 lengths = 1000 } 
2022-07-08 09:51:55.743934 - gail/main.py:174 - [TRPO] iter = 91000 dist_mean = 0.3274 dist_std = 0.9081 vf_loss = 0.6863 grad_norm = 0.5187 nat_grad_norm = 0.6962 cg_residual = 0.0068 step_size = 0.3542 reward = 0.0000 fps = 3 mse_loss = 0.6154 
2022-07-08 09:52:22.363489 - gail/main.py:174 - [TRPO] iter = 92000 dist_mean = 0.3063 dist_std = 0.9057 vf_loss = 1.0505 grad_norm = 0.6092 nat_grad_norm = 0.6842 cg_residual = 0.0049 step_size = 0.3267 reward = 0.0000 fps = 3 mse_loss = 0.5978 
2022-07-08 09:52:42.702183 - gail/main.py:174 - [TRPO] iter = 93000 dist_mean = 0.2963 dist_std = 0.9079 vf_loss = 0.8646 grad_norm = 0.5846 nat_grad_norm = 0.6998 cg_residual = 0.0119 step_size = 0.3264 reward = -0.0000 fps = 3 mse_loss = 0.5793 
2022-07-08 09:53:02.954937 - gail/main.py:174 - [TRPO] iter = 94000 dist_mean = 0.2754 dist_std = 0.9068 vf_loss = 0.7095 grad_norm = 0.5280 nat_grad_norm = 0.6494 cg_residual = 0.0056 step_size = 0.3677 reward = 0.0000 fps = 3 mse_loss = 0.5539 
2022-07-08 09:53:23.652686 - gail/main.py:174 - [TRPO] iter = 95000 dist_mean = 0.2852 dist_std = 0.9095 vf_loss = 0.8536 grad_norm = 0.5046 nat_grad_norm = 0.7314 cg_residual = 0.0094 step_size = 0.3509 reward = 0.0000 fps = 2 mse_loss = 0.5499 
2022-07-08 09:53:24.459631 - gail/main.py:201 - [Discriminator] iter = 95000 loss = -5.6894 grad_norm = 13.5988 grad_penalty = 0.7161 regularization = 0.0000 true_logits = 0.9430 fake_logits = -5.4625 true_prob = 0.6950 fake_prob = 0.0068 
2022-07-08 09:57:10.431114 - gail/main.py:142 - [Evaluate] iter = 95000 episode={ returns = 1190.0326 lengths = 1000 } discounted_episode={ returns = 719.6773 lengths = 1000 } 
2022-07-08 09:57:30.429778 - gail/main.py:174 - [TRPO] iter = 96000 dist_mean = 0.2872 dist_std = 0.9059 vf_loss = 1.2177 grad_norm = 0.5633 nat_grad_norm = 0.6568 cg_residual = 0.0115 step_size = 0.3684 reward = 0.0000 fps = 4 mse_loss = 0.5254 
2022-07-08 09:57:49.311439 - gail/main.py:174 - [TRPO] iter = 97000 dist_mean = 0.2796 dist_std = 0.9056 vf_loss = 1.1929 grad_norm = 0.5471 nat_grad_norm = 0.7681 cg_residual = 0.0091 step_size = 0.3237 reward = -0.0000 fps = 3 mse_loss = 0.4682 
2022-07-08 09:58:08.584522 - gail/main.py:174 - [TRPO] iter = 98000 dist_mean = 0.2916 dist_std = 0.9079 vf_loss = 1.2397 grad_norm = 0.5358 nat_grad_norm = 0.6805 cg_residual = 0.0095 step_size = 0.3635 reward = 0.0000 fps = 3 mse_loss = 0.5085 
2022-07-08 09:58:29.093227 - gail/main.py:174 - [TRPO] iter = 99000 dist_mean = 0.2773 dist_std = 0.9080 vf_loss = 0.9066 grad_norm = 0.6017 nat_grad_norm = 0.6985 cg_residual = 0.0097 step_size = 0.3385 reward = 0.0000 fps = 3 mse_loss = 0.5637 
2022-07-08 09:58:50.012906 - gail/main.py:174 - [TRPO] iter = 100000 dist_mean = 0.2699 dist_std = 0.9017 vf_loss = 1.0470 grad_norm = 0.6228 nat_grad_norm = 0.7461 cg_residual = 0.0189 step_size = 0.3238 reward = 0.0000 fps = 3 mse_loss = 0.5329 
2022-07-08 09:58:50.705636 - gail/main.py:201 - [Discriminator] iter = 100000 loss = -5.7552 grad_norm = 20.1841 grad_penalty = 0.9041 regularization = 0.0000 true_logits = 0.9552 fake_logits = -5.7041 true_prob = 0.6943 fake_prob = 0.0054 
2022-07-08 10:03:00.008819 - gail/main.py:142 - [Evaluate] iter = 100000 episode={ returns = 1464.3388 lengths = 1000 } discounted_episode={ returns = 859.5560 lengths = 1000 } 
2022-07-08 10:03:22.627996 - gail/main.py:174 - [TRPO] iter = 101000 dist_mean = 0.2504 dist_std = 0.9014 vf_loss = 2.1080 grad_norm = 0.5838 nat_grad_norm = 0.8216 cg_residual = 0.0298 step_size = 0.3018 reward = -0.0000 fps = 3 mse_loss = 0.5479 
2022-07-08 10:03:46.795266 - gail/main.py:174 - [TRPO] iter = 102000 dist_mean = 0.2960 dist_std = 0.9014 vf_loss = 1.1693 grad_norm = 0.6052 nat_grad_norm = 0.7157 cg_residual = 0.0140 step_size = 0.3364 reward = 0.0000 fps = 3 mse_loss = 0.5184 
2022-07-08 10:04:10.863420 - gail/main.py:174 - [TRPO] iter = 103000 dist_mean = 0.2930 dist_std = 0.9004 vf_loss = 1.0093 grad_norm = 0.5761 nat_grad_norm = 0.7613 cg_residual = 0.0127 step_size = 0.3226 reward = 0.0000 fps = 3 mse_loss = 0.5703 
2022-07-08 10:04:33.448820 - gail/main.py:174 - [TRPO] iter = 104000 dist_mean = 0.2775 dist_std = 0.8999 vf_loss = 1.0255 grad_norm = 0.6131 nat_grad_norm = 0.7082 cg_residual = 0.0179 step_size = 0.3358 reward = 0.0000 fps = 2 mse_loss = 0.5259 
2022-07-08 10:04:56.322907 - gail/main.py:174 - [TRPO] iter = 105000 dist_mean = 0.2705 dist_std = 0.8998 vf_loss = 1.1781 grad_norm = 0.5566 nat_grad_norm = 0.6862 cg_residual = 0.0163 step_size = 0.3365 reward = 0.0000 fps = 2 mse_loss = 0.5104 
2022-07-08 10:04:57.114572 - gail/main.py:201 - [Discriminator] iter = 105000 loss = -6.2282 grad_norm = 14.5704 grad_penalty = 0.8059 regularization = 0.0000 true_logits = 0.8987 fake_logits = -6.1355 true_prob = 0.6806 fake_prob = 0.0038 
2022-07-08 10:09:21.952933 - gail/main.py:142 - [Evaluate] iter = 105000 episode={ returns = 1406.3254 lengths = 1000 } discounted_episode={ returns = 871.4514 lengths = 1000 } 
2022-07-08 10:09:45.406855 - gail/main.py:174 - [TRPO] iter = 106000 dist_mean = 0.2741 dist_std = 0.9035 vf_loss = 1.2652 grad_norm = 0.5971 nat_grad_norm = 0.7023 cg_residual = 0.0146 step_size = 0.3403 reward = 0.0000 fps = 3 mse_loss = 0.5166 
2022-07-08 10:10:08.020194 - gail/main.py:174 - [TRPO] iter = 107000 dist_mean = 0.2593 dist_std = 0.9023 vf_loss = 1.2223 grad_norm = 0.7438 nat_grad_norm = 0.7335 cg_residual = 0.0196 step_size = 0.3111 reward = 0.0000 fps = 3 mse_loss = 0.5069 
2022-07-08 10:10:30.462485 - gail/main.py:174 - [TRPO] iter = 108000 dist_mean = 0.2648 dist_std = 0.9043 vf_loss = 1.1034 grad_norm = 0.6312 nat_grad_norm = 0.7027 cg_residual = 0.0206 step_size = 0.3221 reward = -0.0000 fps = 3 mse_loss = 0.4792 
2022-07-08 10:10:52.817164 - gail/main.py:174 - [TRPO] iter = 109000 dist_mean = 0.2676 dist_std = 0.9021 vf_loss = 0.8900 grad_norm = 0.5495 nat_grad_norm = 0.6934 cg_residual = 0.0149 step_size = 0.3600 reward = 0.0000 fps = 2 mse_loss = 0.4787 
2022-07-08 10:11:15.475076 - gail/main.py:174 - [TRPO] iter = 110000 dist_mean = 0.2664 dist_std = 0.8992 vf_loss = 1.9477 grad_norm = 0.5889 nat_grad_norm = 0.8690 cg_residual = 0.0196 step_size = 0.3037 reward = 0.0000 fps = 2 mse_loss = 0.4571 
2022-07-08 10:11:16.398052 - gail/main.py:201 - [Discriminator] iter = 110000 loss = -6.5716 grad_norm = 21.0604 grad_penalty = 0.8842 regularization = 0.0000 true_logits = 0.8565 fake_logits = -6.5993 true_prob = 0.6732 fake_prob = 0.0032 
2022-07-08 10:15:45.238428 - gail/main.py:142 - [Evaluate] iter = 110000 episode={ returns = 1866.8496 lengths = 1000 } discounted_episode={ returns = 1192.1367 lengths = 1000 } 
2022-07-08 10:16:08.136191 - gail/main.py:174 - [TRPO] iter = 111000 dist_mean = 0.2590 dist_std = 0.8992 vf_loss = 1.0667 grad_norm = 0.5547 nat_grad_norm = 0.6601 cg_residual = 0.0146 step_size = 0.3510 reward = -0.0000 fps = 3 mse_loss = 0.5023 
2022-07-08 10:16:30.026224 - gail/main.py:174 - [TRPO] iter = 112000 dist_mean = 0.2697 dist_std = 0.8953 vf_loss = 0.9232 grad_norm = 0.5628 nat_grad_norm = 0.6735 cg_residual = 0.0181 step_size = 0.3340 reward = 0.0000 fps = 3 mse_loss = 0.4961 
2022-07-08 10:16:53.108043 - gail/main.py:174 - [TRPO] iter = 113000 dist_mean = 0.2684 dist_std = 0.8966 vf_loss = 1.0105 grad_norm = 0.6167 nat_grad_norm = 0.7001 cg_residual = 0.0172 step_size = 0.3325 reward = 0.0000 fps = 2 mse_loss = 0.4737 
2022-07-08 10:17:16.227460 - gail/main.py:174 - [TRPO] iter = 114000 dist_mean = 0.2661 dist_std = 0.8938 vf_loss = 1.1810 grad_norm = 0.5243 nat_grad_norm = 0.7319 cg_residual = 0.0199 step_size = 0.3394 reward = 0.0000 fps = 2 mse_loss = 0.4563 
2022-07-08 10:17:38.500247 - gail/main.py:174 - [TRPO] iter = 115000 dist_mean = 0.2658 dist_std = 0.8910 vf_loss = 1.1102 grad_norm = 0.6986 nat_grad_norm = 0.7314 cg_residual = 0.0196 step_size = 0.3144 reward = -0.0000 fps = 2 mse_loss = 0.4644 
2022-07-08 10:17:39.283954 - gail/main.py:201 - [Discriminator] iter = 115000 loss = -6.7261 grad_norm = 18.8294 grad_penalty = 0.9445 regularization = 0.0000 true_logits = 0.8775 fake_logits = -6.7932 true_prob = 0.6764 fake_prob = 0.0031 
2022-07-08 10:21:56.763394 - gail/main.py:142 - [Evaluate] iter = 115000 episode={ returns = 1900.2756 lengths = 1000 } discounted_episode={ returns = 1162.0952 lengths = 1000 } 
2022-07-08 10:22:18.805126 - gail/main.py:174 - [TRPO] iter = 116000 dist_mean = 0.2597 dist_std = 0.8907 vf_loss = 0.9093 grad_norm = 0.6468 nat_grad_norm = 0.6826 cg_residual = 0.0214 step_size = 0.3172 reward = -0.0000 fps = 3 mse_loss = 0.4801 
2022-07-08 10:22:40.588380 - gail/main.py:174 - [TRPO] iter = 117000 dist_mean = 0.3551 dist_std = 0.8874 vf_loss = 0.5606 grad_norm = 0.5884 nat_grad_norm = 0.6610 cg_residual = 0.0328 step_size = 0.3392 reward = -0.0000 fps = 3 mse_loss = 0.4928 
2022-07-08 10:23:03.402564 - gail/main.py:174 - [TRPO] iter = 118000 dist_mean = 0.2482 dist_std = 0.8907 vf_loss = 1.3522 grad_norm = 0.6168 nat_grad_norm = 0.7225 cg_residual = 0.0381 step_size = 0.3205 reward = 0.0000 fps = 3 mse_loss = 0.4857 
2022-07-08 10:23:25.236414 - gail/main.py:174 - [TRPO] iter = 119000 dist_mean = 0.2573 dist_std = 0.8906 vf_loss = 1.5639 grad_norm = 0.6277 nat_grad_norm = 0.7225 cg_residual = 0.0258 step_size = 0.3286 reward = 0.0000 fps = 2 mse_loss = 0.5005 
2022-07-08 10:23:47.268967 - gail/main.py:174 - [TRPO] iter = 120000 dist_mean = 0.2489 dist_std = 0.8904 vf_loss = 1.4945 grad_norm = 0.6783 nat_grad_norm = 0.7495 cg_residual = 0.0285 step_size = 0.3034 reward = -0.0000 fps = 2 mse_loss = 0.4653 
2022-07-08 10:23:48.130864 - gail/main.py:201 - [Discriminator] iter = 120000 loss = -6.4632 grad_norm = 15.4522 grad_penalty = 0.9132 regularization = 0.0000 true_logits = 0.8508 fake_logits = -6.5257 true_prob = 0.6734 fake_prob = 0.0034 
2022-07-08 10:28:16.159575 - gail/main.py:142 - [Evaluate] iter = 120000 episode={ returns = 1958.9336 lengths = 1000 } discounted_episode={ returns = 1189.3724 lengths = 1000 } 
2022-07-08 10:28:40.511903 - gail/main.py:174 - [TRPO] iter = 121000 dist_mean = 0.2540 dist_std = 0.8907 vf_loss = 1.1310 grad_norm = 0.8246 nat_grad_norm = 0.7161 cg_residual = 0.0324 step_size = 0.3131 reward = 0.0000 fps = 3 mse_loss = 0.4668 
2022-07-08 10:29:04.107229 - gail/main.py:174 - [TRPO] iter = 122000 dist_mean = 0.3673 dist_std = 0.8901 vf_loss = 0.3796 grad_norm = 0.5297 nat_grad_norm = 0.7562 cg_residual = 0.0487 step_size = 0.2884 reward = -0.0000 fps = 3 mse_loss = 0.4503 
2022-07-08 10:29:27.931104 - gail/main.py:174 - [TRPO] iter = 123000 dist_mean = 0.2919 dist_std = 0.8910 vf_loss = 0.9962 grad_norm = 0.6593 nat_grad_norm = 0.7300 cg_residual = 0.0349 step_size = 0.3174 reward = 0.0000 fps = 2 mse_loss = 0.4425 
2022-07-08 10:29:52.459248 - gail/main.py:174 - [TRPO] iter = 124000 dist_mean = 0.2874 dist_std = 0.8899 vf_loss = 1.1478 grad_norm = 0.5632 nat_grad_norm = 0.7539 cg_residual = 0.0288 step_size = 0.3231 reward = -0.0000 fps = 2 mse_loss = 0.4312 
2022-07-08 10:30:16.212533 - gail/main.py:174 - [TRPO] iter = 125000 dist_mean = 0.2687 dist_std = 0.8900 vf_loss = 1.3455 grad_norm = 0.7384 nat_grad_norm = 0.6528 cg_residual = 0.0255 step_size = 0.3342 reward = 0.0000 fps = 2 mse_loss = 0.4487 
2022-07-08 10:30:17.047190 - gail/main.py:201 - [Discriminator] iter = 125000 loss = -6.7838 grad_norm = 12.9860 grad_penalty = 0.8988 regularization = 0.0000 true_logits = 0.9576 fake_logits = -6.7251 true_prob = 0.6855 fake_prob = 0.0027 
2022-07-08 10:32:57.908650 - gail/main.py:142 - [Evaluate] iter = 125000 episode={ returns = 2481.5280 lengths = 1000 } discounted_episode={ returns = 1465.8695 lengths = 1000 } 
2022-07-08 10:33:05.355415 - gail/main.py:174 - [TRPO] iter = 126000 dist_mean = 0.2712 dist_std = 0.8881 vf_loss = 1.7744 grad_norm = 0.7490 nat_grad_norm = 0.7114 cg_residual = 0.0408 step_size = 0.3166 reward = -0.0000 fps = 5 mse_loss = 0.4281 
2022-07-08 10:33:13.308776 - gail/main.py:174 - [TRPO] iter = 127000 dist_mean = 0.3561 dist_std = 0.8868 vf_loss = 0.3957 grad_norm = 0.5897 nat_grad_norm = 0.6711 cg_residual = 0.0611 step_size = 0.3049 reward = -0.0000 fps = 5 mse_loss = 0.4098 
2022-07-08 10:33:20.878627 - gail/main.py:174 - [TRPO] iter = 128000 dist_mean = 0.3096 dist_std = 0.8861 vf_loss = 1.7866 grad_norm = 0.8436 nat_grad_norm = 0.6457 cg_residual = 0.0308 step_size = 0.3050 reward = -0.0000 fps = 5 mse_loss = 0.4292 
2022-07-08 10:33:29.089262 - gail/main.py:174 - [TRPO] iter = 129000 dist_mean = 0.3185 dist_std = 0.8844 vf_loss = 0.9069 grad_norm = 0.5930 nat_grad_norm = 0.7327 cg_residual = 0.0328 step_size = 0.3244 reward = 0.0000 fps = 5 mse_loss = 0.4048 
2022-07-08 10:33:36.745350 - gail/main.py:174 - [TRPO] iter = 130000 dist_mean = 0.3108 dist_std = 0.8855 vf_loss = 1.4462 grad_norm = 0.6564 nat_grad_norm = 0.7321 cg_residual = 0.0353 step_size = 0.2943 reward = 0.0000 fps = 5 mse_loss = 0.3977 
2022-07-08 10:33:37.039315 - gail/main.py:201 - [Discriminator] iter = 130000 loss = -6.9275 grad_norm = 16.1536 grad_penalty = 0.9987 regularization = 0.0000 true_logits = 0.8537 fake_logits = -7.0725 true_prob = 0.6683 fake_prob = 0.0022 
