2022-07-08 09:04:48.802450 - utils/flags.py:257 - log_dir = logs/gail_w-HalfCheetah-v2-300-2022-07-08-09-04-48
2022-07-08 09:05:07.527159 - gail/utils/replay_buffer.py:86 - Load dataset from /content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/HalfCheetah-v2
2022-07-08 09:05:27.672305 - gail/main.py:80 - Expert Reward 6594.485577
2022-07-08 09:05:28.612068 - gail/main.py:84 - Original dataset size 3000
2022-07-08 09:05:28.709420 - gail/main.py:86 - Subsampled dataset size 3000
2022-07-08 09:05:28.838236 - gail/main.py:87 - np random: 864 random : 786
2022-07-08 09:05:28.839174 - gail/main.py:91 - Sampled obs: 0.6422, acs: 0.2570
2022-07-08 09:05:31.479068 - lunzi/nn/flat_param.py:17 - Enabling flattening... ['GaussianMLPPolicy_1/log_std:0', 'GaussianMLPPolicy_1/Variable:0', 'GaussianMLPPolicy_1/Variable_1:0', 'GaussianMLPPolicy_1/Variable_2:0', 'GaussianMLPPolicy_1/Variable_3:0', 'GaussianMLPPolicy_1/Variable_4:0', 'GaussianMLPPolicy_1/Variable_5:0']
2022-07-08 09:05:54.611717 - gail/discriminator/discriminator.py:30 - Use predefined normalization.
2022-07-08 09:05:54.634342 - gail/discriminator/discriminator.py:33 - Normalizer loc:[[-1.0330443e-01 -9.3026832e-03  2.9791275e-01  8.1040427e-02
   1.7489190e-01  4.0859219e-01 -3.0232381e-02 -9.5055901e-02
   7.1118622e+00  5.5424552e-03 -1.8441556e-01  3.1117105e-01
  -2.7393317e-01  2.6658252e-01  4.0665963e-01 -2.8198040e-01
   2.5822452e-01]] 
 scale:[[ 0.03102476  0.07177044  0.37663096  0.5276757   0.46577376  0.12057849
   0.35446092  0.3551628   1.290817    0.4731574   1.4939526  12.314076
  14.637607   14.150712    3.6817648  11.124868    9.962843  ]]
2022-07-08 09:06:04.775238 - gail/discriminator/discriminator.py:67 - Discriminator uses Wasserstein distance.
2022-07-08 09:06:04.784026 - gail/discriminator/discriminator.py:68 - [<tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable:0' shape=(23, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_1:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_2:0' shape=(100, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_3:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_4:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_5:0' shape=(1,) dtype=float32_ref>]
2022-07-08 09:06:04.796595 - gail/discriminator/discriminator.py:69 - Use gradient penalty regularization (coef = 10.000000)
2022-07-08 09:06:07.357867 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions_mean]
2022-07-08 09:08:43.934467 - gail/main.py:142 - [Evaluate] iter = 0 episode={ returns = 0.0497 lengths = 1000 } discounted_episode={ returns = -0.2886 lengths = 1000 } 
2022-07-08 09:08:43.947780 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions]
2022-07-08 09:09:03.915386 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [fake_states fake_actions] => [scaled_neural_reward]
2022-07-08 09:09:04.881160 - lunzi/nn/module.py:71 - [MLPVFunction] is making TensorFlow callables, key = [states] => [values]
2022-07-08 09:09:06.579175 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [] => [sync_old]
2022-07-08 09:09:07.428114 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss flat_grad dist_std mean_kl dist_mean]
2022-07-08 09:09:12.833040 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states tangents actions] => [hessian_vec_prod]
2022-07-08 09:09:22.455131 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [] => [get_flat]
2022-07-08 09:09:23.366821 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [feed_flat] => [set_flat]
2022-07-08 09:09:24.259172 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss mean_kl]
2022-07-08 09:09:25.873128 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [train_vf vf_loss]
2022-07-08 09:09:28.653262 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [vf_loss]
2022-07-08 09:09:29.585900 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states actions_] => [mse_loss]
2022-07-08 09:09:30.436567 - gail/main.py:174 - [TRPO] iter = 1000 dist_mean = -0.0000 dist_std = 1.0000 vf_loss = 0.9310 grad_norm = 0.3044 nat_grad_norm = 0.5189 cg_residual = 0.0000 step_size = 0.4012 reward = -0.0000 fps = 4 mse_loss = 0.9230 
2022-07-08 09:09:47.923483 - gail/main.py:174 - [TRPO] iter = 2000 dist_mean = 0.0076 dist_std = 0.9946 vf_loss = 0.8663 grad_norm = 0.3123 nat_grad_norm = 0.5171 cg_residual = 0.0000 step_size = 0.4079 reward = -0.0000 fps = 4 mse_loss = 0.9096 
2022-07-08 09:10:04.882548 - gail/main.py:174 - [TRPO] iter = 3000 dist_mean = 0.0113 dist_std = 0.9992 vf_loss = 1.8100 grad_norm = 0.2948 nat_grad_norm = 0.5528 cg_residual = 0.0000 step_size = 0.3915 reward = 0.0000 fps = 4 mse_loss = 0.9029 
2022-07-08 09:10:25.211816 - gail/main.py:174 - [TRPO] iter = 4000 dist_mean = 0.0131 dist_std = 1.0011 vf_loss = 2.5428 grad_norm = 0.3180 nat_grad_norm = 0.5054 cg_residual = 0.0000 step_size = 0.4217 reward = 0.0000 fps = 3 mse_loss = 0.9105 
2022-07-08 09:10:46.139965 - gail/main.py:174 - [TRPO] iter = 5000 dist_mean = 0.0048 dist_std = 1.0023 vf_loss = 1.0869 grad_norm = 0.2934 nat_grad_norm = 0.6174 cg_residual = 0.0001 step_size = 0.3895 reward = 0.0000 fps = 3 mse_loss = 0.8791 
2022-07-08 09:10:46.151726 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [true_states true_actions fake_states fake_actions true_masks] => [train loss true_logits fake_logits true_prob fake_prob grad_norm grad_penalty regularization]
2022-07-08 09:10:54.489233 - gail/main.py:201 - [Discriminator] iter = 5000 loss = 1.9551 grad_norm = 80.9477 grad_penalty = 1.7928 regularization = 0.0000 true_logits = -0.0399 fake_logits = 0.1225 true_prob = 0.4902 fake_prob = 0.5273 
2022-07-08 09:15:35.051281 - gail/main.py:142 - [Evaluate] iter = 5000 episode={ returns = -1.7388 lengths = 1000 } discounted_episode={ returns = -0.7453 lengths = 1000 } 
2022-07-08 09:15:58.814600 - gail/main.py:174 - [TRPO] iter = 6000 dist_mean = 0.0025 dist_std = 1.0057 vf_loss = 0.5450 grad_norm = 0.3096 nat_grad_norm = 0.5398 cg_residual = 0.0000 step_size = 0.3837 reward = 0.0000 fps = 3 mse_loss = 0.9021 
2022-07-08 09:16:21.878249 - gail/main.py:174 - [TRPO] iter = 7000 dist_mean = -0.0038 dist_std = 1.0050 vf_loss = 0.1864 grad_norm = 0.2801 nat_grad_norm = 0.4563 cg_residual = 0.0000 step_size = 0.4640 reward = -0.0000 fps = 3 mse_loss = 0.8705 
2022-07-08 09:16:45.042563 - gail/main.py:174 - [TRPO] iter = 8000 dist_mean = 0.0139 dist_std = 1.0151 vf_loss = 0.3025 grad_norm = 0.3201 nat_grad_norm = 0.5450 cg_residual = 0.0000 step_size = 0.3828 reward = -0.0000 fps = 2 mse_loss = 0.8830 
2022-07-08 09:17:08.360293 - gail/main.py:174 - [TRPO] iter = 9000 dist_mean = 0.0264 dist_std = 1.0125 vf_loss = 0.2016 grad_norm = 0.3185 nat_grad_norm = 0.5782 cg_residual = 0.0000 step_size = 0.4086 reward = 0.0000 fps = 2 mse_loss = 0.9440 
2022-07-08 09:17:31.962889 - gail/main.py:174 - [TRPO] iter = 10000 dist_mean = 0.0239 dist_std = 1.0178 vf_loss = 0.5258 grad_norm = 0.2946 nat_grad_norm = 0.5352 cg_residual = 0.0000 step_size = 0.4058 reward = -0.0000 fps = 2 mse_loss = 0.9555 
2022-07-08 09:17:32.774038 - gail/main.py:201 - [Discriminator] iter = 10000 loss = 0.0920 grad_norm = 47.7800 grad_penalty = 1.9564 regularization = 0.0000 true_logits = 0.0672 fake_logits = -1.7972 true_prob = 0.5167 fake_prob = 0.1803 
2022-07-08 09:22:11.673183 - gail/main.py:142 - [Evaluate] iter = 10000 episode={ returns = -14.3423 lengths = 1000 } discounted_episode={ returns = -8.9294 lengths = 1000 } 
2022-07-08 09:22:33.704118 - gail/main.py:174 - [TRPO] iter = 11000 dist_mean = 0.0249 dist_std = 1.0209 vf_loss = 1.9844 grad_norm = 0.3148 nat_grad_norm = 0.5348 cg_residual = 0.0000 step_size = 0.3959 reward = -0.0000 fps = 3 mse_loss = 0.9599 
2022-07-08 09:22:56.585501 - gail/main.py:174 - [TRPO] iter = 12000 dist_mean = 0.0304 dist_std = 1.0232 vf_loss = 2.2726 grad_norm = 0.3083 nat_grad_norm = 0.6730 cg_residual = 0.0000 step_size = 0.3563 reward = 0.0000 fps = 3 mse_loss = 1.0812 
2022-07-08 09:23:19.681218 - gail/main.py:174 - [TRPO] iter = 13000 dist_mean = 0.0014 dist_std = 1.0236 vf_loss = 5.7882 grad_norm = 0.2803 nat_grad_norm = 0.4871 cg_residual = 0.0000 step_size = 0.4550 reward = 0.0000 fps = 2 mse_loss = 0.9914 
2022-07-08 09:23:42.680635 - gail/main.py:174 - [TRPO] iter = 14000 dist_mean = -0.0133 dist_std = 1.0200 vf_loss = 6.8675 grad_norm = 0.3228 nat_grad_norm = 0.5562 cg_residual = 0.0000 step_size = 0.4018 reward = 0.0000 fps = 2 mse_loss = 1.0269 
2022-07-08 09:24:04.753203 - gail/main.py:174 - [TRPO] iter = 15000 dist_mean = 0.0387 dist_std = 1.0142 vf_loss = 4.7836 grad_norm = 0.3106 nat_grad_norm = 0.5628 cg_residual = 0.0000 step_size = 0.3987 reward = 0.0000 fps = 2 mse_loss = 0.9920 
2022-07-08 09:24:05.535251 - gail/main.py:201 - [Discriminator] iter = 15000 loss = -1.2221 grad_norm = 24.5887 grad_penalty = 1.6917 regularization = 0.0000 true_logits = 0.1376 fake_logits = -2.7762 true_prob = 0.5340 fake_prob = 0.1200 
2022-07-08 09:28:21.948340 - gail/main.py:142 - [Evaluate] iter = 15000 episode={ returns = -16.1296 lengths = 1000 } discounted_episode={ returns = -10.2912 lengths = 1000 } 
2022-07-08 09:28:44.484382 - gail/main.py:174 - [TRPO] iter = 16000 dist_mean = 0.0583 dist_std = 1.0140 vf_loss = 2.2366 grad_norm = 0.3028 nat_grad_norm = 0.6005 cg_residual = 0.0000 step_size = 0.3810 reward = 0.0000 fps = 3 mse_loss = 0.9942 
2022-07-08 09:29:06.670226 - gail/main.py:174 - [TRPO] iter = 17000 dist_mean = 0.0640 dist_std = 1.0195 vf_loss = 0.9651 grad_norm = 0.3105 nat_grad_norm = 0.7245 cg_residual = 0.0001 step_size = 0.3580 reward = 0.0000 fps = 3 mse_loss = 1.0556 
2022-07-08 09:29:27.774788 - gail/main.py:174 - [TRPO] iter = 18000 dist_mean = 0.0288 dist_std = 1.0215 vf_loss = 12.0084 grad_norm = 0.3581 nat_grad_norm = 0.5606 cg_residual = 0.0001 step_size = 0.3846 reward = 0.0000 fps = 3 mse_loss = 1.0047 
2022-07-08 09:29:48.381143 - gail/main.py:174 - [TRPO] iter = 19000 dist_mean = -0.0137 dist_std = 1.0154 vf_loss = 1.5065 grad_norm = 0.3489 nat_grad_norm = 0.5931 cg_residual = 0.0003 step_size = 0.3623 reward = -0.0000 fps = 2 mse_loss = 1.0726 
2022-07-08 09:30:09.543116 - gail/main.py:174 - [TRPO] iter = 20000 dist_mean = 0.0375 dist_std = 1.0201 vf_loss = 1.9757 grad_norm = 0.3471 nat_grad_norm = 0.5715 cg_residual = 0.0000 step_size = 0.3821 reward = -0.0000 fps = 2 mse_loss = 0.9998 
2022-07-08 09:30:10.368984 - gail/main.py:201 - [Discriminator] iter = 20000 loss = 1.1496 grad_norm = 27.3559 grad_penalty = 2.1111 regularization = 0.0000 true_logits = 0.2078 fake_logits = -0.7537 true_prob = 0.5508 fake_prob = 0.3235 
2022-07-08 09:34:29.909128 - gail/main.py:142 - [Evaluate] iter = 20000 episode={ returns = -32.8444 lengths = 1000 } discounted_episode={ returns = -20.5234 lengths = 1000 } 
2022-07-08 09:34:50.422264 - gail/main.py:174 - [TRPO] iter = 21000 dist_mean = 0.0249 dist_std = 1.0216 vf_loss = 2.8469 grad_norm = 0.3170 nat_grad_norm = 0.5373 cg_residual = 0.0000 step_size = 0.4206 reward = -0.0000 fps = 3 mse_loss = 1.0349 
2022-07-08 09:35:11.314841 - gail/main.py:174 - [TRPO] iter = 22000 dist_mean = 0.0568 dist_std = 1.0162 vf_loss = 2.1972 grad_norm = 0.3541 nat_grad_norm = 0.6090 cg_residual = 0.0000 step_size = 0.3527 reward = 0.0000 fps = 3 mse_loss = 1.1193 
2022-07-08 09:35:31.907696 - gail/main.py:174 - [TRPO] iter = 23000 dist_mean = 0.0331 dist_std = 1.0218 vf_loss = 3.5304 grad_norm = 0.3232 nat_grad_norm = 0.5602 cg_residual = 0.0000 step_size = 0.3985 reward = -0.0000 fps = 3 mse_loss = 1.1045 
2022-07-08 09:35:52.782977 - gail/main.py:174 - [TRPO] iter = 24000 dist_mean = 0.0264 dist_std = 1.0242 vf_loss = 3.0527 grad_norm = 0.2965 nat_grad_norm = 0.4703 cg_residual = 0.0000 step_size = 0.4851 reward = 0.0000 fps = 2 mse_loss = 1.1268 
2022-07-08 09:36:13.550549 - gail/main.py:174 - [TRPO] iter = 25000 dist_mean = 0.0403 dist_std = 1.0321 vf_loss = 2.2743 grad_norm = 0.3114 nat_grad_norm = 0.5511 cg_residual = 0.0000 step_size = 0.4185 reward = -0.0000 fps = 2 mse_loss = 1.1435 
2022-07-08 09:36:14.476302 - gail/main.py:201 - [Discriminator] iter = 25000 loss = 0.2046 grad_norm = 20.0469 grad_penalty = 1.4515 regularization = 0.0000 true_logits = 0.2889 fake_logits = -0.9580 true_prob = 0.5697 fake_prob = 0.2811 
2022-07-08 09:40:21.565330 - gail/main.py:142 - [Evaluate] iter = 25000 episode={ returns = -29.1826 lengths = 1000 } discounted_episode={ returns = -18.5164 lengths = 1000 } 
2022-07-08 09:40:41.966237 - gail/main.py:174 - [TRPO] iter = 26000 dist_mean = 0.0517 dist_std = 1.0287 vf_loss = 4.5853 grad_norm = 0.3048 nat_grad_norm = 0.4883 cg_residual = 0.0000 step_size = 0.4531 reward = -0.0000 fps = 3 mse_loss = 1.1608 
2022-07-08 09:41:01.868734 - gail/main.py:174 - [TRPO] iter = 27000 dist_mean = 0.0533 dist_std = 1.0212 vf_loss = 2.8777 grad_norm = 0.3353 nat_grad_norm = 0.5534 cg_residual = 0.0000 step_size = 0.4101 reward = -0.0000 fps = 3 mse_loss = 1.0613 
2022-07-08 09:41:21.795641 - gail/main.py:174 - [TRPO] iter = 28000 dist_mean = 0.0523 dist_std = 1.0167 vf_loss = 0.5546 grad_norm = 0.3213 nat_grad_norm = 0.5261 cg_residual = 0.0000 step_size = 0.4329 reward = -0.0000 fps = 3 mse_loss = 1.0754 
2022-07-08 09:41:42.925513 - gail/main.py:174 - [TRPO] iter = 29000 dist_mean = 0.1236 dist_std = 1.0155 vf_loss = 1.2730 grad_norm = 0.3684 nat_grad_norm = 0.5696 cg_residual = 0.0001 step_size = 0.3779 reward = -0.0000 fps = 3 mse_loss = 1.0890 
2022-07-08 09:42:04.746283 - gail/main.py:174 - [TRPO] iter = 30000 dist_mean = 0.0554 dist_std = 1.0168 vf_loss = 1.0473 grad_norm = 0.3237 nat_grad_norm = 0.5172 cg_residual = 0.0000 step_size = 0.4342 reward = -0.0000 fps = 2 mse_loss = 1.0433 
2022-07-08 09:42:06.044540 - gail/main.py:201 - [Discriminator] iter = 30000 loss = -0.3026 grad_norm = 17.2650 grad_penalty = 1.2467 regularization = 0.0000 true_logits = 0.3698 fake_logits = -1.1796 true_prob = 0.5882 fake_prob = 0.2401 
2022-07-08 09:46:05.438731 - gail/main.py:142 - [Evaluate] iter = 30000 episode={ returns = -31.2151 lengths = 1000 } discounted_episode={ returns = -19.5088 lengths = 1000 } 
2022-07-08 09:46:25.017014 - gail/main.py:174 - [TRPO] iter = 31000 dist_mean = 0.0698 dist_std = 1.0104 vf_loss = 1.4108 grad_norm = 0.3304 nat_grad_norm = 0.5987 cg_residual = 0.0001 step_size = 0.4108 reward = 0.0000 fps = 3 mse_loss = 1.0862 
2022-07-08 09:46:45.482544 - gail/main.py:174 - [TRPO] iter = 32000 dist_mean = -0.0901 dist_std = 1.0178 vf_loss = 1.2166 grad_norm = 0.4585 nat_grad_norm = 0.8512 cg_residual = 0.0024 step_size = 0.3035 reward = 0.0000 fps = 3 mse_loss = 1.0479 
2022-07-08 09:47:05.267436 - gail/main.py:174 - [TRPO] iter = 33000 dist_mean = 0.1103 dist_std = 1.0217 vf_loss = 0.9871 grad_norm = 0.3649 nat_grad_norm = 0.6060 cg_residual = 0.0001 step_size = 0.3859 reward = 0.0000 fps = 3 mse_loss = 1.1601 
2022-07-08 09:47:25.387325 - gail/main.py:174 - [TRPO] iter = 34000 dist_mean = 0.1045 dist_std = 1.0166 vf_loss = 0.8172 grad_norm = 0.4214 nat_grad_norm = 0.5571 cg_residual = 0.0001 step_size = 0.3900 reward = 0.0000 fps = 3 mse_loss = 1.1570 
2022-07-08 09:47:45.258101 - gail/main.py:174 - [TRPO] iter = 35000 dist_mean = 0.1042 dist_std = 1.0133 vf_loss = 0.9728 grad_norm = 0.3330 nat_grad_norm = 0.5311 cg_residual = 0.0001 step_size = 0.4299 reward = -0.0000 fps = 2 mse_loss = 1.1221 
2022-07-08 09:47:46.032262 - gail/main.py:201 - [Discriminator] iter = 35000 loss = -0.5935 grad_norm = 20.3795 grad_penalty = 1.2482 regularization = 0.0000 true_logits = 0.4288 fake_logits = -1.4129 true_prob = 0.6011 fake_prob = 0.2009 
2022-07-08 09:51:46.843850 - gail/main.py:142 - [Evaluate] iter = 35000 episode={ returns = -33.4928 lengths = 1000 } discounted_episode={ returns = -21.0073 lengths = 1000 } 
2022-07-08 09:52:14.627097 - gail/main.py:174 - [TRPO] iter = 36000 dist_mean = 0.1275 dist_std = 1.0151 vf_loss = 2.7468 grad_norm = 0.3716 nat_grad_norm = 0.6467 cg_residual = 0.0001 step_size = 0.3593 reward = -0.0000 fps = 3 mse_loss = 1.1033 
2022-07-08 09:52:34.362687 - gail/main.py:174 - [TRPO] iter = 37000 dist_mean = 0.1173 dist_std = 1.0129 vf_loss = 0.6896 grad_norm = 0.3904 nat_grad_norm = 0.5693 cg_residual = 0.0001 step_size = 0.3958 reward = 0.0000 fps = 3 mse_loss = 1.1564 
2022-07-08 09:52:54.118217 - gail/main.py:174 - [TRPO] iter = 38000 dist_mean = 0.1204 dist_std = 1.0174 vf_loss = 0.4355 grad_norm = 0.5082 nat_grad_norm = 0.5706 cg_residual = 0.0001 step_size = 0.3470 reward = 0.0000 fps = 3 mse_loss = 1.1007 
2022-07-08 09:53:14.053549 - gail/main.py:174 - [TRPO] iter = 39000 dist_mean = 0.1397 dist_std = 1.0204 vf_loss = 0.4001 grad_norm = 0.4612 nat_grad_norm = 0.5855 cg_residual = 0.0001 step_size = 0.3586 reward = 0.0000 fps = 3 mse_loss = 0.9920 
2022-07-08 09:53:35.749956 - gail/main.py:174 - [TRPO] iter = 40000 dist_mean = 0.1456 dist_std = 1.0227 vf_loss = 0.5515 grad_norm = 0.4527 nat_grad_norm = 0.5435 cg_residual = 0.0001 step_size = 0.3910 reward = -0.0000 fps = 2 mse_loss = 1.0281 
2022-07-08 09:53:36.502246 - gail/main.py:201 - [Discriminator] iter = 40000 loss = -1.1275 grad_norm = 15.2255 grad_penalty = 0.9479 regularization = 0.0000 true_logits = 0.5212 fake_logits = -1.5542 true_prob = 0.6210 fake_prob = 0.1813 
2022-07-08 09:57:29.166676 - gail/main.py:142 - [Evaluate] iter = 40000 episode={ returns = -34.1951 lengths = 1000 } discounted_episode={ returns = -21.7724 lengths = 1000 } 
2022-07-08 09:57:47.976040 - gail/main.py:174 - [TRPO] iter = 41000 dist_mean = 0.1551 dist_std = 1.0181 vf_loss = 0.3698 grad_norm = 0.5068 nat_grad_norm = 0.5745 cg_residual = 0.0001 step_size = 0.3454 reward = 0.0000 fps = 3 mse_loss = 0.9402 
2022-07-08 09:58:07.069378 - gail/main.py:174 - [TRPO] iter = 42000 dist_mean = 0.1667 dist_std = 1.0137 vf_loss = 0.5348 grad_norm = 0.3802 nat_grad_norm = 0.5790 cg_residual = 0.0001 step_size = 0.3889 reward = 0.0000 fps = 3 mse_loss = 0.9016 
2022-07-08 09:58:27.125701 - gail/main.py:174 - [TRPO] iter = 43000 dist_mean = 0.3855 dist_std = 1.0122 vf_loss = 0.3970 grad_norm = 0.3657 nat_grad_norm = 0.7944 cg_residual = 0.0036 step_size = 0.3102 reward = 0.0000 fps = 3 mse_loss = 1.0559 
2022-07-08 09:58:47.686251 - gail/main.py:174 - [TRPO] iter = 44000 dist_mean = 0.2137 dist_std = 1.0122 vf_loss = 0.5646 grad_norm = 0.3842 nat_grad_norm = 0.5464 cg_residual = 0.0001 step_size = 0.4097 reward = 0.0000 fps = 3 mse_loss = 0.9620 
2022-07-08 09:59:07.285452 - gail/main.py:174 - [TRPO] iter = 45000 dist_mean = 0.2204 dist_std = 1.0104 vf_loss = 0.4237 grad_norm = 0.4770 nat_grad_norm = 0.5485 cg_residual = 0.0001 step_size = 0.3939 reward = -0.0000 fps = 3 mse_loss = 1.0136 
2022-07-08 09:59:08.075826 - gail/main.py:201 - [Discriminator] iter = 45000 loss = -1.5381 grad_norm = 13.5048 grad_penalty = 0.8845 regularization = 0.0000 true_logits = 0.6105 fake_logits = -1.8121 true_prob = 0.6391 fake_prob = 0.1479 
2022-07-08 10:03:31.697201 - gail/main.py:142 - [Evaluate] iter = 45000 episode={ returns = -33.3467 lengths = 1000 } discounted_episode={ returns = -20.8312 lengths = 1000 } 
2022-07-08 10:03:56.551348 - gail/main.py:174 - [TRPO] iter = 46000 dist_mean = 0.2291 dist_std = 1.0137 vf_loss = 0.4417 grad_norm = 0.4096 nat_grad_norm = 0.5569 cg_residual = 0.0001 step_size = 0.3971 reward = -0.0000 fps = 3 mse_loss = 1.0014 
2022-07-08 10:04:18.689594 - gail/main.py:174 - [TRPO] iter = 47000 dist_mean = 0.0323 dist_std = 1.0132 vf_loss = 0.3443 grad_norm = 0.5520 nat_grad_norm = 0.9630 cg_residual = 0.0038 step_size = 0.2728 reward = -0.0000 fps = 3 mse_loss = 1.1538 
2022-07-08 10:04:42.313535 - gail/main.py:174 - [TRPO] iter = 48000 dist_mean = 0.2324 dist_std = 1.0193 vf_loss = 0.5848 grad_norm = 0.3663 nat_grad_norm = 0.5649 cg_residual = 0.0002 step_size = 0.4119 reward = 0.0000 fps = 2 mse_loss = 1.0796 
2022-07-08 10:05:04.636799 - gail/main.py:174 - [TRPO] iter = 49000 dist_mean = 0.2317 dist_std = 1.0216 vf_loss = 0.5568 grad_norm = 0.4132 nat_grad_norm = 0.5796 cg_residual = 0.0003 step_size = 0.4018 reward = 0.0000 fps = 2 mse_loss = 1.0624 
2022-07-08 10:05:26.741928 - gail/main.py:174 - [TRPO] iter = 50000 dist_mean = 0.2381 dist_std = 1.0209 vf_loss = 0.4377 grad_norm = 0.4411 nat_grad_norm = 0.5880 cg_residual = 0.0002 step_size = 0.3853 reward = -0.0000 fps = 2 mse_loss = 1.0246 
2022-07-08 10:05:27.620878 - gail/main.py:201 - [Discriminator] iter = 50000 loss = -1.5074 grad_norm = 19.3714 grad_penalty = 0.9845 regularization = 0.0000 true_logits = 0.6667 fake_logits = -1.8252 true_prob = 0.6500 fake_prob = 0.1444 
2022-07-08 10:10:03.510126 - gail/main.py:142 - [Evaluate] iter = 50000 episode={ returns = -48.7324 lengths = 1000 } discounted_episode={ returns = -29.8998 lengths = 1000 } 
2022-07-08 10:10:26.120396 - gail/main.py:174 - [TRPO] iter = 51000 dist_mean = 0.2645 dist_std = 1.0218 vf_loss = 0.5650 grad_norm = 0.4244 nat_grad_norm = 0.5592 cg_residual = 0.0001 step_size = 0.3849 reward = -0.0000 fps = 3 mse_loss = 1.0329 
2022-07-08 10:10:48.739466 - gail/main.py:174 - [TRPO] iter = 52000 dist_mean = 0.2572 dist_std = 1.0198 vf_loss = 0.5545 grad_norm = 0.4299 nat_grad_norm = 0.5538 cg_residual = 0.0002 step_size = 0.4036 reward = -0.0000 fps = 3 mse_loss = 1.0814 
2022-07-08 10:11:11.090321 - gail/main.py:174 - [TRPO] iter = 53000 dist_mean = 0.2735 dist_std = 1.0219 vf_loss = 0.2840 grad_norm = 0.5379 nat_grad_norm = 0.6236 cg_residual = 0.0003 step_size = 0.3336 reward = -0.0000 fps = 2 mse_loss = 1.0514 
2022-07-08 10:11:35.173522 - gail/main.py:174 - [TRPO] iter = 54000 dist_mean = 0.2739 dist_std = 1.0253 vf_loss = 0.3985 grad_norm = 0.4405 nat_grad_norm = 0.6155 cg_residual = 0.0003 step_size = 0.3702 reward = -0.0000 fps = 2 mse_loss = 1.0410 
2022-07-08 10:12:05.792976 - gail/main.py:174 - [TRPO] iter = 55000 dist_mean = 0.2715 dist_std = 1.0276 vf_loss = 0.4056 grad_norm = 0.4407 nat_grad_norm = 0.5833 cg_residual = 0.0002 step_size = 0.3833 reward = 0.0000 fps = 2 mse_loss = 0.9731 
2022-07-08 10:12:06.667776 - gail/main.py:201 - [Discriminator] iter = 55000 loss = -1.8907 grad_norm = 19.5786 grad_penalty = 0.9540 regularization = 0.0000 true_logits = 0.7383 fake_logits = -2.1064 true_prob = 0.6645 fake_prob = 0.1135 
2022-07-08 10:16:38.133399 - gail/main.py:142 - [Evaluate] iter = 55000 episode={ returns = -61.2603 lengths = 1000 } discounted_episode={ returns = -35.3850 lengths = 1000 } 
2022-07-08 10:17:00.798737 - gail/main.py:174 - [TRPO] iter = 56000 dist_mean = 0.2864 dist_std = 1.0305 vf_loss = 0.3135 grad_norm = 0.4207 nat_grad_norm = 0.5822 cg_residual = 0.0003 step_size = 0.3905 reward = -0.0000 fps = 3 mse_loss = 1.0435 
2022-07-08 10:17:23.493416 - gail/main.py:174 - [TRPO] iter = 57000 dist_mean = 0.2899 dist_std = 1.0324 vf_loss = 0.5071 grad_norm = 0.4107 nat_grad_norm = 0.5510 cg_residual = 0.0002 step_size = 0.4007 reward = 0.0000 fps = 3 mse_loss = 1.0106 
2022-07-08 10:17:45.970699 - gail/main.py:174 - [TRPO] iter = 58000 dist_mean = 0.2898 dist_std = 1.0351 vf_loss = 0.3118 grad_norm = 0.4117 nat_grad_norm = 0.5517 cg_residual = 0.0003 step_size = 0.4117 reward = 0.0000 fps = 2 mse_loss = 1.0581 
2022-07-08 10:18:08.612954 - gail/main.py:174 - [TRPO] iter = 59000 dist_mean = 0.2946 dist_std = 1.0351 vf_loss = 0.3060 grad_norm = 0.5070 nat_grad_norm = 0.6147 cg_residual = 0.0003 step_size = 0.3563 reward = 0.0000 fps = 2 mse_loss = 0.9545 
2022-07-08 10:18:30.849500 - gail/main.py:174 - [TRPO] iter = 60000 dist_mean = 0.2892 dist_std = 1.0404 vf_loss = 0.3036 grad_norm = 0.4401 nat_grad_norm = 0.5820 cg_residual = 0.0004 step_size = 0.3924 reward = -0.0000 fps = 2 mse_loss = 0.9172 
2022-07-08 10:18:31.684152 - gail/main.py:201 - [Discriminator] iter = 60000 loss = -2.3826 grad_norm = 16.9095 grad_penalty = 0.7978 regularization = 0.0000 true_logits = 0.8491 fake_logits = -2.3312 true_prob = 0.6837 fake_prob = 0.0939 
2022-07-08 10:22:59.860036 - gail/main.py:142 - [Evaluate] iter = 60000 episode={ returns = -96.4857 lengths = 1000 } discounted_episode={ returns = -56.5462 lengths = 1000 } 
2022-07-08 10:23:21.262758 - gail/main.py:174 - [TRPO] iter = 61000 dist_mean = 0.2860 dist_std = 1.0362 vf_loss = 0.3403 grad_norm = 0.4576 nat_grad_norm = 0.5800 cg_residual = 0.0005 step_size = 0.3665 reward = 0.0000 fps = 3 mse_loss = 0.8737 
2022-07-08 10:23:43.804711 - gail/main.py:174 - [TRPO] iter = 62000 dist_mean = 0.2916 dist_std = 1.0378 vf_loss = 0.3683 grad_norm = 0.4514 nat_grad_norm = 0.5992 cg_residual = 0.0006 step_size = 0.3631 reward = 0.0000 fps = 3 mse_loss = 0.8492 
2022-07-08 10:24:06.061453 - gail/main.py:174 - [TRPO] iter = 63000 dist_mean = 0.2947 dist_std = 1.0405 vf_loss = 0.3069 grad_norm = 0.4099 nat_grad_norm = 0.6199 cg_residual = 0.0003 step_size = 0.3674 reward = 0.0000 fps = 2 mse_loss = 0.8492 
2022-07-08 10:24:27.878394 - gail/main.py:174 - [TRPO] iter = 64000 dist_mean = 0.2955 dist_std = 1.0391 vf_loss = 0.3841 grad_norm = 0.3911 nat_grad_norm = 0.5810 cg_residual = 0.0005 step_size = 0.4031 reward = -0.0000 fps = 2 mse_loss = 0.8472 
2022-07-08 10:24:51.724371 - gail/main.py:174 - [TRPO] iter = 65000 dist_mean = 0.2871 dist_std = 1.0419 vf_loss = 0.4481 grad_norm = 0.4698 nat_grad_norm = 0.6055 cg_residual = 0.0005 step_size = 0.3562 reward = -0.0000 fps = 2 mse_loss = 0.8890 
2022-07-08 10:24:52.513440 - gail/main.py:201 - [Discriminator] iter = 65000 loss = -2.9395 grad_norm = 18.8935 grad_penalty = 0.6377 regularization = 0.0000 true_logits = 0.9108 fake_logits = -2.6665 true_prob = 0.6950 fake_prob = 0.0704 
2022-07-08 10:29:34.179797 - gail/main.py:142 - [Evaluate] iter = 65000 episode={ returns = -146.5866 lengths = 1000 } discounted_episode={ returns = -87.4492 lengths = 1000 } 
2022-07-08 10:29:59.412502 - gail/main.py:174 - [TRPO] iter = 66000 dist_mean = 0.2785 dist_std = 1.0424 vf_loss = 0.3360 grad_norm = 0.4013 nat_grad_norm = 0.6420 cg_residual = 0.0004 step_size = 0.3722 reward = 0.0000 fps = 3 mse_loss = 0.8371 
2022-07-08 10:30:23.062882 - gail/main.py:174 - [TRPO] iter = 67000 dist_mean = 0.2890 dist_std = 1.0425 vf_loss = 0.5313 grad_norm = 0.3795 nat_grad_norm = 0.5910 cg_residual = 0.0006 step_size = 0.4023 reward = 0.0000 fps = 3 mse_loss = 0.8040 
2022-07-08 10:30:47.405525 - gail/main.py:174 - [TRPO] iter = 68000 dist_mean = 0.2877 dist_std = 1.0445 vf_loss = 0.6444 grad_norm = 0.4016 nat_grad_norm = 0.5814 cg_residual = 0.0007 step_size = 0.4024 reward = 0.0000 fps = 2 mse_loss = 0.7682 
2022-07-08 10:31:11.347455 - gail/main.py:174 - [TRPO] iter = 69000 dist_mean = 0.2871 dist_std = 1.0443 vf_loss = 0.4080 grad_norm = 0.3745 nat_grad_norm = 0.6159 cg_residual = 0.0006 step_size = 0.4184 reward = 0.0000 fps = 2 mse_loss = 0.7680 
2022-07-08 10:31:41.869242 - gail/main.py:174 - [TRPO] iter = 70000 dist_mean = 0.2729 dist_std = 1.0455 vf_loss = 0.3034 grad_norm = 0.4298 nat_grad_norm = 0.6456 cg_residual = 0.0008 step_size = 0.3646 reward = 0.0000 fps = 2 mse_loss = 0.7786 
2022-07-08 10:31:42.637855 - gail/main.py:201 - [Discriminator] iter = 70000 loss = -3.4366 grad_norm = 14.5353 grad_penalty = 0.6012 regularization = 0.0000 true_logits = 0.9979 fake_logits = -3.0399 true_prob = 0.7109 fake_prob = 0.0509 
