2022-07-08 09:04:45.461924 - utils/flags.py:257 - log_dir = logs/gail_w-HalfCheetah-v2-100-2022-07-08-09-04-45
2022-07-08 09:05:01.571688 - gail/utils/replay_buffer.py:86 - Load dataset from /content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/HalfCheetah-v2
2022-07-08 09:05:22.147045 - gail/main.py:80 - Expert Reward 6594.485577
2022-07-08 09:05:23.063642 - gail/main.py:84 - Original dataset size 3000
2022-07-08 09:05:23.159948 - gail/main.py:86 - Subsampled dataset size 3000
2022-07-08 09:05:23.181467 - gail/main.py:87 - np random: 864 random : 786
2022-07-08 09:05:23.185133 - gail/main.py:91 - Sampled obs: 0.6422, acs: 0.2570
2022-07-08 09:05:25.852049 - lunzi/nn/flat_param.py:17 - Enabling flattening... ['GaussianMLPPolicy_1/log_std:0', 'GaussianMLPPolicy_1/Variable:0', 'GaussianMLPPolicy_1/Variable_1:0', 'GaussianMLPPolicy_1/Variable_2:0', 'GaussianMLPPolicy_1/Variable_3:0', 'GaussianMLPPolicy_1/Variable_4:0', 'GaussianMLPPolicy_1/Variable_5:0']
2022-07-08 09:05:49.631400 - gail/discriminator/discriminator.py:30 - Use predefined normalization.
2022-07-08 09:05:49.654118 - gail/discriminator/discriminator.py:33 - Normalizer loc:[[-1.0330443e-01 -9.3026832e-03  2.9791275e-01  8.1040427e-02
   1.7489190e-01  4.0859219e-01 -3.0232381e-02 -9.5055901e-02
   7.1118622e+00  5.5424552e-03 -1.8441556e-01  3.1117105e-01
  -2.7393317e-01  2.6658252e-01  4.0665963e-01 -2.8198040e-01
   2.5822452e-01]] 
 scale:[[ 0.03102476  0.07177044  0.37663096  0.5276757   0.46577376  0.12057849
   0.35446092  0.3551628   1.290817    0.4731574   1.4939526  12.314076
  14.637607   14.150712    3.6817648  11.124868    9.962843  ]]
2022-07-08 09:05:59.914068 - gail/discriminator/discriminator.py:67 - Discriminator uses Wasserstein distance.
2022-07-08 09:05:59.919904 - gail/discriminator/discriminator.py:68 - [<tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable:0' shape=(23, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_1:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_2:0' shape=(100, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_3:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_4:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_5:0' shape=(1,) dtype=float32_ref>]
2022-07-08 09:05:59.924741 - gail/discriminator/discriminator.py:69 - Use gradient penalty regularization (coef = 10.000000)
2022-07-08 09:06:02.153794 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions_mean]
2022-07-08 09:08:39.263859 - gail/main.py:142 - [Evaluate] iter = 0 episode={ returns = -0.4623 lengths = 1000 } discounted_episode={ returns = -0.4535 lengths = 1000 } 
2022-07-08 09:08:39.265827 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions]
2022-07-08 09:08:58.310073 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [fake_states fake_actions] => [scaled_neural_reward]
2022-07-08 09:08:59.069810 - lunzi/nn/module.py:71 - [MLPVFunction] is making TensorFlow callables, key = [states] => [values]
2022-07-08 09:09:00.711065 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [] => [sync_old]
2022-07-08 09:09:01.409064 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss flat_grad dist_std mean_kl dist_mean]
2022-07-08 09:09:06.323671 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states tangents actions] => [hessian_vec_prod]
2022-07-08 09:09:15.833592 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [] => [get_flat]
2022-07-08 09:09:16.706385 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [feed_flat] => [set_flat]
2022-07-08 09:09:17.562131 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss mean_kl]
2022-07-08 09:09:19.135239 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [train_vf vf_loss]
2022-07-08 09:09:21.904982 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [vf_loss]
2022-07-08 09:09:22.859709 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states actions_] => [mse_loss]
2022-07-08 09:09:23.697442 - gail/main.py:174 - [TRPO] iter = 1000 dist_mean = -0.0000 dist_std = 1.0000 vf_loss = 0.6155 grad_norm = 0.2855 nat_grad_norm = 0.5289 cg_residual = 0.0000 step_size = 0.4203 reward = 0.0000 fps = 4 mse_loss = 0.9499 
2022-07-08 09:09:42.526407 - gail/main.py:174 - [TRPO] iter = 2000 dist_mean = -0.0088 dist_std = 1.0052 vf_loss = 1.3476 grad_norm = 0.2954 nat_grad_norm = 0.5110 cg_residual = 0.0000 step_size = 0.4162 reward = 0.0000 fps = 4 mse_loss = 0.8927 
2022-07-08 09:09:58.771273 - gail/main.py:174 - [TRPO] iter = 3000 dist_mean = 0.0005 dist_std = 1.0109 vf_loss = 0.6947 grad_norm = 0.3094 nat_grad_norm = 0.5694 cg_residual = 0.0000 step_size = 0.3867 reward = 0.0000 fps = 4 mse_loss = 0.9085 
2022-07-08 09:10:17.924879 - gail/main.py:174 - [TRPO] iter = 4000 dist_mean = 0.0034 dist_std = 1.0135 vf_loss = 0.7183 grad_norm = 0.2627 nat_grad_norm = 0.4766 cg_residual = 0.0000 step_size = 0.4691 reward = -0.0000 fps = 3 mse_loss = 0.9739 
2022-07-08 09:10:39.643279 - gail/main.py:174 - [TRPO] iter = 5000 dist_mean = 0.0106 dist_std = 1.0138 vf_loss = 0.8461 grad_norm = 0.3189 nat_grad_norm = 0.5651 cg_residual = 0.0000 step_size = 0.3892 reward = -0.0000 fps = 3 mse_loss = 1.0079 
2022-07-08 09:10:39.647138 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [true_states true_actions fake_states fake_actions true_masks] => [train loss true_logits fake_logits true_prob fake_prob grad_norm grad_penalty regularization]
2022-07-08 09:10:48.257728 - gail/main.py:201 - [Discriminator] iter = 5000 loss = 0.3933 grad_norm = 38.9849 grad_penalty = 1.8469 regularization = 0.0000 true_logits = -0.0813 fake_logits = -1.5349 true_prob = 0.4797 fake_prob = 0.2288 
2022-07-08 09:15:30.299866 - gail/main.py:142 - [Evaluate] iter = 5000 episode={ returns = -4.0572 lengths = 1000 } discounted_episode={ returns = -2.6743 lengths = 1000 } 
2022-07-08 09:15:53.635433 - gail/main.py:174 - [TRPO] iter = 6000 dist_mean = 0.0169 dist_std = 1.0109 vf_loss = 1.0888 grad_norm = 0.3653 nat_grad_norm = 0.7862 cg_residual = 0.0000 step_size = 0.3024 reward = -0.0000 fps = 3 mse_loss = 0.9676 
2022-07-08 09:16:16.200051 - gail/main.py:174 - [TRPO] iter = 7000 dist_mean = 0.0231 dist_std = 1.0130 vf_loss = 3.8571 grad_norm = 0.2787 nat_grad_norm = 0.5615 cg_residual = 0.0001 step_size = 0.4153 reward = 0.0000 fps = 3 mse_loss = 1.0235 
2022-07-08 09:16:37.933864 - gail/main.py:174 - [TRPO] iter = 8000 dist_mean = 0.0182 dist_std = 1.0125 vf_loss = 0.9335 grad_norm = 0.4036 nat_grad_norm = 0.5168 cg_residual = 0.0001 step_size = 0.3964 reward = -0.0000 fps = 2 mse_loss = 1.0459 
2022-07-08 09:17:01.409675 - gail/main.py:174 - [TRPO] iter = 9000 dist_mean = 0.0192 dist_std = 1.0068 vf_loss = 1.6190 grad_norm = 0.3112 nat_grad_norm = 0.5929 cg_residual = 0.0000 step_size = 0.3840 reward = 0.0000 fps = 2 mse_loss = 1.0027 
2022-07-08 09:17:24.452706 - gail/main.py:174 - [TRPO] iter = 10000 dist_mean = 0.0214 dist_std = 1.0072 vf_loss = 3.6663 grad_norm = 0.3028 nat_grad_norm = 0.5128 cg_residual = 0.0000 step_size = 0.4310 reward = 0.0000 fps = 2 mse_loss = 0.9865 
2022-07-08 09:17:25.274287 - gail/main.py:201 - [Discriminator] iter = 10000 loss = -0.3619 grad_norm = 34.4331 grad_penalty = 1.9658 regularization = 0.0000 true_logits = -0.0334 fake_logits = -2.3611 true_prob = 0.4917 fake_prob = 0.1517 
2022-07-08 09:22:02.722335 - gail/main.py:142 - [Evaluate] iter = 10000 episode={ returns = -10.1127 lengths = 1000 } discounted_episode={ returns = -6.7216 lengths = 1000 } 
2022-07-08 09:22:25.665622 - gail/main.py:174 - [TRPO] iter = 11000 dist_mean = -0.0233 dist_std = 1.0061 vf_loss = 1.3460 grad_norm = 0.3120 nat_grad_norm = 0.5757 cg_residual = 0.0001 step_size = 0.3977 reward = -0.0000 fps = 3 mse_loss = 0.9906 
2022-07-08 09:22:48.870735 - gail/main.py:174 - [TRPO] iter = 12000 dist_mean = -0.0152 dist_std = 1.0105 vf_loss = 2.8153 grad_norm = 0.3072 nat_grad_norm = 0.4712 cg_residual = 0.0000 step_size = 0.4671 reward = -0.0000 fps = 3 mse_loss = 0.9661 
2022-07-08 09:23:11.919685 - gail/main.py:174 - [TRPO] iter = 13000 dist_mean = 0.0180 dist_std = 1.0171 vf_loss = 1.9099 grad_norm = 0.2819 nat_grad_norm = 0.5699 cg_residual = 0.0000 step_size = 0.4182 reward = -0.0000 fps = 2 mse_loss = 0.9853 
2022-07-08 09:23:35.501614 - gail/main.py:174 - [TRPO] iter = 14000 dist_mean = 0.0236 dist_std = 1.0202 vf_loss = 4.0901 grad_norm = 0.2811 nat_grad_norm = 0.5132 cg_residual = 0.0000 step_size = 0.4333 reward = -0.0000 fps = 2 mse_loss = 1.0281 
2022-07-08 09:23:58.326491 - gail/main.py:174 - [TRPO] iter = 15000 dist_mean = 0.0223 dist_std = 1.0182 vf_loss = 2.5975 grad_norm = 0.3162 nat_grad_norm = 0.6553 cg_residual = 0.0000 step_size = 0.3637 reward = -0.0000 fps = 2 mse_loss = 0.9883 
2022-07-08 09:23:59.033123 - gail/main.py:201 - [Discriminator] iter = 15000 loss = -2.5453 grad_norm = 22.1433 grad_penalty = 0.8238 regularization = 0.0000 true_logits = 0.0332 fake_logits = -3.3359 true_prob = 0.5083 fake_prob = 0.0509 
2022-07-08 09:28:18.359958 - gail/main.py:142 - [Evaluate] iter = 15000 episode={ returns = -10.6767 lengths = 1000 } discounted_episode={ returns = -6.7731 lengths = 1000 } 
2022-07-08 09:28:40.656033 - gail/main.py:174 - [TRPO] iter = 16000 dist_mean = 0.0285 dist_std = 1.0198 vf_loss = 2.7145 grad_norm = 0.3407 nat_grad_norm = 0.6946 cg_residual = 0.0000 step_size = 0.3396 reward = 0.0000 fps = 3 mse_loss = 0.9980 
2022-07-08 09:29:02.766175 - gail/main.py:174 - [TRPO] iter = 17000 dist_mean = 0.0648 dist_std = 1.0211 vf_loss = 4.6963 grad_norm = 0.2955 nat_grad_norm = 0.5285 cg_residual = 0.0000 step_size = 0.4272 reward = 0.0000 fps = 3 mse_loss = 0.9699 
2022-07-08 09:29:24.566982 - gail/main.py:174 - [TRPO] iter = 18000 dist_mean = 0.0423 dist_std = 1.0226 vf_loss = 6.8839 grad_norm = 0.2906 nat_grad_norm = 0.5204 cg_residual = 0.0000 step_size = 0.4403 reward = 0.0000 fps = 3 mse_loss = 1.0002 
2022-07-08 09:29:45.620350 - gail/main.py:174 - [TRPO] iter = 19000 dist_mean = 0.0432 dist_std = 1.0162 vf_loss = 6.2110 grad_norm = 0.2903 nat_grad_norm = 0.4909 cg_residual = 0.0000 step_size = 0.4459 reward = -0.0000 fps = 2 mse_loss = 1.0264 
2022-07-08 09:30:06.704423 - gail/main.py:174 - [TRPO] iter = 20000 dist_mean = 0.0545 dist_std = 1.0193 vf_loss = 4.9877 grad_norm = 0.2868 nat_grad_norm = 0.5528 cg_residual = 0.0000 step_size = 0.4364 reward = 0.0000 fps = 2 mse_loss = 1.0043 
2022-07-08 09:30:07.467657 - gail/main.py:201 - [Discriminator] iter = 20000 loss = 0.8056 grad_norm = 25.9049 grad_penalty = 1.5739 regularization = 0.0000 true_logits = 0.1060 fake_logits = -0.6623 true_prob = 0.5264 fake_prob = 0.3449 
2022-07-08 09:34:27.372395 - gail/main.py:142 - [Evaluate] iter = 20000 episode={ returns = -4.9802 lengths = 1000 } discounted_episode={ returns = -3.9296 lengths = 1000 } 
2022-07-08 09:34:48.807395 - gail/main.py:174 - [TRPO] iter = 21000 dist_mean = 0.0611 dist_std = 1.0222 vf_loss = 3.5499 grad_norm = 0.3057 nat_grad_norm = 0.4804 cg_residual = 0.0000 step_size = 0.4698 reward = 0.0000 fps = 3 mse_loss = 0.9638 
2022-07-08 09:35:09.690634 - gail/main.py:174 - [TRPO] iter = 22000 dist_mean = 0.0702 dist_std = 1.0230 vf_loss = 1.2693 grad_norm = 0.3479 nat_grad_norm = 0.5611 cg_residual = 0.0000 step_size = 0.4183 reward = -0.0000 fps = 3 mse_loss = 1.0051 
2022-07-08 09:35:30.494321 - gail/main.py:174 - [TRPO] iter = 23000 dist_mean = 0.0795 dist_std = 1.0216 vf_loss = 3.1766 grad_norm = 0.2692 nat_grad_norm = 0.4861 cg_residual = 0.0000 step_size = 0.4772 reward = 0.0000 fps = 3 mse_loss = 0.9742 
2022-07-08 09:35:50.932696 - gail/main.py:174 - [TRPO] iter = 24000 dist_mean = 0.1043 dist_std = 1.0218 vf_loss = 1.7529 grad_norm = 0.3681 nat_grad_norm = 0.5345 cg_residual = 0.0000 step_size = 0.4106 reward = -0.0000 fps = 2 mse_loss = 1.0032 
2022-07-08 09:36:11.527740 - gail/main.py:174 - [TRPO] iter = 25000 dist_mean = 0.0934 dist_std = 1.0229 vf_loss = 2.2879 grad_norm = 0.2642 nat_grad_norm = 0.5454 cg_residual = 0.0000 step_size = 0.4479 reward = -0.0000 fps = 2 mse_loss = 1.0384 
2022-07-08 09:36:12.332414 - gail/main.py:201 - [Discriminator] iter = 25000 loss = -2.9322 grad_norm = 25.2504 grad_penalty = 0.7118 regularization = 0.0000 true_logits = 0.1558 fake_logits = -3.4881 true_prob = 0.5388 fake_prob = 0.0518 
2022-07-08 09:40:21.019158 - gail/main.py:142 - [Evaluate] iter = 25000 episode={ returns = -19.0641 lengths = 1000 } discounted_episode={ returns = -12.3346 lengths = 1000 } 
2022-07-08 09:40:41.983340 - gail/main.py:174 - [TRPO] iter = 26000 dist_mean = 0.1373 dist_std = 1.0203 vf_loss = 3.3495 grad_norm = 0.3536 nat_grad_norm = 0.5319 cg_residual = 0.0001 step_size = 0.4062 reward = -0.0000 fps = 3 mse_loss = 1.0386 
2022-07-08 09:41:01.875791 - gail/main.py:174 - [TRPO] iter = 27000 dist_mean = 0.1020 dist_std = 1.0126 vf_loss = 2.3186 grad_norm = 0.3468 nat_grad_norm = 0.6294 cg_residual = 0.0000 step_size = 0.3678 reward = 0.0000 fps = 3 mse_loss = 1.0199 
2022-07-08 09:41:22.342059 - gail/main.py:174 - [TRPO] iter = 28000 dist_mean = 0.1620 dist_std = 1.0150 vf_loss = 5.5702 grad_norm = 0.3095 nat_grad_norm = 0.5669 cg_residual = 0.0001 step_size = 0.4194 reward = -0.0000 fps = 3 mse_loss = 1.0362 
2022-07-08 09:41:43.023037 - gail/main.py:174 - [TRPO] iter = 29000 dist_mean = 0.1456 dist_std = 1.0159 vf_loss = 2.4632 grad_norm = 0.3267 nat_grad_norm = 0.5690 cg_residual = 0.0000 step_size = 0.4081 reward = -0.0000 fps = 3 mse_loss = 1.1037 
2022-07-08 09:42:04.516504 - gail/main.py:174 - [TRPO] iter = 30000 dist_mean = 0.1499 dist_std = 1.0090 vf_loss = 1.1807 grad_norm = 0.3343 nat_grad_norm = 0.5407 cg_residual = 0.0000 step_size = 0.4143 reward = 0.0000 fps = 2 mse_loss = 1.0784 
2022-07-08 09:42:05.811634 - gail/main.py:201 - [Discriminator] iter = 30000 loss = -0.6850 grad_norm = 29.3487 grad_penalty = 1.2840 regularization = 0.0000 true_logits = 0.1955 fake_logits = -1.7735 true_prob = 0.5485 fake_prob = 0.1855 
2022-07-08 09:46:04.848557 - gail/main.py:142 - [Evaluate] iter = 30000 episode={ returns = -33.0873 lengths = 1000 } discounted_episode={ returns = -21.6265 lengths = 1000 } 
2022-07-08 09:46:24.611815 - gail/main.py:174 - [TRPO] iter = 31000 dist_mean = 0.1840 dist_std = 1.0074 vf_loss = 4.3252 grad_norm = 0.3763 nat_grad_norm = 0.5895 cg_residual = 0.0000 step_size = 0.3706 reward = 0.0000 fps = 3 mse_loss = 1.0875 
2022-07-08 09:46:44.932614 - gail/main.py:174 - [TRPO] iter = 32000 dist_mean = 0.2019 dist_std = 1.0050 vf_loss = 1.9712 grad_norm = 0.3304 nat_grad_norm = 0.5065 cg_residual = 0.0000 step_size = 0.4483 reward = 0.0000 fps = 3 mse_loss = 1.1095 
2022-07-08 09:47:05.307315 - gail/main.py:174 - [TRPO] iter = 33000 dist_mean = 0.2106 dist_std = 0.9996 vf_loss = 1.7871 grad_norm = 0.3570 nat_grad_norm = 0.5647 cg_residual = 0.0001 step_size = 0.3897 reward = 0.0000 fps = 3 mse_loss = 1.0805 
2022-07-08 09:47:25.354882 - gail/main.py:174 - [TRPO] iter = 34000 dist_mean = 0.2097 dist_std = 0.9953 vf_loss = 0.7664 grad_norm = 0.4186 nat_grad_norm = 0.5359 cg_residual = 0.0001 step_size = 0.3817 reward = 0.0000 fps = 3 mse_loss = 0.9805 
2022-07-08 09:47:45.138487 - gail/main.py:174 - [TRPO] iter = 35000 dist_mean = 0.2188 dist_std = 0.9960 vf_loss = 0.8671 grad_norm = 0.3630 nat_grad_norm = 0.5353 cg_residual = 0.0001 step_size = 0.4062 reward = -0.0000 fps = 2 mse_loss = 0.9529 
2022-07-08 09:47:45.886478 - gail/main.py:201 - [Discriminator] iter = 35000 loss = -0.2401 grad_norm = 15.9093 grad_penalty = 1.0870 regularization = 0.0000 true_logits = 0.2522 fake_logits = -1.0749 true_prob = 0.5623 fake_prob = 0.2610 
2022-07-08 09:51:44.077494 - gail/main.py:142 - [Evaluate] iter = 35000 episode={ returns = -40.0057 lengths = 1000 } discounted_episode={ returns = -25.7558 lengths = 1000 } 
2022-07-08 09:52:11.794229 - gail/main.py:174 - [TRPO] iter = 36000 dist_mean = 0.2294 dist_std = 0.9887 vf_loss = 1.2824 grad_norm = 0.3744 nat_grad_norm = 0.5772 cg_residual = 0.0001 step_size = 0.3803 reward = 0.0000 fps = 3 mse_loss = 0.9582 
2022-07-08 09:52:31.541383 - gail/main.py:174 - [TRPO] iter = 37000 dist_mean = 0.1818 dist_std = 0.9886 vf_loss = 1.4700 grad_norm = 0.3455 nat_grad_norm = 0.5389 cg_residual = 0.0000 step_size = 0.4118 reward = -0.0000 fps = 3 mse_loss = 0.9878 
2022-07-08 09:52:52.104558 - gail/main.py:174 - [TRPO] iter = 38000 dist_mean = 0.2313 dist_std = 0.9939 vf_loss = 1.7274 grad_norm = 0.3765 nat_grad_norm = 0.5776 cg_residual = 0.0001 step_size = 0.3851 reward = 0.0000 fps = 3 mse_loss = 0.9820 
2022-07-08 09:53:12.950224 - gail/main.py:174 - [TRPO] iter = 39000 dist_mean = 0.1931 dist_std = 0.9967 vf_loss = 2.8112 grad_norm = 0.3735 nat_grad_norm = 0.5898 cg_residual = 0.0001 step_size = 0.3735 reward = -0.0000 fps = 3 mse_loss = 0.9864 
2022-07-08 09:53:34.470146 - gail/main.py:174 - [TRPO] iter = 40000 dist_mean = 0.2457 dist_std = 0.9922 vf_loss = 1.2412 grad_norm = 0.3372 nat_grad_norm = 0.5323 cg_residual = 0.0001 step_size = 0.4345 reward = -0.0000 fps = 2 mse_loss = 1.0130 
2022-07-08 09:53:35.237854 - gail/main.py:201 - [Discriminator] iter = 40000 loss = -0.6912 grad_norm = 15.3772 grad_penalty = 0.9456 regularization = 0.0000 true_logits = 0.3220 fake_logits = -1.3148 true_prob = 0.5790 fake_prob = 0.2198 
2022-07-08 09:57:27.976232 - gail/main.py:142 - [Evaluate] iter = 40000 episode={ returns = -41.6862 lengths = 1000 } discounted_episode={ returns = -27.6150 lengths = 1000 } 
2022-07-08 09:57:46.935218 - gail/main.py:174 - [TRPO] iter = 41000 dist_mean = 0.2145 dist_std = 0.9847 vf_loss = 4.4396 grad_norm = 0.3390 nat_grad_norm = 0.5778 cg_residual = 0.0000 step_size = 0.4000 reward = -0.0000 fps = 3 mse_loss = 0.9533 
2022-07-08 09:58:06.057560 - gail/main.py:174 - [TRPO] iter = 42000 dist_mean = 0.2434 dist_std = 0.9871 vf_loss = 1.1560 grad_norm = 0.3954 nat_grad_norm = 0.5742 cg_residual = 0.0001 step_size = 0.3898 reward = 0.0000 fps = 3 mse_loss = 0.9623 
2022-07-08 09:58:25.969473 - gail/main.py:174 - [TRPO] iter = 43000 dist_mean = 0.2592 dist_std = 0.9898 vf_loss = 0.9575 grad_norm = 0.3553 nat_grad_norm = 0.5648 cg_residual = 0.0001 step_size = 0.4177 reward = 0.0000 fps = 3 mse_loss = 0.9173 
2022-07-08 09:58:47.251890 - gail/main.py:174 - [TRPO] iter = 44000 dist_mean = 0.2566 dist_std = 0.9876 vf_loss = 1.0315 grad_norm = 0.3742 nat_grad_norm = 0.5285 cg_residual = 0.0001 step_size = 0.4225 reward = -0.0000 fps = 3 mse_loss = 0.8705 
2022-07-08 09:59:07.413880 - gail/main.py:174 - [TRPO] iter = 45000 dist_mean = 0.2648 dist_std = 0.9785 vf_loss = 0.5719 grad_norm = 0.3688 nat_grad_norm = 0.5008 cg_residual = 0.0001 step_size = 0.4497 reward = 0.0000 fps = 3 mse_loss = 0.8755 
2022-07-08 09:59:08.167906 - gail/main.py:201 - [Discriminator] iter = 45000 loss = -0.9098 grad_norm = 14.3278 grad_penalty = 0.8616 regularization = 0.0000 true_logits = 0.3857 fake_logits = -1.3857 true_prob = 0.5939 fake_prob = 0.2081 
2022-07-08 10:03:34.673234 - gail/main.py:142 - [Evaluate] iter = 45000 episode={ returns = -67.8794 lengths = 1000 } discounted_episode={ returns = -43.3818 lengths = 1000 } 
2022-07-08 10:03:59.400605 - gail/main.py:174 - [TRPO] iter = 46000 dist_mean = 0.2727 dist_std = 0.9836 vf_loss = 0.7996 grad_norm = 0.4401 nat_grad_norm = 0.5868 cg_residual = 0.0002 step_size = 0.3582 reward = 0.0000 fps = 3 mse_loss = 0.8581 
2022-07-08 10:04:21.754590 - gail/main.py:174 - [TRPO] iter = 47000 dist_mean = 0.2869 dist_std = 0.9855 vf_loss = 0.7223 grad_norm = 0.4446 nat_grad_norm = 0.5807 cg_residual = 0.0001 step_size = 0.3743 reward = -0.0000 fps = 3 mse_loss = 0.8115 
2022-07-08 10:04:44.448138 - gail/main.py:174 - [TRPO] iter = 48000 dist_mean = 0.2281 dist_std = 0.9811 vf_loss = 3.9300 grad_norm = 0.3458 nat_grad_norm = 0.5534 cg_residual = 0.0001 step_size = 0.4267 reward = -0.0000 fps = 2 mse_loss = 0.8097 
2022-07-08 10:05:06.704094 - gail/main.py:174 - [TRPO] iter = 49000 dist_mean = 0.3007 dist_std = 0.9816 vf_loss = 1.0749 grad_norm = 0.3848 nat_grad_norm = 0.6540 cg_residual = 0.0002 step_size = 0.3836 reward = -0.0000 fps = 2 mse_loss = 0.8127 
2022-07-08 10:05:29.704357 - gail/main.py:174 - [TRPO] iter = 50000 dist_mean = 0.2519 dist_std = 0.9831 vf_loss = 4.8162 grad_norm = 0.3937 nat_grad_norm = 0.6848 cg_residual = 0.0001 step_size = 0.3581 reward = 0.0000 fps = 2 mse_loss = 0.8065 
2022-07-08 10:05:30.568290 - gail/main.py:201 - [Discriminator] iter = 50000 loss = -1.7827 grad_norm = 16.4631 grad_penalty = 0.9549 regularization = 0.0000 true_logits = 0.4618 fake_logits = -2.2758 true_prob = 0.6113 fake_prob = 0.1314 
2022-07-08 10:10:07.863816 - gail/main.py:142 - [Evaluate] iter = 50000 episode={ returns = -80.5169 lengths = 1000 } discounted_episode={ returns = -52.2835 lengths = 1000 } 
2022-07-08 10:10:30.672644 - gail/main.py:174 - [TRPO] iter = 51000 dist_mean = 0.2989 dist_std = 0.9809 vf_loss = 0.5899 grad_norm = 0.3864 nat_grad_norm = 0.5476 cg_residual = 0.0002 step_size = 0.4167 reward = 0.0000 fps = 3 mse_loss = 0.8329 
2022-07-08 10:10:53.712680 - gail/main.py:174 - [TRPO] iter = 52000 dist_mean = 0.3184 dist_std = 0.9788 vf_loss = 0.7270 grad_norm = 0.4184 nat_grad_norm = 0.6256 cg_residual = 0.0002 step_size = 0.3744 reward = 0.0000 fps = 3 mse_loss = 0.8409 
2022-07-08 10:11:17.047995 - gail/main.py:174 - [TRPO] iter = 53000 dist_mean = 0.3185 dist_std = 0.9784 vf_loss = 0.5538 grad_norm = 0.4103 nat_grad_norm = 0.5768 cg_residual = 0.0003 step_size = 0.3961 reward = -0.0000 fps = 2 mse_loss = 0.8436 
2022-07-08 10:11:40.843504 - gail/main.py:174 - [TRPO] iter = 54000 dist_mean = 0.3196 dist_std = 0.9759 vf_loss = 0.6255 grad_norm = 0.4390 nat_grad_norm = 0.6630 cg_residual = 0.0003 step_size = 0.3564 reward = 0.0000 fps = 2 mse_loss = 0.7917 
2022-07-08 10:12:10.858592 - gail/main.py:174 - [TRPO] iter = 55000 dist_mean = 0.3331 dist_std = 0.9718 vf_loss = 0.5712 grad_norm = 0.3950 nat_grad_norm = 0.6249 cg_residual = 0.0003 step_size = 0.3757 reward = -0.0000 fps = 2 mse_loss = 0.7591 
2022-07-08 10:12:11.704062 - gail/main.py:201 - [Discriminator] iter = 55000 loss = -1.5867 grad_norm = 13.2073 grad_penalty = 0.7605 regularization = 0.0000 true_logits = 0.5543 fake_logits = -1.7930 true_prob = 0.6313 fake_prob = 0.1512 
2022-07-08 10:16:43.146141 - gail/main.py:142 - [Evaluate] iter = 55000 episode={ returns = -109.6824 lengths = 1000 } discounted_episode={ returns = -70.0702 lengths = 1000 } 
2022-07-08 10:17:06.243145 - gail/main.py:174 - [TRPO] iter = 56000 dist_mean = 0.3424 dist_std = 0.9687 vf_loss = 0.4623 grad_norm = 0.4570 nat_grad_norm = 0.5961 cg_residual = 0.0003 step_size = 0.3760 reward = -0.0000 fps = 3 mse_loss = 0.7392 
2022-07-08 10:17:28.477983 - gail/main.py:174 - [TRPO] iter = 57000 dist_mean = 0.3395 dist_std = 0.9645 vf_loss = 0.3427 grad_norm = 0.3936 nat_grad_norm = 0.6016 cg_residual = 0.0002 step_size = 0.3902 reward = 0.0000 fps = 3 mse_loss = 0.6862 
2022-07-08 10:17:51.548446 - gail/main.py:174 - [TRPO] iter = 58000 dist_mean = 0.3570 dist_std = 0.9632 vf_loss = 0.4734 grad_norm = 0.5798 nat_grad_norm = 0.5998 cg_residual = 0.0003 step_size = 0.3433 reward = 0.0000 fps = 2 mse_loss = 0.7088 
2022-07-08 10:18:13.139995 - gail/main.py:174 - [TRPO] iter = 59000 dist_mean = 0.3552 dist_std = 0.9646 vf_loss = 0.3398 grad_norm = 0.3908 nat_grad_norm = 0.5594 cg_residual = 0.0003 step_size = 0.4167 reward = -0.0000 fps = 2 mse_loss = 0.6759 
2022-07-08 10:18:34.859681 - gail/main.py:174 - [TRPO] iter = 60000 dist_mean = 0.3589 dist_std = 0.9545 vf_loss = 0.2731 grad_norm = 0.4412 nat_grad_norm = 0.6127 cg_residual = 0.0007 step_size = 0.3868 reward = -0.0000 fps = 2 mse_loss = 0.6614 
2022-07-08 10:18:35.694467 - gail/main.py:201 - [Discriminator] iter = 60000 loss = -1.9706 grad_norm = 13.4872 grad_penalty = 0.7744 regularization = 0.0000 true_logits = 0.6622 fake_logits = -2.0828 true_prob = 0.6543 fake_prob = 0.1186 
2022-07-08 10:23:06.105880 - gail/main.py:142 - [Evaluate] iter = 60000 episode={ returns = -131.2378 lengths = 1000 } discounted_episode={ returns = -82.9888 lengths = 1000 } 
2022-07-08 10:23:29.038156 - gail/main.py:174 - [TRPO] iter = 61000 dist_mean = 0.3627 dist_std = 0.9549 vf_loss = 0.3436 grad_norm = 0.4162 nat_grad_norm = 0.5845 cg_residual = 0.0004 step_size = 0.3939 reward = 0.0000 fps = 3 mse_loss = 0.6805 
2022-07-08 10:23:51.463623 - gail/main.py:174 - [TRPO] iter = 62000 dist_mean = 0.3645 dist_std = 0.9565 vf_loss = 0.4182 grad_norm = 0.4624 nat_grad_norm = 0.6106 cg_residual = 0.0005 step_size = 0.3690 reward = -0.0000 fps = 3 mse_loss = 0.6549 
2022-07-08 10:24:13.636482 - gail/main.py:174 - [TRPO] iter = 63000 dist_mean = 0.3861 dist_std = 0.9569 vf_loss = 0.3498 grad_norm = 0.4478 nat_grad_norm = 0.5736 cg_residual = 0.0005 step_size = 0.3961 reward = -0.0000 fps = 2 mse_loss = 0.6414 
2022-07-08 10:24:36.414548 - gail/main.py:174 - [TRPO] iter = 64000 dist_mean = 0.3660 dist_std = 0.9526 vf_loss = 0.3343 grad_norm = 0.4523 nat_grad_norm = 0.6070 cg_residual = 0.0007 step_size = 0.3743 reward = -0.0000 fps = 2 mse_loss = 0.6448 
2022-07-08 10:24:58.805240 - gail/main.py:174 - [TRPO] iter = 65000 dist_mean = 0.3809 dist_std = 0.9526 vf_loss = 0.4780 grad_norm = 0.4492 nat_grad_norm = 0.6018 cg_residual = 0.0007 step_size = 0.3682 reward = 0.0000 fps = 2 mse_loss = 0.6049 
2022-07-08 10:24:59.504364 - gail/main.py:201 - [Discriminator] iter = 65000 loss = -2.5229 grad_norm = 14.2517 grad_penalty = 0.6811 regularization = 0.0000 true_logits = 0.6876 fake_logits = -2.5164 true_prob = 0.6589 fake_prob = 0.0817 
2022-07-08 10:29:41.794647 - gail/main.py:142 - [Evaluate] iter = 65000 episode={ returns = -160.4824 lengths = 1000 } discounted_episode={ returns = -101.4913 lengths = 1000 } 
2022-07-08 10:30:06.001258 - gail/main.py:174 - [TRPO] iter = 66000 dist_mean = 0.3856 dist_std = 0.9493 vf_loss = 0.3558 grad_norm = 0.4826 nat_grad_norm = 0.6158 cg_residual = 0.0009 step_size = 0.3650 reward = -0.0000 fps = 3 mse_loss = 0.5936 
2022-07-08 10:30:30.578687 - gail/main.py:174 - [TRPO] iter = 67000 dist_mean = 0.3897 dist_std = 0.9483 vf_loss = 0.5946 grad_norm = 0.3833 nat_grad_norm = 0.5782 cg_residual = 0.0004 step_size = 0.4066 reward = -0.0000 fps = 3 mse_loss = 0.5885 
2022-07-08 10:30:55.099196 - gail/main.py:174 - [TRPO] iter = 68000 dist_mean = 0.3850 dist_std = 0.9437 vf_loss = 0.4756 grad_norm = 0.4625 nat_grad_norm = 0.5961 cg_residual = 0.0007 step_size = 0.3654 reward = 0.0000 fps = 2 mse_loss = 0.6103 
2022-07-08 10:31:18.329505 - gail/main.py:174 - [TRPO] iter = 69000 dist_mean = 0.3741 dist_std = 0.9416 vf_loss = 0.5862 grad_norm = 0.4793 nat_grad_norm = 0.6965 cg_residual = 0.0013 step_size = 0.3499 reward = -0.0000 fps = 2 mse_loss = 0.5952 
2022-07-08 10:31:48.212382 - gail/main.py:174 - [TRPO] iter = 70000 dist_mean = 0.3795 dist_std = 0.9386 vf_loss = 0.9139 grad_norm = 0.4592 nat_grad_norm = 0.5714 cg_residual = 0.0009 step_size = 0.3915 reward = -0.0000 fps = 2 mse_loss = 0.6220 
2022-07-08 10:31:49.011352 - gail/main.py:201 - [Discriminator] iter = 70000 loss = -2.9881 grad_norm = 16.2178 grad_penalty = 0.6705 regularization = 0.0000 true_logits = 0.7608 fake_logits = -2.8978 true_prob = 0.6717 fake_prob = 0.0590 
