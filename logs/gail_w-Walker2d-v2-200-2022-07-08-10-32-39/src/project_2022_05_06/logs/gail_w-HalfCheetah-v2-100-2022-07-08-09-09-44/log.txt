2022-07-08 09:09:44.833612 - utils/flags.py:257 - log_dir = logs/gail_w-HalfCheetah-v2-100-2022-07-08-09-09-44
2022-07-08 09:10:16.271979 - gail/utils/replay_buffer.py:86 - Load dataset from /content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/HalfCheetah-v2
2022-07-08 09:10:45.722219 - gail/main.py:80 - Expert Reward 6594.485577
2022-07-08 09:10:47.110102 - gail/main.py:84 - Original dataset size 3000
2022-07-08 09:10:47.278065 - gail/main.py:86 - Subsampled dataset size 3000
2022-07-08 09:10:47.287407 - gail/main.py:87 - np random: 864 random : 786
2022-07-08 09:10:47.301140 - gail/main.py:91 - Sampled obs: 0.6422, acs: 0.2570
2022-07-08 09:10:51.183574 - lunzi/nn/flat_param.py:17 - Enabling flattening... ['GaussianMLPPolicy_1/log_std:0', 'GaussianMLPPolicy_1/Variable:0', 'GaussianMLPPolicy_1/Variable_1:0', 'GaussianMLPPolicy_1/Variable_2:0', 'GaussianMLPPolicy_1/Variable_3:0', 'GaussianMLPPolicy_1/Variable_4:0', 'GaussianMLPPolicy_1/Variable_5:0']
2022-07-08 09:11:33.844953 - gail/discriminator/discriminator.py:30 - Use predefined normalization.
2022-07-08 09:11:33.902910 - gail/discriminator/discriminator.py:33 - Normalizer loc:[[-1.0330443e-01 -9.3026832e-03  2.9791275e-01  8.1040427e-02
   1.7489190e-01  4.0859219e-01 -3.0232381e-02 -9.5055901e-02
   7.1118622e+00  5.5424552e-03 -1.8441556e-01  3.1117105e-01
  -2.7393317e-01  2.6658252e-01  4.0665963e-01 -2.8198040e-01
   2.5822452e-01]] 
 scale:[[ 0.03102476  0.07177044  0.37663096  0.5276757   0.46577376  0.12057849
   0.35446092  0.3551628   1.290817    0.4731574   1.4939526  12.314076
  14.637607   14.150712    3.6817648  11.124868    9.962843  ]]
2022-07-08 09:11:50.645644 - gail/discriminator/discriminator.py:67 - Discriminator uses Wasserstein distance.
2022-07-08 09:11:50.657456 - gail/discriminator/discriminator.py:68 - [<tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable:0' shape=(23, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_1:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_2:0' shape=(100, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_3:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_4:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_5:0' shape=(1,) dtype=float32_ref>]
2022-07-08 09:11:50.669636 - gail/discriminator/discriminator.py:69 - Use gradient penalty regularization (coef = 10.000000)
2022-07-08 09:11:54.790498 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions_mean]
2022-07-08 09:16:36.668418 - gail/main.py:142 - [Evaluate] iter = 0 episode={ returns = -0.4623 lengths = 1000 } discounted_episode={ returns = -0.4535 lengths = 1000 } 
2022-07-08 09:16:36.669242 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions]
2022-07-08 09:17:09.151493 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [fake_states fake_actions] => [scaled_neural_reward]
2022-07-08 09:17:10.310385 - lunzi/nn/module.py:71 - [MLPVFunction] is making TensorFlow callables, key = [states] => [values]
2022-07-08 09:17:12.242645 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [] => [sync_old]
2022-07-08 09:17:13.224660 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss flat_grad dist_std mean_kl dist_mean]
2022-07-08 09:17:19.284633 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states tangents actions] => [hessian_vec_prod]
2022-07-08 09:17:30.616150 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [] => [get_flat]
2022-07-08 09:17:31.598792 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [feed_flat] => [set_flat]
2022-07-08 09:17:32.644511 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss mean_kl]
2022-07-08 09:17:34.371826 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [train_vf vf_loss]
2022-07-08 09:17:37.337517 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [vf_loss]
2022-07-08 09:17:38.392060 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states actions_] => [mse_loss]
2022-07-08 09:17:39.358706 - gail/main.py:174 - [TRPO] iter = 1000 dist_mean = -0.0000 dist_std = 1.0000 vf_loss = 0.6155 grad_norm = 0.2855 nat_grad_norm = 0.5289 cg_residual = 0.0000 step_size = 0.4203 reward = 0.0000 fps = 2 mse_loss = 0.9499 
2022-07-08 09:18:02.281808 - gail/main.py:174 - [TRPO] iter = 2000 dist_mean = -0.0088 dist_std = 1.0052 vf_loss = 1.3476 grad_norm = 0.2954 nat_grad_norm = 0.5110 cg_residual = 0.0000 step_size = 0.4162 reward = 0.0000 fps = 2 mse_loss = 0.8927 
2022-07-08 09:18:23.993407 - gail/main.py:174 - [TRPO] iter = 3000 dist_mean = 0.0005 dist_std = 1.0109 vf_loss = 0.6947 grad_norm = 0.3094 nat_grad_norm = 0.5694 cg_residual = 0.0000 step_size = 0.3867 reward = 0.0000 fps = 2 mse_loss = 0.9085 
2022-07-08 09:18:46.200809 - gail/main.py:174 - [TRPO] iter = 4000 dist_mean = 0.0034 dist_std = 1.0135 vf_loss = 0.7183 grad_norm = 0.2627 nat_grad_norm = 0.4766 cg_residual = 0.0000 step_size = 0.4691 reward = -0.0000 fps = 2 mse_loss = 0.9739 
2022-07-08 09:19:09.340382 - gail/main.py:174 - [TRPO] iter = 5000 dist_mean = 0.0106 dist_std = 1.0138 vf_loss = 0.8461 grad_norm = 0.3189 nat_grad_norm = 0.5651 cg_residual = 0.0000 step_size = 0.3892 reward = -0.0000 fps = 2 mse_loss = 1.0079 
2022-07-08 09:19:09.342798 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [true_states true_actions fake_states fake_actions true_masks] => [train loss true_logits fake_logits true_prob fake_prob grad_norm grad_penalty regularization]
2022-07-08 09:19:18.157332 - gail/main.py:201 - [Discriminator] iter = 5000 loss = 0.3933 grad_norm = 38.9849 grad_penalty = 1.8469 regularization = 0.0000 true_logits = -0.0813 fake_logits = -1.5349 true_prob = 0.4797 fake_prob = 0.2288 
2022-07-08 09:23:54.916546 - gail/main.py:142 - [Evaluate] iter = 5000 episode={ returns = -4.0572 lengths = 1000 } discounted_episode={ returns = -2.6743 lengths = 1000 } 
2022-07-08 09:24:17.021942 - gail/main.py:174 - [TRPO] iter = 6000 dist_mean = 0.0169 dist_std = 1.0109 vf_loss = 1.0888 grad_norm = 0.3653 nat_grad_norm = 0.7862 cg_residual = 0.0000 step_size = 0.3024 reward = -0.0000 fps = 3 mse_loss = 0.9676 
2022-07-08 09:24:38.696934 - gail/main.py:174 - [TRPO] iter = 7000 dist_mean = 0.0231 dist_std = 1.0130 vf_loss = 3.8571 grad_norm = 0.2787 nat_grad_norm = 0.5615 cg_residual = 0.0001 step_size = 0.4153 reward = 0.0000 fps = 3 mse_loss = 1.0235 
2022-07-08 09:25:00.202079 - gail/main.py:174 - [TRPO] iter = 8000 dist_mean = 0.0182 dist_std = 1.0125 vf_loss = 0.9335 grad_norm = 0.4036 nat_grad_norm = 0.5168 cg_residual = 0.0001 step_size = 0.3964 reward = -0.0000 fps = 2 mse_loss = 1.0459 
2022-07-08 09:25:20.822491 - gail/main.py:174 - [TRPO] iter = 9000 dist_mean = 0.0192 dist_std = 1.0068 vf_loss = 1.6190 grad_norm = 0.3112 nat_grad_norm = 0.5929 cg_residual = 0.0000 step_size = 0.3840 reward = 0.0000 fps = 2 mse_loss = 1.0027 
2022-07-08 09:25:42.209911 - gail/main.py:174 - [TRPO] iter = 10000 dist_mean = 0.0214 dist_std = 1.0072 vf_loss = 3.6663 grad_norm = 0.3028 nat_grad_norm = 0.5128 cg_residual = 0.0000 step_size = 0.4310 reward = 0.0000 fps = 2 mse_loss = 0.9865 
2022-07-08 09:25:43.035936 - gail/main.py:201 - [Discriminator] iter = 10000 loss = -0.3619 grad_norm = 34.4331 grad_penalty = 1.9658 regularization = 0.0000 true_logits = -0.0334 fake_logits = -2.3611 true_prob = 0.4917 fake_prob = 0.1517 
2022-07-08 09:30:01.899068 - gail/main.py:142 - [Evaluate] iter = 10000 episode={ returns = -10.1127 lengths = 1000 } discounted_episode={ returns = -6.7216 lengths = 1000 } 
2022-07-08 09:30:24.241921 - gail/main.py:174 - [TRPO] iter = 11000 dist_mean = -0.0233 dist_std = 1.0061 vf_loss = 1.3460 grad_norm = 0.3120 nat_grad_norm = 0.5757 cg_residual = 0.0001 step_size = 0.3977 reward = -0.0000 fps = 3 mse_loss = 0.9906 
2022-07-08 09:30:44.910389 - gail/main.py:174 - [TRPO] iter = 12000 dist_mean = -0.0152 dist_std = 1.0105 vf_loss = 2.8153 grad_norm = 0.3072 nat_grad_norm = 0.4712 cg_residual = 0.0000 step_size = 0.4671 reward = -0.0000 fps = 3 mse_loss = 0.9661 
2022-07-08 09:31:06.489338 - gail/main.py:174 - [TRPO] iter = 13000 dist_mean = 0.0180 dist_std = 1.0171 vf_loss = 1.9099 grad_norm = 0.2819 nat_grad_norm = 0.5699 cg_residual = 0.0000 step_size = 0.4182 reward = -0.0000 fps = 3 mse_loss = 0.9853 
2022-07-08 09:31:27.403412 - gail/main.py:174 - [TRPO] iter = 14000 dist_mean = 0.0236 dist_std = 1.0202 vf_loss = 4.0901 grad_norm = 0.2811 nat_grad_norm = 0.5132 cg_residual = 0.0000 step_size = 0.4333 reward = -0.0000 fps = 2 mse_loss = 1.0281 
2022-07-08 09:31:49.190623 - gail/main.py:174 - [TRPO] iter = 15000 dist_mean = 0.0223 dist_std = 1.0182 vf_loss = 2.5975 grad_norm = 0.3162 nat_grad_norm = 0.6553 cg_residual = 0.0000 step_size = 0.3637 reward = -0.0000 fps = 2 mse_loss = 0.9883 
2022-07-08 09:31:49.960944 - gail/main.py:201 - [Discriminator] iter = 15000 loss = -2.5453 grad_norm = 22.1433 grad_penalty = 0.8238 regularization = 0.0000 true_logits = 0.0332 fake_logits = -3.3359 true_prob = 0.5083 fake_prob = 0.0509 
2022-07-08 09:36:05.076718 - gail/main.py:142 - [Evaluate] iter = 15000 episode={ returns = -10.6767 lengths = 1000 } discounted_episode={ returns = -6.7731 lengths = 1000 } 
2022-07-08 09:36:26.640127 - gail/main.py:174 - [TRPO] iter = 16000 dist_mean = 0.0285 dist_std = 1.0198 vf_loss = 2.7145 grad_norm = 0.3407 nat_grad_norm = 0.6946 cg_residual = 0.0000 step_size = 0.3396 reward = 0.0000 fps = 3 mse_loss = 0.9980 
2022-07-08 09:36:47.588677 - gail/main.py:174 - [TRPO] iter = 17000 dist_mean = 0.0648 dist_std = 1.0211 vf_loss = 4.6963 grad_norm = 0.2955 nat_grad_norm = 0.5285 cg_residual = 0.0000 step_size = 0.4272 reward = 0.0000 fps = 3 mse_loss = 0.9699 
2022-07-08 09:37:08.355492 - gail/main.py:174 - [TRPO] iter = 18000 dist_mean = 0.0423 dist_std = 1.0226 vf_loss = 6.8839 grad_norm = 0.2906 nat_grad_norm = 0.5204 cg_residual = 0.0000 step_size = 0.4403 reward = 0.0000 fps = 3 mse_loss = 1.0002 
2022-07-08 09:37:27.676890 - gail/main.py:174 - [TRPO] iter = 19000 dist_mean = 0.0432 dist_std = 1.0162 vf_loss = 6.2110 grad_norm = 0.2903 nat_grad_norm = 0.4909 cg_residual = 0.0000 step_size = 0.4459 reward = -0.0000 fps = 2 mse_loss = 1.0264 
2022-07-08 09:37:48.608554 - gail/main.py:174 - [TRPO] iter = 20000 dist_mean = 0.0545 dist_std = 1.0193 vf_loss = 4.9877 grad_norm = 0.2868 nat_grad_norm = 0.5528 cg_residual = 0.0000 step_size = 0.4364 reward = 0.0000 fps = 2 mse_loss = 1.0043 
2022-07-08 09:37:49.388305 - gail/main.py:201 - [Discriminator] iter = 20000 loss = 0.8056 grad_norm = 25.9049 grad_penalty = 1.5739 regularization = 0.0000 true_logits = 0.1060 fake_logits = -0.6623 true_prob = 0.5264 fake_prob = 0.3449 
2022-07-08 09:41:55.044398 - gail/main.py:142 - [Evaluate] iter = 20000 episode={ returns = -4.9802 lengths = 1000 } discounted_episode={ returns = -3.9296 lengths = 1000 } 
2022-07-08 09:42:22.155105 - gail/main.py:174 - [TRPO] iter = 21000 dist_mean = 0.0611 dist_std = 1.0222 vf_loss = 3.5499 grad_norm = 0.3057 nat_grad_norm = 0.4804 cg_residual = 0.0000 step_size = 0.4698 reward = 0.0000 fps = 3 mse_loss = 0.9638 
2022-07-08 09:42:41.840822 - gail/main.py:174 - [TRPO] iter = 22000 dist_mean = 0.0702 dist_std = 1.0230 vf_loss = 1.2693 grad_norm = 0.3479 nat_grad_norm = 0.5611 cg_residual = 0.0000 step_size = 0.4183 reward = -0.0000 fps = 3 mse_loss = 1.0051 
2022-07-08 09:43:01.182989 - gail/main.py:174 - [TRPO] iter = 23000 dist_mean = 0.0795 dist_std = 1.0216 vf_loss = 3.1766 grad_norm = 0.2692 nat_grad_norm = 0.4861 cg_residual = 0.0000 step_size = 0.4772 reward = 0.0000 fps = 3 mse_loss = 0.9742 
2022-07-08 09:43:19.779789 - gail/main.py:174 - [TRPO] iter = 24000 dist_mean = 0.1043 dist_std = 1.0218 vf_loss = 1.7529 grad_norm = 0.3681 nat_grad_norm = 0.5345 cg_residual = 0.0000 step_size = 0.4106 reward = -0.0000 fps = 3 mse_loss = 1.0032 
2022-07-08 09:43:39.196253 - gail/main.py:174 - [TRPO] iter = 25000 dist_mean = 0.0934 dist_std = 1.0229 vf_loss = 2.2879 grad_norm = 0.2642 nat_grad_norm = 0.5454 cg_residual = 0.0000 step_size = 0.4479 reward = -0.0000 fps = 2 mse_loss = 1.0384 
2022-07-08 09:43:40.007415 - gail/main.py:201 - [Discriminator] iter = 25000 loss = -2.9322 grad_norm = 25.2504 grad_penalty = 0.7118 regularization = 0.0000 true_logits = 0.1558 fake_logits = -3.4881 true_prob = 0.5388 fake_prob = 0.0518 
2022-07-08 09:47:36.188192 - gail/main.py:142 - [Evaluate] iter = 25000 episode={ returns = -19.0641 lengths = 1000 } discounted_episode={ returns = -12.3346 lengths = 1000 } 
2022-07-08 09:47:56.616494 - gail/main.py:174 - [TRPO] iter = 26000 dist_mean = 0.1373 dist_std = 1.0203 vf_loss = 3.3495 grad_norm = 0.3536 nat_grad_norm = 0.5319 cg_residual = 0.0001 step_size = 0.4062 reward = -0.0000 fps = 3 mse_loss = 1.0386 
2022-07-08 09:48:16.425082 - gail/main.py:174 - [TRPO] iter = 27000 dist_mean = 0.1020 dist_std = 1.0126 vf_loss = 2.3186 grad_norm = 0.3468 nat_grad_norm = 0.6294 cg_residual = 0.0000 step_size = 0.3678 reward = 0.0000 fps = 3 mse_loss = 1.0199 
2022-07-08 09:48:35.759880 - gail/main.py:174 - [TRPO] iter = 28000 dist_mean = 0.1620 dist_std = 1.0150 vf_loss = 5.5702 grad_norm = 0.3095 nat_grad_norm = 0.5669 cg_residual = 0.0001 step_size = 0.4194 reward = -0.0000 fps = 3 mse_loss = 1.0362 
2022-07-08 09:48:55.633596 - gail/main.py:174 - [TRPO] iter = 29000 dist_mean = 0.1456 dist_std = 1.0159 vf_loss = 2.4632 grad_norm = 0.3267 nat_grad_norm = 0.5690 cg_residual = 0.0000 step_size = 0.4081 reward = -0.0000 fps = 3 mse_loss = 1.1037 
2022-07-08 09:49:15.903337 - gail/main.py:174 - [TRPO] iter = 30000 dist_mean = 0.1499 dist_std = 1.0090 vf_loss = 1.1807 grad_norm = 0.3343 nat_grad_norm = 0.5407 cg_residual = 0.0000 step_size = 0.4143 reward = 0.0000 fps = 2 mse_loss = 1.0784 
2022-07-08 09:49:16.626900 - gail/main.py:201 - [Discriminator] iter = 30000 loss = -0.6850 grad_norm = 29.3487 grad_penalty = 1.2840 regularization = 0.0000 true_logits = 0.1955 fake_logits = -1.7735 true_prob = 0.5485 fake_prob = 0.1855 
2022-07-08 09:53:25.196938 - gail/main.py:142 - [Evaluate] iter = 30000 episode={ returns = -33.0873 lengths = 1000 } discounted_episode={ returns = -21.6265 lengths = 1000 } 
2022-07-08 09:53:46.490270 - gail/main.py:174 - [TRPO] iter = 31000 dist_mean = 0.1840 dist_std = 1.0074 vf_loss = 4.3252 grad_norm = 0.3763 nat_grad_norm = 0.5895 cg_residual = 0.0000 step_size = 0.3706 reward = 0.0000 fps = 3 mse_loss = 1.0875 
2022-07-08 09:54:07.524069 - gail/main.py:174 - [TRPO] iter = 32000 dist_mean = 0.2019 dist_std = 1.0050 vf_loss = 1.9712 grad_norm = 0.3304 nat_grad_norm = 0.5065 cg_residual = 0.0000 step_size = 0.4483 reward = 0.0000 fps = 3 mse_loss = 1.1095 
2022-07-08 09:54:28.479087 - gail/main.py:174 - [TRPO] iter = 33000 dist_mean = 0.2106 dist_std = 0.9996 vf_loss = 1.7871 grad_norm = 0.3570 nat_grad_norm = 0.5647 cg_residual = 0.0001 step_size = 0.3897 reward = 0.0000 fps = 3 mse_loss = 1.0805 
2022-07-08 09:54:49.176065 - gail/main.py:174 - [TRPO] iter = 34000 dist_mean = 0.2097 dist_std = 0.9953 vf_loss = 0.7664 grad_norm = 0.4186 nat_grad_norm = 0.5359 cg_residual = 0.0001 step_size = 0.3817 reward = 0.0000 fps = 3 mse_loss = 0.9805 
2022-07-08 09:55:09.236209 - gail/main.py:174 - [TRPO] iter = 35000 dist_mean = 0.2188 dist_std = 0.9960 vf_loss = 0.8671 grad_norm = 0.3630 nat_grad_norm = 0.5353 cg_residual = 0.0001 step_size = 0.4062 reward = -0.0000 fps = 2 mse_loss = 0.9529 
2022-07-08 09:55:10.002136 - gail/main.py:201 - [Discriminator] iter = 35000 loss = -0.2401 grad_norm = 15.9093 grad_penalty = 1.0870 regularization = 0.0000 true_logits = 0.2522 fake_logits = -1.0749 true_prob = 0.5623 fake_prob = 0.2610 
2022-07-08 09:58:56.726722 - gail/main.py:142 - [Evaluate] iter = 35000 episode={ returns = -40.0057 lengths = 1000 } discounted_episode={ returns = -25.7558 lengths = 1000 } 
2022-07-08 09:59:15.937695 - gail/main.py:174 - [TRPO] iter = 36000 dist_mean = 0.2294 dist_std = 0.9887 vf_loss = 1.2824 grad_norm = 0.3744 nat_grad_norm = 0.5772 cg_residual = 0.0001 step_size = 0.3803 reward = 0.0000 fps = 4 mse_loss = 0.9582 
2022-07-08 09:59:36.901533 - gail/main.py:174 - [TRPO] iter = 37000 dist_mean = 0.1818 dist_std = 0.9886 vf_loss = 1.4700 grad_norm = 0.3455 nat_grad_norm = 0.5389 cg_residual = 0.0000 step_size = 0.4118 reward = -0.0000 fps = 3 mse_loss = 0.9878 
2022-07-08 09:59:57.168239 - gail/main.py:174 - [TRPO] iter = 38000 dist_mean = 0.2313 dist_std = 0.9939 vf_loss = 1.7274 grad_norm = 0.3765 nat_grad_norm = 0.5776 cg_residual = 0.0001 step_size = 0.3851 reward = 0.0000 fps = 3 mse_loss = 0.9820 
2022-07-08 10:00:17.434608 - gail/main.py:174 - [TRPO] iter = 39000 dist_mean = 0.1931 dist_std = 0.9967 vf_loss = 2.8112 grad_norm = 0.3735 nat_grad_norm = 0.5898 cg_residual = 0.0001 step_size = 0.3735 reward = -0.0000 fps = 3 mse_loss = 0.9864 
2022-07-08 10:00:37.707873 - gail/main.py:174 - [TRPO] iter = 40000 dist_mean = 0.2457 dist_std = 0.9922 vf_loss = 1.2412 grad_norm = 0.3372 nat_grad_norm = 0.5323 cg_residual = 0.0001 step_size = 0.4345 reward = -0.0000 fps = 3 mse_loss = 1.0130 
2022-07-08 10:00:38.481887 - gail/main.py:201 - [Discriminator] iter = 40000 loss = -0.6912 grad_norm = 15.3772 grad_penalty = 0.9456 regularization = 0.0000 true_logits = 0.3220 fake_logits = -1.3148 true_prob = 0.5790 fake_prob = 0.2198 
2022-07-08 10:05:11.707922 - gail/main.py:142 - [Evaluate] iter = 40000 episode={ returns = -41.6862 lengths = 1000 } discounted_episode={ returns = -27.6150 lengths = 1000 } 
2022-07-08 10:05:35.216403 - gail/main.py:174 - [TRPO] iter = 41000 dist_mean = 0.2145 dist_std = 0.9847 vf_loss = 4.4396 grad_norm = 0.3390 nat_grad_norm = 0.5778 cg_residual = 0.0000 step_size = 0.4000 reward = -0.0000 fps = 3 mse_loss = 0.9533 
2022-07-08 10:05:57.998857 - gail/main.py:174 - [TRPO] iter = 42000 dist_mean = 0.2434 dist_std = 0.9871 vf_loss = 1.1560 grad_norm = 0.3954 nat_grad_norm = 0.5742 cg_residual = 0.0001 step_size = 0.3898 reward = 0.0000 fps = 3 mse_loss = 0.9623 
2022-07-08 10:06:21.511713 - gail/main.py:174 - [TRPO] iter = 43000 dist_mean = 0.2592 dist_std = 0.9898 vf_loss = 0.9575 grad_norm = 0.3553 nat_grad_norm = 0.5648 cg_residual = 0.0001 step_size = 0.4177 reward = 0.0000 fps = 2 mse_loss = 0.9173 
2022-07-08 10:06:44.165783 - gail/main.py:174 - [TRPO] iter = 44000 dist_mean = 0.2566 dist_std = 0.9876 vf_loss = 1.0315 grad_norm = 0.3742 nat_grad_norm = 0.5285 cg_residual = 0.0001 step_size = 0.4225 reward = -0.0000 fps = 2 mse_loss = 0.8705 
2022-07-08 10:07:07.254674 - gail/main.py:174 - [TRPO] iter = 45000 dist_mean = 0.2648 dist_std = 0.9785 vf_loss = 0.5719 grad_norm = 0.3688 nat_grad_norm = 0.5008 cg_residual = 0.0001 step_size = 0.4497 reward = 0.0000 fps = 2 mse_loss = 0.8755 
2022-07-08 10:07:08.091099 - gail/main.py:201 - [Discriminator] iter = 45000 loss = -0.9098 grad_norm = 14.3278 grad_penalty = 0.8616 regularization = 0.0000 true_logits = 0.3857 fake_logits = -1.3857 true_prob = 0.5939 fake_prob = 0.2081 
2022-07-08 10:11:40.529054 - gail/main.py:142 - [Evaluate] iter = 45000 episode={ returns = -67.8794 lengths = 1000 } discounted_episode={ returns = -43.3818 lengths = 1000 } 
2022-07-08 10:12:09.978813 - gail/main.py:174 - [TRPO] iter = 46000 dist_mean = 0.2727 dist_std = 0.9836 vf_loss = 0.7996 grad_norm = 0.4401 nat_grad_norm = 0.5868 cg_residual = 0.0002 step_size = 0.3582 reward = 0.0000 fps = 3 mse_loss = 0.8581 
2022-07-08 10:12:33.578858 - gail/main.py:174 - [TRPO] iter = 47000 dist_mean = 0.2869 dist_std = 0.9855 vf_loss = 0.7223 grad_norm = 0.4446 nat_grad_norm = 0.5807 cg_residual = 0.0001 step_size = 0.3743 reward = -0.0000 fps = 3 mse_loss = 0.8115 
2022-07-08 10:12:56.935356 - gail/main.py:174 - [TRPO] iter = 48000 dist_mean = 0.2281 dist_std = 0.9811 vf_loss = 3.9300 grad_norm = 0.3458 nat_grad_norm = 0.5534 cg_residual = 0.0001 step_size = 0.4267 reward = -0.0000 fps = 2 mse_loss = 0.8097 
2022-07-08 10:13:18.449553 - gail/main.py:174 - [TRPO] iter = 49000 dist_mean = 0.3007 dist_std = 0.9816 vf_loss = 1.0749 grad_norm = 0.3848 nat_grad_norm = 0.6540 cg_residual = 0.0002 step_size = 0.3836 reward = -0.0000 fps = 2 mse_loss = 0.8127 
2022-07-08 10:13:40.361136 - gail/main.py:174 - [TRPO] iter = 50000 dist_mean = 0.2519 dist_std = 0.9831 vf_loss = 4.8162 grad_norm = 0.3937 nat_grad_norm = 0.6848 cg_residual = 0.0001 step_size = 0.3581 reward = 0.0000 fps = 2 mse_loss = 0.8065 
2022-07-08 10:13:41.211896 - gail/main.py:201 - [Discriminator] iter = 50000 loss = -1.7827 grad_norm = 16.4631 grad_penalty = 0.9549 regularization = 0.0000 true_logits = 0.4618 fake_logits = -2.2758 true_prob = 0.6113 fake_prob = 0.1314 
2022-07-08 10:18:09.121074 - gail/main.py:142 - [Evaluate] iter = 50000 episode={ returns = -80.5169 lengths = 1000 } discounted_episode={ returns = -52.2835 lengths = 1000 } 
2022-07-08 10:18:30.768077 - gail/main.py:174 - [TRPO] iter = 51000 dist_mean = 0.2989 dist_std = 0.9809 vf_loss = 0.5899 grad_norm = 0.3864 nat_grad_norm = 0.5476 cg_residual = 0.0002 step_size = 0.4167 reward = 0.0000 fps = 3 mse_loss = 0.8329 
2022-07-08 10:18:52.678172 - gail/main.py:174 - [TRPO] iter = 52000 dist_mean = 0.3184 dist_std = 0.9788 vf_loss = 0.7270 grad_norm = 0.4184 nat_grad_norm = 0.6256 cg_residual = 0.0002 step_size = 0.3744 reward = 0.0000 fps = 3 mse_loss = 0.8409 
2022-07-08 10:19:13.775273 - gail/main.py:174 - [TRPO] iter = 53000 dist_mean = 0.3185 dist_std = 0.9784 vf_loss = 0.5538 grad_norm = 0.4103 nat_grad_norm = 0.5768 cg_residual = 0.0003 step_size = 0.3961 reward = -0.0000 fps = 3 mse_loss = 0.8436 
2022-07-08 10:19:36.844282 - gail/main.py:174 - [TRPO] iter = 54000 dist_mean = 0.3196 dist_std = 0.9759 vf_loss = 0.6255 grad_norm = 0.4390 nat_grad_norm = 0.6630 cg_residual = 0.0003 step_size = 0.3564 reward = 0.0000 fps = 2 mse_loss = 0.7917 
2022-07-08 10:19:58.747245 - gail/main.py:174 - [TRPO] iter = 55000 dist_mean = 0.3331 dist_std = 0.9718 vf_loss = 0.5712 grad_norm = 0.3950 nat_grad_norm = 0.6249 cg_residual = 0.0003 step_size = 0.3757 reward = -0.0000 fps = 2 mse_loss = 0.7591 
2022-07-08 10:19:59.541216 - gail/main.py:201 - [Discriminator] iter = 55000 loss = -1.5867 grad_norm = 13.2073 grad_penalty = 0.7605 regularization = 0.0000 true_logits = 0.5543 fake_logits = -1.7930 true_prob = 0.6313 fake_prob = 0.1512 
2022-07-08 10:24:27.337602 - gail/main.py:142 - [Evaluate] iter = 55000 episode={ returns = -109.6824 lengths = 1000 } discounted_episode={ returns = -70.0702 lengths = 1000 } 
2022-07-08 10:24:50.616389 - gail/main.py:174 - [TRPO] iter = 56000 dist_mean = 0.3424 dist_std = 0.9687 vf_loss = 0.4623 grad_norm = 0.4570 nat_grad_norm = 0.5961 cg_residual = 0.0003 step_size = 0.3760 reward = -0.0000 fps = 3 mse_loss = 0.7392 
2022-07-08 10:25:12.341150 - gail/main.py:174 - [TRPO] iter = 57000 dist_mean = 0.3395 dist_std = 0.9645 vf_loss = 0.3427 grad_norm = 0.3936 nat_grad_norm = 0.6016 cg_residual = 0.0002 step_size = 0.3902 reward = 0.0000 fps = 3 mse_loss = 0.6862 
2022-07-08 10:25:34.887869 - gail/main.py:174 - [TRPO] iter = 58000 dist_mean = 0.3570 dist_std = 0.9632 vf_loss = 0.4734 grad_norm = 0.5798 nat_grad_norm = 0.5998 cg_residual = 0.0003 step_size = 0.3433 reward = 0.0000 fps = 2 mse_loss = 0.7088 
2022-07-08 10:25:56.715296 - gail/main.py:174 - [TRPO] iter = 59000 dist_mean = 0.3552 dist_std = 0.9646 vf_loss = 0.3398 grad_norm = 0.3908 nat_grad_norm = 0.5594 cg_residual = 0.0003 step_size = 0.4167 reward = -0.0000 fps = 2 mse_loss = 0.6759 
2022-07-08 10:26:19.774207 - gail/main.py:174 - [TRPO] iter = 60000 dist_mean = 0.3589 dist_std = 0.9545 vf_loss = 0.2731 grad_norm = 0.4412 nat_grad_norm = 0.6127 cg_residual = 0.0007 step_size = 0.3868 reward = -0.0000 fps = 2 mse_loss = 0.6614 
2022-07-08 10:26:20.628154 - gail/main.py:201 - [Discriminator] iter = 60000 loss = -1.9706 grad_norm = 13.4872 grad_penalty = 0.7744 regularization = 0.0000 true_logits = 0.6622 fake_logits = -2.0828 true_prob = 0.6543 fake_prob = 0.1186 
2022-07-08 10:31:09.064079 - gail/main.py:142 - [Evaluate] iter = 60000 episode={ returns = -131.2378 lengths = 1000 } discounted_episode={ returns = -82.9888 lengths = 1000 } 
2022-07-08 10:31:38.885465 - gail/main.py:174 - [TRPO] iter = 61000 dist_mean = 0.3627 dist_std = 0.9549 vf_loss = 0.3436 grad_norm = 0.4162 nat_grad_norm = 0.5845 cg_residual = 0.0004 step_size = 0.3939 reward = 0.0000 fps = 3 mse_loss = 0.6805 
2022-07-08 10:32:01.323218 - gail/main.py:174 - [TRPO] iter = 62000 dist_mean = 0.3645 dist_std = 0.9565 vf_loss = 0.4182 grad_norm = 0.4624 nat_grad_norm = 0.6106 cg_residual = 0.0005 step_size = 0.3690 reward = -0.0000 fps = 2 mse_loss = 0.6549 
