2022-07-08 09:09:38.123735 - utils/flags.py:257 - log_dir = logs/gail_w-Walker2d-v2-200-2022-07-08-09-09-37
2022-07-08 09:10:04.209203 - gail/utils/replay_buffer.py:86 - Load dataset from /content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/Walker2d-v2
2022-07-08 09:10:34.418918 - gail/main.py:80 - Expert Reward 5150.674112
2022-07-08 09:10:35.802622 - gail/main.py:84 - Original dataset size 3000
2022-07-08 09:10:35.924614 - gail/main.py:86 - Subsampled dataset size 3000
2022-07-08 09:10:36.004344 - gail/main.py:87 - np random: 864 random : 786
2022-07-08 09:10:36.021377 - gail/main.py:91 - Sampled obs: 0.0531, acs: 0.2269
2022-07-08 09:10:40.152380 - lunzi/nn/flat_param.py:17 - Enabling flattening... ['GaussianMLPPolicy_1/log_std:0', 'GaussianMLPPolicy_1/Variable:0', 'GaussianMLPPolicy_1/Variable_1:0', 'GaussianMLPPolicy_1/Variable_2:0', 'GaussianMLPPolicy_1/Variable_3:0', 'GaussianMLPPolicy_1/Variable_4:0', 'GaussianMLPPolicy_1/Variable_5:0']
2022-07-08 09:11:22.471681 - gail/discriminator/discriminator.py:30 - Use predefined normalization.
2022-07-08 09:11:22.506058 - gail/discriminator/discriminator.py:33 - Normalizer loc:[[ 1.2194959e+00  2.4445076e-01 -7.8132987e-02 -2.6673764e-01
   1.8222688e-01 -9.5077172e-02 -3.3649772e-01  5.3370733e-02
   4.1614923e+00  4.1431887e-03  3.8142569e-02 -2.6013174e-03
  -1.0202496e-02  5.6982285e-01  2.9836079e-02 -1.5763690e-01
   1.7689442e-02]] 
 scale:[[0.06687175 0.23681822 0.23042987 0.33821535 0.664349   0.20301929
  0.42807332 0.7138035  0.986894   0.65049744 2.0363257  2.3816926
  3.7250905  6.026913   2.0511289  4.406521   6.1475325 ]]
2022-07-08 09:11:40.289064 - gail/discriminator/discriminator.py:67 - Discriminator uses Wasserstein distance.
2022-07-08 09:11:40.310395 - gail/discriminator/discriminator.py:68 - [<tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable:0' shape=(23, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_1:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_2:0' shape=(100, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_3:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_4:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_5:0' shape=(1,) dtype=float32_ref>]
2022-07-08 09:11:40.312755 - gail/discriminator/discriminator.py:69 - Use gradient penalty regularization (coef = 10.000000)
2022-07-08 09:11:44.075690 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions_mean]
2022-07-08 09:12:49.736768 - gail/main.py:142 - [Evaluate] iter = 0 episode={ returns = 113.4058 lengths = 135 } discounted_episode={ returns = 135.8490 lengths = 156 } 
2022-07-08 09:12:49.765831 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions]
2022-07-08 09:13:30.215249 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [fake_states fake_actions] => [scaled_neural_reward]
2022-07-08 09:13:31.437735 - lunzi/nn/module.py:71 - [MLPVFunction] is making TensorFlow callables, key = [states] => [values]
2022-07-08 09:13:33.571123 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [] => [sync_old]
2022-07-08 09:13:34.377273 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss flat_grad dist_std mean_kl dist_mean]
2022-07-08 09:13:40.731113 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states tangents actions] => [hessian_vec_prod]
2022-07-08 09:13:51.778438 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [] => [get_flat]
2022-07-08 09:13:52.756950 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [feed_flat] => [set_flat]
2022-07-08 09:13:53.806073 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss mean_kl]
2022-07-08 09:13:55.598280 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [train_vf vf_loss]
2022-07-08 09:13:58.514830 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [vf_loss]
2022-07-08 09:13:59.565768 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states actions_] => [mse_loss]
2022-07-08 09:14:00.583052 - gail/main.py:174 - [TRPO] iter = 1000 dist_mean = 0.0000 dist_std = 1.0000 vf_loss = 0.2127 grad_norm = 0.3176 nat_grad_norm = 0.3986 cg_residual = 0.0000 step_size = 0.5038 reward = 0.0000 fps = 7 mse_loss = 0.4380 
2022-07-08 09:14:30.545443 - gail/main.py:174 - [TRPO] iter = 2000 dist_mean = -0.0148 dist_std = 1.0010 vf_loss = 0.3019 grad_norm = 0.5345 nat_grad_norm = 0.4888 cg_residual = 0.0000 step_size = 0.3417 reward = -0.0000 fps = 6 mse_loss = 0.4759 
2022-07-08 09:15:02.890799 - gail/main.py:174 - [TRPO] iter = 3000 dist_mean = -0.0348 dist_std = 1.0057 vf_loss = 0.1878 grad_norm = 0.5650 nat_grad_norm = 0.4909 cg_residual = 0.0000 step_size = 0.3328 reward = -0.0000 fps = 5 mse_loss = 0.5481 
2022-07-08 09:15:33.109845 - gail/main.py:174 - [TRPO] iter = 4000 dist_mean = -0.0429 dist_std = 1.0060 vf_loss = 0.1870 grad_norm = 0.4669 nat_grad_norm = 0.5075 cg_residual = 0.0000 step_size = 0.3714 reward = 0.0000 fps = 4 mse_loss = 0.6285 
2022-07-08 09:16:04.211334 - gail/main.py:174 - [TRPO] iter = 5000 dist_mean = -0.0506 dist_std = 1.0078 vf_loss = 0.1613 grad_norm = 0.5281 nat_grad_norm = 0.4704 cg_residual = 0.0001 step_size = 0.3728 reward = -0.0000 fps = 3 mse_loss = 0.6875 
2022-07-08 09:16:04.212586 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [true_states true_actions fake_states fake_actions true_masks] => [train loss true_logits fake_logits true_prob fake_prob grad_norm grad_penalty regularization]
2022-07-08 09:16:12.977335 - gail/main.py:201 - [Discriminator] iter = 5000 loss = 1.4121 grad_norm = 9.8276 grad_penalty = 1.0169 regularization = 0.0000 true_logits = 0.3014 fake_logits = 0.6966 true_prob = 0.5739 fake_prob = 0.6630 
2022-07-08 09:16:23.434075 - gail/main.py:142 - [Evaluate] iter = 5000 episode={ returns = 0.5522 lengths = 26 } discounted_episode={ returns = 0.2826 lengths = 26 } 
2022-07-08 09:16:54.088848 - gail/main.py:174 - [TRPO] iter = 6000 dist_mean = -0.0595 dist_std = 1.0085 vf_loss = 0.3100 grad_norm = 0.4358 nat_grad_norm = 0.4758 cg_residual = 0.0001 step_size = 0.4007 reward = 0.0000 fps = 24 mse_loss = 0.6795 
2022-07-08 09:17:23.798819 - gail/main.py:174 - [TRPO] iter = 7000 dist_mean = -0.0571 dist_std = 1.0104 vf_loss = 0.4002 grad_norm = 0.4682 nat_grad_norm = 0.4719 cg_residual = 0.0001 step_size = 0.3677 reward = -0.0000 fps = 14 mse_loss = 0.6821 
2022-07-08 09:17:54.125830 - gail/main.py:174 - [TRPO] iter = 8000 dist_mean = -0.0578 dist_std = 1.0098 vf_loss = 0.3454 grad_norm = 0.4779 nat_grad_norm = 0.4845 cg_residual = 0.0002 step_size = 0.3702 reward = 0.0000 fps = 9 mse_loss = 0.6815 
2022-07-08 09:18:22.678331 - gail/main.py:174 - [TRPO] iter = 9000 dist_mean = -0.0432 dist_std = 1.0079 vf_loss = 0.3692 grad_norm = 0.4299 nat_grad_norm = 0.4747 cg_residual = 0.0002 step_size = 0.4066 reward = 0.0000 fps = 7 mse_loss = 0.6830 
2022-07-08 09:18:52.436819 - gail/main.py:174 - [TRPO] iter = 10000 dist_mean = -0.0477 dist_std = 1.0020 vf_loss = 0.3104 grad_norm = 0.5218 nat_grad_norm = 0.4771 cg_residual = 0.0001 step_size = 0.3738 reward = -0.0000 fps = 6 mse_loss = 0.7671 
2022-07-08 09:18:53.206122 - gail/main.py:201 - [Discriminator] iter = 10000 loss = 0.7476 grad_norm = 7.4934 grad_penalty = 0.6376 regularization = 0.0000 true_logits = 0.3380 fake_logits = 0.4480 true_prob = 0.5823 fake_prob = 0.6059 
2022-07-08 09:19:03.172085 - gail/main.py:142 - [Evaluate] iter = 10000 episode={ returns = -0.5345 lengths = 24 } discounted_episode={ returns = -0.7446 lengths = 24 } 
2022-07-08 09:19:32.883263 - gail/main.py:174 - [TRPO] iter = 11000 dist_mean = -0.0247 dist_std = 1.0060 vf_loss = 0.6413 grad_norm = 0.4589 nat_grad_norm = 0.5094 cg_residual = 0.0001 step_size = 0.3757 reward = -0.0000 fps = 25 mse_loss = 0.7846 
2022-07-08 09:20:02.708621 - gail/main.py:174 - [TRPO] iter = 12000 dist_mean = -0.0201 dist_std = 1.0071 vf_loss = 0.6034 grad_norm = 0.4672 nat_grad_norm = 0.5023 cg_residual = 0.0002 step_size = 0.3789 reward = -0.0000 fps = 14 mse_loss = 0.8191 
2022-07-08 09:20:33.400471 - gail/main.py:174 - [TRPO] iter = 13000 dist_mean = -0.0128 dist_std = 1.0010 vf_loss = 0.4867 grad_norm = 0.4440 nat_grad_norm = 0.5169 cg_residual = 0.0001 step_size = 0.3828 reward = -0.0000 fps = 9 mse_loss = 0.8075 
2022-07-08 09:21:02.254136 - gail/main.py:174 - [TRPO] iter = 14000 dist_mean = 0.0193 dist_std = 0.9993 vf_loss = 0.5215 grad_norm = 0.4364 nat_grad_norm = 0.5285 cg_residual = 0.0002 step_size = 0.4072 reward = 0.0000 fps = 7 mse_loss = 0.7674 
2022-07-08 09:21:29.830459 - gail/main.py:174 - [TRPO] iter = 15000 dist_mean = 0.0343 dist_std = 0.9990 vf_loss = 0.4964 grad_norm = 0.3391 nat_grad_norm = 0.6331 cg_residual = 0.0004 step_size = 0.3883 reward = 0.0000 fps = 6 mse_loss = 0.7814 
2022-07-08 09:21:30.637920 - gail/main.py:201 - [Discriminator] iter = 15000 loss = 0.4732 grad_norm = 6.6826 grad_penalty = 0.5586 regularization = 0.0000 true_logits = 0.3724 fake_logits = 0.2870 true_prob = 0.5901 fake_prob = 0.5679 
2022-07-08 09:21:56.169360 - gail/main.py:142 - [Evaluate] iter = 15000 episode={ returns = -6.3059 lengths = 53 } discounted_episode={ returns = -6.1579 lengths = 53 } 
2022-07-08 09:22:26.740047 - gail/main.py:174 - [TRPO] iter = 16000 dist_mean = 0.0207 dist_std = 0.9953 vf_loss = 0.5158 grad_norm = 0.4636 nat_grad_norm = 0.6354 cg_residual = 0.0003 step_size = 0.3420 reward = 0.0000 fps = 17 mse_loss = 0.8440 
2022-07-08 09:22:56.080743 - gail/main.py:174 - [TRPO] iter = 17000 dist_mean = 0.0293 dist_std = 0.9932 vf_loss = 0.4927 grad_norm = 0.4326 nat_grad_norm = 0.5414 cg_residual = 0.0003 step_size = 0.4098 reward = 0.0000 fps = 11 mse_loss = 0.8344 
2022-07-08 09:23:26.341061 - gail/main.py:174 - [TRPO] iter = 18000 dist_mean = 0.0529 dist_std = 0.9890 vf_loss = 0.5382 grad_norm = 0.4347 nat_grad_norm = 0.5970 cg_residual = 0.0004 step_size = 0.3815 reward = -0.0000 fps = 8 mse_loss = 0.8124 
2022-07-08 09:23:54.868858 - gail/main.py:174 - [TRPO] iter = 19000 dist_mean = 0.0611 dist_std = 0.9899 vf_loss = 0.3538 grad_norm = 0.3922 nat_grad_norm = 0.5491 cg_residual = 0.0003 step_size = 0.4062 reward = -0.0000 fps = 6 mse_loss = 0.8226 
2022-07-08 09:24:23.507127 - gail/main.py:174 - [TRPO] iter = 20000 dist_mean = 0.0638 dist_std = 0.9921 vf_loss = 0.3143 grad_norm = 0.4093 nat_grad_norm = 0.6924 cg_residual = 0.0009 step_size = 0.3587 reward = 0.0000 fps = 5 mse_loss = 0.8542 
2022-07-08 09:24:24.236208 - gail/main.py:201 - [Discriminator] iter = 20000 loss = -0.0558 grad_norm = 6.1154 grad_penalty = 0.4229 regularization = 0.0000 true_logits = 0.4090 fake_logits = -0.0698 true_prob = 0.5979 fake_prob = 0.4851 
2022-07-08 09:24:40.498073 - gail/main.py:142 - [Evaluate] iter = 20000 episode={ returns = -4.0447 lengths = 45 } discounted_episode={ returns = -4.3344 lengths = 46 } 
2022-07-08 09:25:07.677058 - gail/main.py:174 - [TRPO] iter = 21000 dist_mean = 0.0607 dist_std = 0.9961 vf_loss = 0.5308 grad_norm = 0.3524 nat_grad_norm = 0.5610 cg_residual = 0.0004 step_size = 0.4229 reward = 0.0000 fps = 23 mse_loss = 0.8047 
2022-07-08 09:25:35.622288 - gail/main.py:174 - [TRPO] iter = 22000 dist_mean = 0.0775 dist_std = 0.9976 vf_loss = 0.3949 grad_norm = 0.3932 nat_grad_norm = 0.8176 cg_residual = 0.0008 step_size = 0.3378 reward = -0.0000 fps = 14 mse_loss = 0.8143 
2022-07-08 09:26:04.757709 - gail/main.py:174 - [TRPO] iter = 23000 dist_mean = 0.0764 dist_std = 0.9965 vf_loss = 0.3121 grad_norm = 0.4510 nat_grad_norm = 0.6182 cg_residual = 0.0007 step_size = 0.3569 reward = -0.0000 fps = 9 mse_loss = 0.7806 
2022-07-08 09:26:34.231713 - gail/main.py:174 - [TRPO] iter = 24000 dist_mean = 0.0641 dist_std = 1.0050 vf_loss = 0.2684 grad_norm = 0.3781 nat_grad_norm = 0.5631 cg_residual = 0.0005 step_size = 0.4362 reward = 0.0000 fps = 7 mse_loss = 0.7748 
2022-07-08 09:27:02.136753 - gail/main.py:174 - [TRPO] iter = 25000 dist_mean = 0.0575 dist_std = 1.0038 vf_loss = 0.3993 grad_norm = 0.4084 nat_grad_norm = 0.5325 cg_residual = 0.0003 step_size = 0.4159 reward = 0.0000 fps = 6 mse_loss = 0.7954 
2022-07-08 09:27:02.961260 - gail/main.py:201 - [Discriminator] iter = 25000 loss = -0.6814 grad_norm = 6.0971 grad_penalty = 0.4262 regularization = 0.0000 true_logits = 0.4051 fake_logits = -0.7025 true_prob = 0.5973 fake_prob = 0.3458 
2022-07-08 09:27:19.099967 - gail/main.py:142 - [Evaluate] iter = 25000 episode={ returns = -4.1848 lengths = 46 } discounted_episode={ returns = -4.0676 lengths = 46 } 
2022-07-08 09:27:46.687272 - gail/main.py:174 - [TRPO] iter = 26000 dist_mean = 0.0708 dist_std = 0.9952 vf_loss = 0.2992 grad_norm = 0.3350 nat_grad_norm = 0.5684 cg_residual = 0.0006 step_size = 0.4386 reward = -0.0000 fps = 22 mse_loss = 0.8095 
2022-07-08 09:28:13.676359 - gail/main.py:174 - [TRPO] iter = 27000 dist_mean = 0.0808 dist_std = 0.9876 vf_loss = 0.6416 grad_norm = 0.5263 nat_grad_norm = 0.6126 cg_residual = 0.0004 step_size = 0.3557 reward = 0.0000 fps = 14 mse_loss = 0.8210 
2022-07-08 09:28:41.165675 - gail/main.py:174 - [TRPO] iter = 28000 dist_mean = 0.0750 dist_std = 0.9846 vf_loss = 0.4300 grad_norm = 0.4662 nat_grad_norm = 0.5532 cg_residual = 0.0005 step_size = 0.3959 reward = 0.0000 fps = 10 mse_loss = 0.8766 
2022-07-08 09:29:09.111371 - gail/main.py:174 - [TRPO] iter = 29000 dist_mean = 0.0496 dist_std = 0.9820 vf_loss = 0.2634 grad_norm = 0.3508 nat_grad_norm = 0.5377 cg_residual = 0.0004 step_size = 0.4324 reward = -0.0000 fps = 7 mse_loss = 0.8969 
2022-07-08 09:29:36.635171 - gail/main.py:174 - [TRPO] iter = 30000 dist_mean = 0.0837 dist_std = 0.9816 vf_loss = 0.6232 grad_norm = 0.4092 nat_grad_norm = 0.5376 cg_residual = 0.0006 step_size = 0.4242 reward = -0.0000 fps = 6 mse_loss = 0.9464 
2022-07-08 09:29:37.400328 - gail/main.py:201 - [Discriminator] iter = 30000 loss = -1.2253 grad_norm = 6.1716 grad_penalty = 0.3971 regularization = 0.0000 true_logits = 0.4149 fake_logits = -1.2074 true_prob = 0.5997 fake_prob = 0.2488 
2022-07-08 09:29:53.231057 - gail/main.py:142 - [Evaluate] iter = 30000 episode={ returns = -5.3251 lengths = 43 } discounted_episode={ returns = -4.8255 lengths = 43 } 
2022-07-08 09:30:20.821749 - gail/main.py:174 - [TRPO] iter = 31000 dist_mean = 0.0808 dist_std = 0.9832 vf_loss = 0.4438 grad_norm = 0.4137 nat_grad_norm = 0.6252 cg_residual = 0.0008 step_size = 0.3814 reward = -0.0000 fps = 23 mse_loss = 1.0183 
2022-07-08 09:30:47.130113 - gail/main.py:174 - [TRPO] iter = 32000 dist_mean = 0.0765 dist_std = 0.9853 vf_loss = 1.2024 grad_norm = 0.4307 nat_grad_norm = 0.6430 cg_residual = 0.0011 step_size = 0.3620 reward = -0.0000 fps = 14 mse_loss = 0.9503 
2022-07-08 09:31:14.883782 - gail/main.py:174 - [TRPO] iter = 33000 dist_mean = 0.0851 dist_std = 0.9777 vf_loss = 0.3926 grad_norm = 0.3696 nat_grad_norm = 0.5513 cg_residual = 0.0006 step_size = 0.4381 reward = -0.0000 fps = 10 mse_loss = 0.9473 
2022-07-08 09:31:41.972797 - gail/main.py:174 - [TRPO] iter = 34000 dist_mean = 0.0902 dist_std = 0.9760 vf_loss = 0.7552 grad_norm = 0.4280 nat_grad_norm = 0.5816 cg_residual = 0.0012 step_size = 0.3750 reward = 0.0000 fps = 8 mse_loss = 1.0080 
2022-07-08 09:32:11.109123 - gail/main.py:174 - [TRPO] iter = 35000 dist_mean = 0.0716 dist_std = 0.9777 vf_loss = 0.5485 grad_norm = 0.3812 nat_grad_norm = 0.6887 cg_residual = 0.0008 step_size = 0.3734 reward = -0.0000 fps = 6 mse_loss = 0.9314 
2022-07-08 09:32:12.434256 - gail/main.py:201 - [Discriminator] iter = 35000 loss = -1.9746 grad_norm = 6.8495 grad_penalty = 0.4146 regularization = 0.0000 true_logits = 0.4951 fake_logits = -1.8941 true_prob = 0.6183 fake_prob = 0.1569 
2022-07-08 09:32:43.450853 - gail/main.py:142 - [Evaluate] iter = 35000 episode={ returns = 1.0737 lengths = 75 } discounted_episode={ returns = 0.4192 lengths = 76 } 
2022-07-08 09:33:09.599701 - gail/main.py:174 - [TRPO] iter = 36000 dist_mean = 0.0841 dist_std = 0.9776 vf_loss = 0.5749 grad_norm = 0.4295 nat_grad_norm = 0.5811 cg_residual = 0.0007 step_size = 0.3955 reward = -0.0000 fps = 17 mse_loss = 0.9696 
2022-07-08 09:33:36.143909 - gail/main.py:174 - [TRPO] iter = 37000 dist_mean = 0.0804 dist_std = 0.9828 vf_loss = 0.4274 grad_norm = 0.3948 nat_grad_norm = 0.5591 cg_residual = 0.0012 step_size = 0.4050 reward = -0.0000 fps = 11 mse_loss = 1.0068 
2022-07-08 09:34:02.800312 - gail/main.py:174 - [TRPO] iter = 38000 dist_mean = 0.1322 dist_std = 0.9731 vf_loss = 0.9000 grad_norm = 0.3682 nat_grad_norm = 0.5828 cg_residual = 0.0012 step_size = 0.4010 reward = -0.0000 fps = 9 mse_loss = 0.9440 
2022-07-08 09:34:29.091277 - gail/main.py:174 - [TRPO] iter = 39000 dist_mean = 0.1124 dist_std = 0.9731 vf_loss = 0.7508 grad_norm = 0.3424 nat_grad_norm = 0.6417 cg_residual = 0.0012 step_size = 0.4292 reward = -0.0000 fps = 7 mse_loss = 0.9167 
2022-07-08 09:34:55.905954 - gail/main.py:174 - [TRPO] iter = 40000 dist_mean = 0.0933 dist_std = 0.9726 vf_loss = 0.4449 grad_norm = 0.4652 nat_grad_norm = 0.6734 cg_residual = 0.0010 step_size = 0.3512 reward = -0.0000 fps = 6 mse_loss = 0.8471 
2022-07-08 09:34:56.718969 - gail/main.py:201 - [Discriminator] iter = 40000 loss = -2.5910 grad_norm = 7.0262 grad_penalty = 0.4180 regularization = 0.0000 true_logits = 0.5172 fake_logits = -2.4918 true_prob = 0.6239 fake_prob = 0.1018 
2022-07-08 09:35:35.778907 - gail/main.py:142 - [Evaluate] iter = 40000 episode={ returns = 13.6172 lengths = 103 } discounted_episode={ returns = 10.1206 lengths = 116 } 
2022-07-08 09:36:02.231650 - gail/main.py:174 - [TRPO] iter = 41000 dist_mean = 0.0945 dist_std = 0.9731 vf_loss = 0.3788 grad_norm = 0.3621 nat_grad_norm = 0.5873 cg_residual = 0.0005 step_size = 0.4280 reward = 0.0000 fps = 15 mse_loss = 0.8816 
2022-07-08 09:36:29.137718 - gail/main.py:174 - [TRPO] iter = 42000 dist_mean = 0.1059 dist_std = 0.9704 vf_loss = 0.6891 grad_norm = 0.4261 nat_grad_norm = 0.5634 cg_residual = 0.0013 step_size = 0.3911 reward = 0.0000 fps = 10 mse_loss = 0.8912 
2022-07-08 09:36:55.323547 - gail/main.py:174 - [TRPO] iter = 43000 dist_mean = 0.1876 dist_std = 0.9687 vf_loss = 0.4119 grad_norm = 0.4199 nat_grad_norm = 0.5722 cg_residual = 0.0041 step_size = 0.4090 reward = 0.0000 fps = 8 mse_loss = 0.8855 
2022-07-08 09:37:20.583135 - gail/main.py:174 - [TRPO] iter = 44000 dist_mean = 0.1457 dist_std = 0.9708 vf_loss = 0.6276 grad_norm = 0.4092 nat_grad_norm = 0.6689 cg_residual = 0.0035 step_size = 0.3674 reward = 0.0000 fps = 6 mse_loss = 0.8967 
2022-07-08 09:37:46.889367 - gail/main.py:174 - [TRPO] iter = 45000 dist_mean = 0.2067 dist_std = 0.9714 vf_loss = 0.2892 grad_norm = 0.5024 nat_grad_norm = 0.6049 cg_residual = 0.0069 step_size = 0.3422 reward = -0.0000 fps = 5 mse_loss = 0.8880 
2022-07-08 09:37:47.651546 - gail/main.py:201 - [Discriminator] iter = 45000 loss = -2.9693 grad_norm = 6.8531 grad_penalty = 0.4071 regularization = 0.0000 true_logits = 0.5964 fake_logits = -2.7800 true_prob = 0.6416 fake_prob = 0.0801 
2022-07-08 09:38:30.345065 - gail/main.py:142 - [Evaluate] iter = 45000 episode={ returns = 209.4827 lengths = 120 } discounted_episode={ returns = 194.7423 lengths = 122 } 
2022-07-08 09:38:56.358309 - gail/main.py:174 - [TRPO] iter = 46000 dist_mean = 0.1635 dist_std = 0.9728 vf_loss = 0.8451 grad_norm = 0.5590 nat_grad_norm = 0.5555 cg_residual = 0.0027 step_size = 0.3676 reward = -0.0000 fps = 14 mse_loss = 0.8987 
2022-07-08 09:39:22.292447 - gail/main.py:174 - [TRPO] iter = 47000 dist_mean = 0.1606 dist_std = 0.9724 vf_loss = 0.5208 grad_norm = 0.4000 nat_grad_norm = 0.6898 cg_residual = 0.0033 step_size = 0.3670 reward = 0.0000 fps = 10 mse_loss = 0.8770 
2022-07-08 09:39:49.541868 - gail/main.py:174 - [TRPO] iter = 48000 dist_mean = 0.1537 dist_std = 0.9699 vf_loss = 0.4611 grad_norm = 0.4212 nat_grad_norm = 0.5260 cg_residual = 0.0029 step_size = 0.4189 reward = 0.0000 fps = 8 mse_loss = 0.9336 
2022-07-08 09:40:16.783357 - gail/main.py:174 - [TRPO] iter = 49000 dist_mean = 0.1795 dist_std = 0.9704 vf_loss = 0.3398 grad_norm = 0.4876 nat_grad_norm = 0.5540 cg_residual = 0.0050 step_size = 0.3738 reward = 0.0000 fps = 6 mse_loss = 0.9282 
2022-07-08 09:40:43.559628 - gail/main.py:174 - [TRPO] iter = 50000 dist_mean = 0.1712 dist_std = 0.9718 vf_loss = 0.3391 grad_norm = 0.5145 nat_grad_norm = 0.6019 cg_residual = 0.0042 step_size = 0.3648 reward = 0.0000 fps = 5 mse_loss = 0.8871 
2022-07-08 09:40:44.286420 - gail/main.py:201 - [Discriminator] iter = 50000 loss = -3.3414 grad_norm = 7.1683 grad_penalty = 0.4685 regularization = 0.0000 true_logits = 0.5944 fake_logits = -3.2155 true_prob = 0.6391 fake_prob = 0.0590 
2022-07-08 09:41:55.789802 - gail/main.py:142 - [Evaluate] iter = 50000 episode={ returns = 301.5934 lengths = 205 } discounted_episode={ returns = 261.6042 lengths = 201 } 
2022-07-08 09:42:28.156659 - gail/main.py:174 - [TRPO] iter = 51000 dist_mean = 0.1593 dist_std = 0.9666 vf_loss = 0.3671 grad_norm = 0.4422 nat_grad_norm = 0.6572 cg_residual = 0.0031 step_size = 0.3754 reward = 0.0000 fps = 9 mse_loss = 0.9385 
2022-07-08 09:42:53.647601 - gail/main.py:174 - [TRPO] iter = 52000 dist_mean = 0.2076 dist_std = 0.9737 vf_loss = 0.3936 grad_norm = 0.4636 nat_grad_norm = 0.6035 cg_residual = 0.0039 step_size = 0.3853 reward = 0.0000 fps = 7 mse_loss = 0.9047 
2022-07-08 09:43:18.465827 - gail/main.py:174 - [TRPO] iter = 53000 dist_mean = 0.1928 dist_std = 0.9747 vf_loss = 0.2557 grad_norm = 0.4766 nat_grad_norm = 0.5652 cg_residual = 0.0040 step_size = 0.3880 reward = -0.0000 fps = 6 mse_loss = 0.8822 
2022-07-08 09:43:43.411899 - gail/main.py:174 - [TRPO] iter = 54000 dist_mean = 0.1745 dist_std = 0.9800 vf_loss = 0.4214 grad_norm = 0.4095 nat_grad_norm = 0.6188 cg_residual = 0.0025 step_size = 0.4011 reward = 0.0000 fps = 5 mse_loss = 0.9565 
2022-07-08 09:44:08.820583 - gail/main.py:174 - [TRPO] iter = 55000 dist_mean = 0.1637 dist_std = 0.9765 vf_loss = 0.4069 grad_norm = 0.4406 nat_grad_norm = 0.5961 cg_residual = 0.0020 step_size = 0.4192 reward = -0.0000 fps = 4 mse_loss = 0.8732 
2022-07-08 09:44:09.561924 - gail/main.py:201 - [Discriminator] iter = 55000 loss = -3.8434 grad_norm = 7.2907 grad_penalty = 0.4936 regularization = 0.0000 true_logits = 0.6312 fake_logits = -3.7058 true_prob = 0.6472 fake_prob = 0.0401 
2022-07-08 09:47:53.975407 - gail/main.py:142 - [Evaluate] iter = 55000 episode={ returns = 633.8180 lengths = 583 } discounted_episode={ returns = 493.0809 lengths = 696 } 
2022-07-08 09:48:19.595619 - gail/main.py:174 - [TRPO] iter = 56000 dist_mean = 0.2100 dist_std = 0.9743 vf_loss = 0.3729 grad_norm = 0.4018 nat_grad_norm = 0.6548 cg_residual = 0.0026 step_size = 0.3804 reward = 0.0000 fps = 3 mse_loss = 0.9255 
2022-07-08 09:48:44.867156 - gail/main.py:174 - [TRPO] iter = 57000 dist_mean = 0.1941 dist_std = 0.9739 vf_loss = 0.6683 grad_norm = 0.4424 nat_grad_norm = 0.6956 cg_residual = 0.0058 step_size = 0.3824 reward = -0.0000 fps = 3 mse_loss = 0.9132 
2022-07-08 09:49:09.797518 - gail/main.py:174 - [TRPO] iter = 58000 dist_mean = 0.1752 dist_std = 0.9751 vf_loss = 0.3971 grad_norm = 0.4321 nat_grad_norm = 0.5468 cg_residual = 0.0022 step_size = 0.4521 reward = -0.0000 fps = 3 mse_loss = 0.9371 
2022-07-08 09:49:35.356759 - gail/main.py:174 - [TRPO] iter = 59000 dist_mean = 0.2000 dist_std = 0.9722 vf_loss = 0.4621 grad_norm = 0.4532 nat_grad_norm = 0.6619 cg_residual = 0.0046 step_size = 0.3718 reward = 0.0000 fps = 3 mse_loss = 0.8718 
2022-07-08 09:50:03.066474 - gail/main.py:174 - [TRPO] iter = 60000 dist_mean = 0.2191 dist_std = 0.9682 vf_loss = 0.4348 grad_norm = 0.3654 nat_grad_norm = 0.5664 cg_residual = 0.0030 step_size = 0.4346 reward = -0.0000 fps = 2 mse_loss = 0.9349 
2022-07-08 09:50:03.818735 - gail/main.py:201 - [Discriminator] iter = 60000 loss = -4.0519 grad_norm = 6.5735 grad_penalty = 0.5249 regularization = 0.0000 true_logits = 0.5748 fake_logits = -4.0020 true_prob = 0.6346 fake_prob = 0.0343 
2022-07-08 09:56:06.557827 - gail/main.py:142 - [Evaluate] iter = 60000 episode={ returns = 979.3089 lengths = 1000 } discounted_episode={ returns = 610.1385 lengths = 1000 } 
2022-07-08 09:56:30.684039 - gail/main.py:174 - [TRPO] iter = 61000 dist_mean = 0.1761 dist_std = 0.9647 vf_loss = 0.5222 grad_norm = 0.5043 nat_grad_norm = 0.6761 cg_residual = 0.0035 step_size = 0.3571 reward = -0.0000 fps = 2 mse_loss = 0.9076 
2022-07-08 09:56:54.410189 - gail/main.py:174 - [TRPO] iter = 62000 dist_mean = 0.2361 dist_std = 0.9679 vf_loss = 0.5194 grad_norm = 0.5148 nat_grad_norm = 0.6267 cg_residual = 0.0051 step_size = 0.3622 reward = -0.0000 fps = 2 mse_loss = 0.8254 
2022-07-08 09:57:18.837474 - gail/main.py:174 - [TRPO] iter = 63000 dist_mean = 0.2137 dist_std = 0.9624 vf_loss = 0.4210 grad_norm = 0.5061 nat_grad_norm = 0.6520 cg_residual = 0.0065 step_size = 0.3649 reward = 0.0000 fps = 2 mse_loss = 0.8544 
2022-07-08 09:57:43.082893 - gail/main.py:174 - [TRPO] iter = 64000 dist_mean = 0.2148 dist_std = 0.9613 vf_loss = 0.3843 grad_norm = 0.5410 nat_grad_norm = 0.6142 cg_residual = 0.0064 step_size = 0.3455 reward = -0.0000 fps = 2 mse_loss = 0.7693 
2022-07-08 09:58:07.460039 - gail/main.py:174 - [TRPO] iter = 65000 dist_mean = 0.2037 dist_std = 0.9628 vf_loss = 0.2969 grad_norm = 0.4342 nat_grad_norm = 0.6171 cg_residual = 0.0048 step_size = 0.4027 reward = 0.0000 fps = 2 mse_loss = 0.8961 
2022-07-08 09:58:08.215157 - gail/main.py:201 - [Discriminator] iter = 65000 loss = -4.0902 grad_norm = 9.3328 grad_penalty = 0.5762 regularization = 0.0000 true_logits = 0.4626 fake_logits = -4.2038 true_prob = 0.6126 fake_prob = 0.0317 
2022-07-08 10:00:36.110091 - gail/main.py:142 - [Evaluate] iter = 65000 episode={ returns = 525.1393 lengths = 441 } discounted_episode={ returns = 373.0434 lengths = 379 } 
2022-07-08 10:01:03.428625 - gail/main.py:174 - [TRPO] iter = 66000 dist_mean = 0.2132 dist_std = 0.9567 vf_loss = 0.3874 grad_norm = 0.4576 nat_grad_norm = 0.5785 cg_residual = 0.0056 step_size = 0.3939 reward = 0.0000 fps = 5 mse_loss = 0.8303 
2022-07-08 10:01:29.640723 - gail/main.py:174 - [TRPO] iter = 67000 dist_mean = 0.1999 dist_std = 0.9551 vf_loss = 0.2728 grad_norm = 0.4993 nat_grad_norm = 0.6640 cg_residual = 0.0055 step_size = 0.3528 reward = 0.0000 fps = 4 mse_loss = 0.7501 
2022-07-08 10:02:00.954509 - gail/main.py:174 - [TRPO] iter = 68000 dist_mean = 0.1889 dist_std = 0.9544 vf_loss = 0.2162 grad_norm = 0.4592 nat_grad_norm = 0.7452 cg_residual = 0.0044 step_size = 0.3546 reward = -0.0000 fps = 4 mse_loss = 0.6964 
2022-07-08 10:02:33.698913 - gail/main.py:174 - [TRPO] iter = 69000 dist_mean = 0.1716 dist_std = 0.9499 vf_loss = 0.3977 grad_norm = 0.4715 nat_grad_norm = 0.5468 cg_residual = 0.0059 step_size = 0.3992 reward = 0.0000 fps = 3 mse_loss = 0.6751 
2022-07-08 10:03:02.521516 - gail/main.py:174 - [TRPO] iter = 70000 dist_mean = 0.2031 dist_std = 0.9488 vf_loss = 0.3125 grad_norm = 0.4906 nat_grad_norm = 0.6069 cg_residual = 0.0056 step_size = 0.3886 reward = 0.0000 fps = 3 mse_loss = 0.7365 
2022-07-08 10:03:03.405822 - gail/main.py:201 - [Discriminator] iter = 70000 loss = -4.4120 grad_norm = 6.7568 grad_penalty = 0.5008 regularization = 0.0000 true_logits = 0.3385 fake_logits = -4.5744 true_prob = 0.5859 fake_prob = 0.0239 
2022-07-08 10:04:12.287258 - gail/main.py:142 - [Evaluate] iter = 70000 episode={ returns = 250.9397 lengths = 163 } discounted_episode={ returns = 243.8782 lengths = 182 } 
2022-07-08 10:04:40.285797 - gail/main.py:174 - [TRPO] iter = 71000 dist_mean = 0.1927 dist_std = 0.9423 vf_loss = 0.2177 grad_norm = 0.4573 nat_grad_norm = 0.6651 cg_residual = 0.0070 step_size = 0.3751 reward = -0.0000 fps = 10 mse_loss = 0.7699 
2022-07-08 10:05:09.149375 - gail/main.py:174 - [TRPO] iter = 72000 dist_mean = 0.1739 dist_std = 0.9455 vf_loss = 0.4405 grad_norm = 0.4642 nat_grad_norm = 0.6649 cg_residual = 0.0047 step_size = 0.3534 reward = -0.0000 fps = 7 mse_loss = 0.7685 
2022-07-08 10:05:38.482458 - gail/main.py:174 - [TRPO] iter = 73000 dist_mean = 0.1555 dist_std = 0.9472 vf_loss = 0.4347 grad_norm = 0.4727 nat_grad_norm = 0.6425 cg_residual = 0.0043 step_size = 0.3737 reward = 0.0000 fps = 6 mse_loss = 0.7094 
2022-07-08 10:06:08.193118 - gail/main.py:174 - [TRPO] iter = 74000 dist_mean = 0.1991 dist_std = 0.9428 vf_loss = 0.4380 grad_norm = 0.4853 nat_grad_norm = 0.6254 cg_residual = 0.0064 step_size = 0.3615 reward = 0.0000 fps = 5 mse_loss = 0.8192 
2022-07-08 10:06:36.609792 - gail/main.py:174 - [TRPO] iter = 75000 dist_mean = 0.1780 dist_std = 0.9450 vf_loss = 0.2840 grad_norm = 0.5192 nat_grad_norm = 0.6730 cg_residual = 0.0072 step_size = 0.3445 reward = 0.0000 fps = 4 mse_loss = 0.8180 
2022-07-08 10:06:37.497212 - gail/main.py:201 - [Discriminator] iter = 75000 loss = -4.8045 grad_norm = 5.3498 grad_penalty = 0.4941 regularization = 0.0000 true_logits = 0.2715 fake_logits = -5.0271 true_prob = 0.5718 fake_prob = 0.0175 
2022-07-08 10:09:43.343862 - gail/main.py:142 - [Evaluate] iter = 75000 episode={ returns = 553.9214 lengths = 449 } discounted_episode={ returns = 417.8958 lengths = 477 } 
2022-07-08 10:10:12.581845 - gail/main.py:174 - [TRPO] iter = 76000 dist_mean = 0.1979 dist_std = 0.9407 vf_loss = 0.2521 grad_norm = 0.5584 nat_grad_norm = 0.6514 cg_residual = 0.0062 step_size = 0.3342 reward = 0.0000 fps = 4 mse_loss = 0.8586 
2022-07-08 10:10:41.847056 - gail/main.py:174 - [TRPO] iter = 77000 dist_mean = 0.1781 dist_std = 0.9406 vf_loss = 0.3151 grad_norm = 0.5862 nat_grad_norm = 0.6565 cg_residual = 0.0082 step_size = 0.3398 reward = -0.0000 fps = 4 mse_loss = 0.7765 
2022-07-08 10:11:10.392664 - gail/main.py:174 - [TRPO] iter = 78000 dist_mean = 0.2102 dist_std = 0.9366 vf_loss = 0.1778 grad_norm = 0.4645 nat_grad_norm = 0.6504 cg_residual = 0.0064 step_size = 0.3522 reward = -0.0000 fps = 3 mse_loss = 0.8000 
2022-07-08 10:11:40.351790 - gail/main.py:174 - [TRPO] iter = 79000 dist_mean = 0.2086 dist_std = 0.9337 vf_loss = 0.2037 grad_norm = 0.5338 nat_grad_norm = 0.6206 cg_residual = 0.0099 step_size = 0.3810 reward = -0.0000 fps = 3 mse_loss = 0.8078 
2022-07-08 10:12:16.837923 - gail/main.py:174 - [TRPO] iter = 80000 dist_mean = 0.2211 dist_std = 0.9318 vf_loss = 0.2600 grad_norm = 0.4715 nat_grad_norm = 0.6631 cg_residual = 0.0042 step_size = 0.3966 reward = -0.0000 fps = 2 mse_loss = 0.7453 
2022-07-08 10:12:17.679648 - gail/main.py:201 - [Discriminator] iter = 80000 loss = -4.8310 grad_norm = 6.7165 grad_penalty = 0.5316 regularization = 0.0000 true_logits = 0.0964 fake_logits = -5.2662 true_prob = 0.5397 fake_prob = 0.0148 
2022-07-08 10:14:58.812539 - gail/main.py:142 - [Evaluate] iter = 80000 episode={ returns = 435.2746 lengths = 320 } discounted_episode={ returns = 424.8347 lengths = 501 } 
2022-07-08 10:15:27.897265 - gail/main.py:174 - [TRPO] iter = 81000 dist_mean = 0.2519 dist_std = 0.9349 vf_loss = 0.4089 grad_norm = 0.5868 nat_grad_norm = 0.6152 cg_residual = 0.0149 step_size = 0.3375 reward = -0.0000 fps = 5 mse_loss = 0.7763 
2022-07-08 10:15:57.032506 - gail/main.py:174 - [TRPO] iter = 82000 dist_mean = 0.2213 dist_std = 0.9356 vf_loss = 0.3020 grad_norm = 0.5476 nat_grad_norm = 0.6586 cg_residual = 0.0088 step_size = 0.3461 reward = -0.0000 fps = 4 mse_loss = 0.7142 
2022-07-08 10:16:25.852161 - gail/main.py:174 - [TRPO] iter = 83000 dist_mean = 0.2233 dist_std = 0.9302 vf_loss = 0.2157 grad_norm = 0.4910 nat_grad_norm = 0.6464 cg_residual = 0.0107 step_size = 0.3708 reward = -0.0000 fps = 4 mse_loss = 0.8093 
2022-07-08 10:16:53.309089 - gail/main.py:174 - [TRPO] iter = 84000 dist_mean = 0.2102 dist_std = 0.9230 vf_loss = 0.3436 grad_norm = 0.4723 nat_grad_norm = 0.6516 cg_residual = 0.0054 step_size = 0.3710 reward = 0.0000 fps = 3 mse_loss = 0.7759 
2022-07-08 10:17:21.456747 - gail/main.py:174 - [TRPO] iter = 85000 dist_mean = 0.2200 dist_std = 0.9186 vf_loss = 0.5861 grad_norm = 0.3913 nat_grad_norm = 0.5731 cg_residual = 0.0049 step_size = 0.4188 reward = 0.0000 fps = 3 mse_loss = 0.7747 
2022-07-08 10:17:22.283536 - gail/main.py:201 - [Discriminator] iter = 85000 loss = -5.0019 grad_norm = 6.3925 grad_penalty = 0.5884 regularization = 0.0000 true_logits = 0.0131 fake_logits = -5.5773 true_prob = 0.5265 fake_prob = 0.0115 
2022-07-08 10:23:57.606095 - gail/main.py:142 - [Evaluate] iter = 85000 episode={ returns = 964.6305 lengths = 1000 } discounted_episode={ returns = 597.3228 lengths = 1000 } 
2022-07-08 10:24:25.920317 - gail/main.py:174 - [TRPO] iter = 86000 dist_mean = 0.2342 dist_std = 0.9125 vf_loss = 0.3197 grad_norm = 0.5238 nat_grad_norm = 0.6119 cg_residual = 0.0120 step_size = 0.3649 reward = -0.0000 fps = 2 mse_loss = 0.7758 
2022-07-08 10:24:55.022604 - gail/main.py:174 - [TRPO] iter = 87000 dist_mean = 0.2565 dist_std = 0.9114 vf_loss = 0.3986 grad_norm = 0.4685 nat_grad_norm = 0.5137 cg_residual = 0.0128 step_size = 0.4172 reward = -0.0000 fps = 2 mse_loss = 0.7191 
2022-07-08 10:25:22.734589 - gail/main.py:174 - [TRPO] iter = 88000 dist_mean = 0.2835 dist_std = 0.9108 vf_loss = 0.2307 grad_norm = 0.4718 nat_grad_norm = 0.6354 cg_residual = 0.0147 step_size = 0.3674 reward = -0.0000 fps = 2 mse_loss = 0.7032 
2022-07-08 10:25:51.554278 - gail/main.py:174 - [TRPO] iter = 89000 dist_mean = 0.2432 dist_std = 0.9109 vf_loss = 0.3259 grad_norm = 0.4987 nat_grad_norm = 0.6099 cg_residual = 0.0136 step_size = 0.3722 reward = -0.0000 fps = 1 mse_loss = 0.7979 
2022-07-08 10:26:21.636067 - gail/main.py:174 - [TRPO] iter = 90000 dist_mean = 0.2771 dist_std = 0.9056 vf_loss = 0.3093 grad_norm = 0.6256 nat_grad_norm = 0.5325 cg_residual = 0.0102 step_size = 0.4057 reward = 0.0000 fps = 1 mse_loss = 0.7606 
2022-07-08 10:26:22.406563 - gail/main.py:201 - [Discriminator] iter = 90000 loss = -4.5799 grad_norm = 6.0707 grad_penalty = 0.5947 regularization = 0.0000 true_logits = -0.0861 fake_logits = -5.2606 true_prob = 0.5046 fake_prob = 0.0144 
