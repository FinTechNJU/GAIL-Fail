2022-07-08 09:09:37.061014 - utils/flags.py:257 - log_dir = logs/gail_w-Walker2d-v2-300-2022-07-08-09-09-36
2022-07-08 09:10:08.279170 - gail/utils/replay_buffer.py:86 - Load dataset from /content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/Walker2d-v2
2022-07-08 09:10:38.124528 - gail/main.py:80 - Expert Reward 5150.674112
2022-07-08 09:10:39.320065 - gail/main.py:84 - Original dataset size 3000
2022-07-08 09:10:39.467321 - gail/main.py:86 - Subsampled dataset size 3000
2022-07-08 09:10:39.504459 - gail/main.py:87 - np random: 864 random : 786
2022-07-08 09:10:39.506442 - gail/main.py:91 - Sampled obs: 0.0531, acs: 0.2269
2022-07-08 09:10:43.744534 - lunzi/nn/flat_param.py:17 - Enabling flattening... ['GaussianMLPPolicy_1/log_std:0', 'GaussianMLPPolicy_1/Variable:0', 'GaussianMLPPolicy_1/Variable_1:0', 'GaussianMLPPolicy_1/Variable_2:0', 'GaussianMLPPolicy_1/Variable_3:0', 'GaussianMLPPolicy_1/Variable_4:0', 'GaussianMLPPolicy_1/Variable_5:0']
2022-07-08 09:11:26.051574 - gail/discriminator/discriminator.py:30 - Use predefined normalization.
2022-07-08 09:11:26.084242 - gail/discriminator/discriminator.py:33 - Normalizer loc:[[ 1.2194959e+00  2.4445076e-01 -7.8132987e-02 -2.6673764e-01
   1.8222688e-01 -9.5077172e-02 -3.3649772e-01  5.3370733e-02
   4.1614923e+00  4.1431887e-03  3.8142569e-02 -2.6013174e-03
  -1.0202496e-02  5.6982285e-01  2.9836079e-02 -1.5763690e-01
   1.7689442e-02]] 
 scale:[[0.06687175 0.23681822 0.23042987 0.33821535 0.664349   0.20301929
  0.42807332 0.7138035  0.986894   0.65049744 2.0363257  2.3816926
  3.7250905  6.026913   2.0511289  4.406521   6.1475325 ]]
2022-07-08 09:11:43.403408 - gail/discriminator/discriminator.py:67 - Discriminator uses Wasserstein distance.
2022-07-08 09:11:43.413364 - gail/discriminator/discriminator.py:68 - [<tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable:0' shape=(23, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_1:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_2:0' shape=(100, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_3:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_4:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_5:0' shape=(1,) dtype=float32_ref>]
2022-07-08 09:11:43.439769 - gail/discriminator/discriminator.py:69 - Use gradient penalty regularization (coef = 10.000000)
2022-07-08 09:11:47.277447 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions_mean]
2022-07-08 09:12:25.028585 - gail/main.py:142 - [Evaluate] iter = 0 episode={ returns = 70.4326 lengths = 87 } discounted_episode={ returns = 68.4323 lengths = 86 } 
2022-07-08 09:12:25.039292 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions]
2022-07-08 09:13:06.786307 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [fake_states fake_actions] => [scaled_neural_reward]
2022-07-08 09:13:07.995071 - lunzi/nn/module.py:71 - [MLPVFunction] is making TensorFlow callables, key = [states] => [values]
2022-07-08 09:13:10.144721 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [] => [sync_old]
2022-07-08 09:13:11.124660 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss flat_grad dist_std mean_kl dist_mean]
2022-07-08 09:13:16.847252 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states tangents actions] => [hessian_vec_prod]
2022-07-08 09:13:28.337447 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [] => [get_flat]
2022-07-08 09:13:29.512902 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [feed_flat] => [set_flat]
2022-07-08 09:13:30.711739 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss mean_kl]
2022-07-08 09:13:32.889205 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [train_vf vf_loss]
2022-07-08 09:13:36.245081 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [vf_loss]
2022-07-08 09:13:37.448225 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states actions_] => [mse_loss]
2022-07-08 09:13:38.611052 - gail/main.py:174 - [TRPO] iter = 1000 dist_mean = 0.0001 dist_std = 1.0000 vf_loss = 0.2010 grad_norm = 0.3681 nat_grad_norm = 0.5166 cg_residual = 0.0000 step_size = 0.3969 reward = -0.0000 fps = 9 mse_loss = 0.4353 
2022-07-08 09:14:07.113216 - gail/main.py:174 - [TRPO] iter = 2000 dist_mean = -0.0136 dist_std = 1.0050 vf_loss = 0.2367 grad_norm = 0.4137 nat_grad_norm = 0.5294 cg_residual = 0.0000 step_size = 0.3510 reward = 0.0000 fps = 7 mse_loss = 0.4500 
2022-07-08 09:14:37.834023 - gail/main.py:174 - [TRPO] iter = 3000 dist_mean = -0.0347 dist_std = 1.0003 vf_loss = 0.2748 grad_norm = 0.4021 nat_grad_norm = 0.5523 cg_residual = 0.0000 step_size = 0.3446 reward = 0.0000 fps = 5 mse_loss = 0.4777 
2022-07-08 09:15:10.067663 - gail/main.py:174 - [TRPO] iter = 4000 dist_mean = -0.0571 dist_std = 0.9992 vf_loss = 0.3172 grad_norm = 0.4847 nat_grad_norm = 0.5268 cg_residual = 0.0000 step_size = 0.3402 reward = -0.0000 fps = 4 mse_loss = 0.5019 
2022-07-08 09:15:40.134703 - gail/main.py:174 - [TRPO] iter = 5000 dist_mean = -0.0731 dist_std = 0.9978 vf_loss = 0.2435 grad_norm = 0.4364 nat_grad_norm = 0.4939 cg_residual = 0.0001 step_size = 0.3658 reward = 0.0000 fps = 4 mse_loss = 0.5351 
2022-07-08 09:15:40.137129 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [true_states true_actions fake_states fake_actions true_masks] => [train loss true_logits fake_logits true_prob fake_prob grad_norm grad_penalty regularization]
2022-07-08 09:15:49.320905 - gail/main.py:201 - [Discriminator] iter = 5000 loss = 1.0945 grad_norm = 12.5178 grad_penalty = 1.4682 regularization = 0.0000 true_logits = 0.0199 fake_logits = -0.3538 true_prob = 0.5051 fake_prob = 0.4135 
2022-07-08 09:15:54.975479 - gail/main.py:142 - [Evaluate] iter = 5000 episode={ returns = -0.5671 lengths = 14 } discounted_episode={ returns = -0.5516 lengths = 14 } 
2022-07-08 09:16:26.253691 - gail/main.py:174 - [TRPO] iter = 6000 dist_mean = -0.0906 dist_std = 0.9921 vf_loss = 0.2433 grad_norm = 0.4037 nat_grad_norm = 0.5582 cg_residual = 0.0001 step_size = 0.3564 reward = -0.0000 fps = 27 mse_loss = 0.5707 
2022-07-08 09:16:56.632336 - gail/main.py:174 - [TRPO] iter = 7000 dist_mean = -0.0971 dist_std = 0.9881 vf_loss = 0.2158 grad_norm = 0.5261 nat_grad_norm = 0.5300 cg_residual = 0.0001 step_size = 0.3186 reward = -0.0000 fps = 14 mse_loss = 0.6026 
2022-07-08 09:17:27.128860 - gail/main.py:174 - [TRPO] iter = 8000 dist_mean = -0.0925 dist_std = 0.9873 vf_loss = 0.1910 grad_norm = 0.5658 nat_grad_norm = 0.4657 cg_residual = 0.0001 step_size = 0.3512 reward = 0.0000 fps = 10 mse_loss = 0.5898 
2022-07-08 09:17:57.638551 - gail/main.py:174 - [TRPO] iter = 9000 dist_mean = -0.0862 dist_std = 0.9851 vf_loss = 0.1513 grad_norm = 0.5103 nat_grad_norm = 0.5467 cg_residual = 0.0002 step_size = 0.3423 reward = -0.0000 fps = 7 mse_loss = 0.5874 
2022-07-08 09:18:27.236490 - gail/main.py:174 - [TRPO] iter = 10000 dist_mean = -0.0759 dist_std = 0.9834 vf_loss = 0.1626 grad_norm = 0.4182 nat_grad_norm = 0.4988 cg_residual = 0.0002 step_size = 0.3925 reward = 0.0000 fps = 6 mse_loss = 0.5784 
2022-07-08 09:18:28.034385 - gail/main.py:201 - [Discriminator] iter = 10000 loss = -0.0570 grad_norm = 10.2298 grad_penalty = 0.7629 regularization = 0.0000 true_logits = 0.0693 fake_logits = -0.7506 true_prob = 0.5175 fake_prob = 0.3231 
2022-07-08 09:18:35.494051 - gail/main.py:142 - [Evaluate] iter = 10000 episode={ returns = -6.1994 lengths = 19 } discounted_episode={ returns = -5.9969 lengths = 19 } 
2022-07-08 09:19:06.723678 - gail/main.py:174 - [TRPO] iter = 11000 dist_mean = -0.0770 dist_std = 0.9791 vf_loss = 0.1842 grad_norm = 0.5616 nat_grad_norm = 0.4952 cg_residual = 0.0003 step_size = 0.3470 reward = 0.0000 fps = 25 mse_loss = 0.5736 
2022-07-08 09:19:36.517975 - gail/main.py:174 - [TRPO] iter = 12000 dist_mean = -0.0512 dist_std = 0.9734 vf_loss = 0.1647 grad_norm = 0.6783 nat_grad_norm = 0.5140 cg_residual = 0.0003 step_size = 0.3139 reward = -0.0000 fps = 14 mse_loss = 0.5450 
2022-07-08 09:20:06.225263 - gail/main.py:174 - [TRPO] iter = 13000 dist_mean = -0.0205 dist_std = 0.9743 vf_loss = 0.2575 grad_norm = 0.5948 nat_grad_norm = 0.4629 cg_residual = 0.0004 step_size = 0.3750 reward = 0.0000 fps = 10 mse_loss = 0.5528 
2022-07-08 09:20:35.777704 - gail/main.py:174 - [TRPO] iter = 14000 dist_mean = 0.0182 dist_std = 0.9704 vf_loss = 0.2109 grad_norm = 0.6776 nat_grad_norm = 0.4841 cg_residual = 0.0005 step_size = 0.3499 reward = -0.0000 fps = 7 mse_loss = 0.5004 
2022-07-08 09:21:04.953699 - gail/main.py:174 - [TRPO] iter = 15000 dist_mean = 0.0482 dist_std = 0.9685 vf_loss = 0.1603 grad_norm = 0.4470 nat_grad_norm = 0.5387 cg_residual = 0.0008 step_size = 0.4014 reward = 0.0000 fps = 6 mse_loss = 0.5178 
2022-07-08 09:21:05.753061 - gail/main.py:201 - [Discriminator] iter = 15000 loss = -0.7841 grad_norm = 6.8347 grad_penalty = 0.4714 regularization = 0.0000 true_logits = 0.1413 fake_logits = -1.1141 true_prob = 0.5358 fake_prob = 0.2544 
2022-07-08 09:21:32.177194 - gail/main.py:142 - [Evaluate] iter = 15000 episode={ returns = -19.3789 lengths = 72 } discounted_episode={ returns = -18.2623 lengths = 72 } 
2022-07-08 09:22:07.421935 - gail/main.py:174 - [TRPO] iter = 16000 dist_mean = 0.0549 dist_std = 0.9703 vf_loss = 0.1552 grad_norm = 0.5514 nat_grad_norm = 0.5207 cg_residual = 0.0004 step_size = 0.3732 reward = -0.0000 fps = 16 mse_loss = 0.5612 
2022-07-08 09:22:36.195677 - gail/main.py:174 - [TRPO] iter = 17000 dist_mean = 0.0922 dist_std = 0.9710 vf_loss = 0.1164 grad_norm = 0.4994 nat_grad_norm = 0.4943 cg_residual = 0.0006 step_size = 0.4093 reward = 0.0000 fps = 11 mse_loss = 0.5737 
2022-07-08 09:23:05.566953 - gail/main.py:174 - [TRPO] iter = 18000 dist_mean = 0.1107 dist_std = 0.9665 vf_loss = 0.1664 grad_norm = 0.4362 nat_grad_norm = 0.4632 cg_residual = 0.0004 step_size = 0.4384 reward = -0.0000 fps = 8 mse_loss = 0.5612 
2022-07-08 09:23:35.884958 - gail/main.py:174 - [TRPO] iter = 19000 dist_mean = 0.1319 dist_std = 0.9634 vf_loss = 0.0944 grad_norm = 0.5515 nat_grad_norm = 0.6018 cg_residual = 0.0010 step_size = 0.3501 reward = 0.0000 fps = 6 mse_loss = 0.6418 
2022-07-08 09:24:04.685713 - gail/main.py:174 - [TRPO] iter = 20000 dist_mean = 0.1498 dist_std = 0.9619 vf_loss = 0.1741 grad_norm = 0.5041 nat_grad_norm = 0.4881 cg_residual = 0.0006 step_size = 0.4128 reward = 0.0000 fps = 5 mse_loss = 0.7263 
2022-07-08 09:24:05.478948 - gail/main.py:201 - [Discriminator] iter = 20000 loss = -1.2548 grad_norm = 6.7575 grad_penalty = 0.4616 regularization = 0.0000 true_logits = 0.1910 fake_logits = -1.5253 true_prob = 0.5483 fake_prob = 0.1903 
2022-07-08 09:24:24.505657 - gail/main.py:142 - [Evaluate] iter = 20000 episode={ returns = -8.0259 lengths = 55 } discounted_episode={ returns = -6.8165 lengths = 49 } 
2022-07-08 09:24:51.806443 - gail/main.py:174 - [TRPO] iter = 21000 dist_mean = 0.1712 dist_std = 0.9609 vf_loss = 0.1330 grad_norm = 0.4769 nat_grad_norm = 0.4800 cg_residual = 0.0006 step_size = 0.4453 reward = -0.0000 fps = 21 mse_loss = 0.7436 
2022-07-08 09:25:18.882517 - gail/main.py:174 - [TRPO] iter = 22000 dist_mean = 0.1970 dist_std = 0.9619 vf_loss = 0.1244 grad_norm = 0.6710 nat_grad_norm = 0.5022 cg_residual = 0.0008 step_size = 0.3608 reward = -0.0000 fps = 13 mse_loss = 0.8306 
2022-07-08 09:25:46.717777 - gail/main.py:174 - [TRPO] iter = 23000 dist_mean = 0.2170 dist_std = 0.9576 vf_loss = 0.2515 grad_norm = 0.7846 nat_grad_norm = 0.4655 cg_residual = 0.0008 step_size = 0.3362 reward = 0.0000 fps = 9 mse_loss = 0.8819 
2022-07-08 09:26:17.350148 - gail/main.py:174 - [TRPO] iter = 24000 dist_mean = 0.2446 dist_std = 0.9506 vf_loss = 0.1608 grad_norm = 0.6016 nat_grad_norm = 0.5013 cg_residual = 0.0008 step_size = 0.3591 reward = -0.0000 fps = 7 mse_loss = 1.0401 
2022-07-08 09:26:46.437960 - gail/main.py:174 - [TRPO] iter = 25000 dist_mean = 0.3028 dist_std = 0.9521 vf_loss = 0.3759 grad_norm = 0.4558 nat_grad_norm = 0.3943 cg_residual = 0.0006 step_size = 0.5044 reward = 0.0000 fps = 6 mse_loss = 0.9699 
2022-07-08 09:26:47.172416 - gail/main.py:201 - [Discriminator] iter = 25000 loss = -1.7051 grad_norm = 6.1630 grad_penalty = 0.3433 regularization = 0.0000 true_logits = 0.1819 fake_logits = -1.8665 true_prob = 0.5469 fake_prob = 0.1477 
2022-07-08 09:27:28.673390 - gail/main.py:142 - [Evaluate] iter = 25000 episode={ returns = 28.3291 lengths = 105 } discounted_episode={ returns = 28.2561 lengths = 104 } 
2022-07-08 09:27:55.859767 - gail/main.py:174 - [TRPO] iter = 26000 dist_mean = 0.3239 dist_std = 0.9516 vf_loss = 0.5443 grad_norm = 0.6160 nat_grad_norm = 0.4184 cg_residual = 0.0005 step_size = 0.4124 reward = 0.0000 fps = 14 mse_loss = 1.0390 
2022-07-08 09:28:22.804828 - gail/main.py:174 - [TRPO] iter = 27000 dist_mean = 0.4078 dist_std = 0.9513 vf_loss = 0.7579 grad_norm = 0.3325 nat_grad_norm = 0.4940 cg_residual = 0.0038 step_size = 0.4848 reward = 0.0000 fps = 10 mse_loss = 0.9761 
2022-07-08 09:28:51.633792 - gail/main.py:174 - [TRPO] iter = 28000 dist_mean = 0.3372 dist_std = 0.9504 vf_loss = 0.5919 grad_norm = 0.5256 nat_grad_norm = 0.4937 cg_residual = 0.0009 step_size = 0.4122 reward = -0.0000 fps = 8 mse_loss = 1.1166 
2022-07-08 09:29:19.520258 - gail/main.py:174 - [TRPO] iter = 29000 dist_mean = 0.3266 dist_std = 0.9441 vf_loss = 0.4702 grad_norm = 0.5705 nat_grad_norm = 0.4617 cg_residual = 0.0007 step_size = 0.4143 reward = -0.0000 fps = 6 mse_loss = 1.1905 
2022-07-08 09:29:46.937556 - gail/main.py:174 - [TRPO] iter = 30000 dist_mean = 0.5308 dist_std = 0.9373 vf_loss = 0.8212 grad_norm = 0.6709 nat_grad_norm = 0.4318 cg_residual = 0.0014 step_size = 0.4728 reward = -0.0000 fps = 5 mse_loss = 1.2254 
2022-07-08 09:29:47.745395 - gail/main.py:201 - [Discriminator] iter = 30000 loss = -1.8010 grad_norm = 5.8737 grad_penalty = 0.3305 regularization = 0.0000 true_logits = 0.2371 fake_logits = -1.8944 true_prob = 0.5604 fake_prob = 0.1507 
2022-07-08 09:30:40.679421 - gail/main.py:142 - [Evaluate] iter = 30000 episode={ returns = 254.7125 lengths = 138 } discounted_episode={ returns = 233.9096 lengths = 138 } 
2022-07-08 09:31:09.039219 - gail/main.py:174 - [TRPO] iter = 31000 dist_mean = 0.5480 dist_std = 0.9413 vf_loss = 1.0898 grad_norm = 0.5074 nat_grad_norm = 0.4945 cg_residual = 0.0035 step_size = 0.3845 reward = 0.0000 fps = 12 mse_loss = 1.2904 
2022-07-08 09:31:36.574970 - gail/main.py:174 - [TRPO] iter = 32000 dist_mean = 0.6210 dist_std = 0.9377 vf_loss = 1.0347 grad_norm = 0.4522 nat_grad_norm = 0.3818 cg_residual = 0.0019 step_size = 0.5525 reward = 0.0000 fps = 9 mse_loss = 1.3196 
2022-07-08 09:32:04.395928 - gail/main.py:174 - [TRPO] iter = 33000 dist_mean = 0.6403 dist_std = 0.9369 vf_loss = 0.9889 grad_norm = 0.5022 nat_grad_norm = 0.4454 cg_residual = 0.0068 step_size = 0.4332 reward = 0.0000 fps = 7 mse_loss = 1.4259 
2022-07-08 09:32:38.415596 - gail/main.py:174 - [TRPO] iter = 34000 dist_mean = 0.4351 dist_std = 0.9352 vf_loss = 1.3910 grad_norm = 0.4856 nat_grad_norm = 0.4196 cg_residual = 0.0033 step_size = 0.4525 reward = 0.0000 fps = 5 mse_loss = 1.5087 
2022-07-08 09:33:05.756713 - gail/main.py:174 - [TRPO] iter = 35000 dist_mean = 0.5969 dist_std = 0.9328 vf_loss = 1.6288 grad_norm = 0.3255 nat_grad_norm = 0.4548 cg_residual = 0.0018 step_size = 0.4563 reward = 0.0000 fps = 5 mse_loss = 1.5279 
2022-07-08 09:33:06.568041 - gail/main.py:201 - [Discriminator] iter = 35000 loss = -2.1083 grad_norm = 5.9056 grad_penalty = 0.2593 regularization = 0.0000 true_logits = 0.1977 fake_logits = -2.1699 true_prob = 0.5531 fake_prob = 0.1165 
2022-07-08 09:33:58.287208 - gail/main.py:142 - [Evaluate] iter = 35000 episode={ returns = 253.2754 lengths = 136 } discounted_episode={ returns = 230.3078 lengths = 134 } 
2022-07-08 09:34:24.901873 - gail/main.py:174 - [TRPO] iter = 36000 dist_mean = 0.6404 dist_std = 0.9230 vf_loss = 1.5652 grad_norm = 0.3823 nat_grad_norm = 0.4967 cg_residual = 0.0022 step_size = 0.4330 reward = -0.0000 fps = 12 mse_loss = 1.5583 
2022-07-08 09:34:51.921828 - gail/main.py:174 - [TRPO] iter = 37000 dist_mean = 0.5410 dist_std = 0.9294 vf_loss = 0.8179 grad_norm = 0.3289 nat_grad_norm = 0.4535 cg_residual = 0.0050 step_size = 0.5081 reward = -0.0000 fps = 9 mse_loss = 1.6333 
2022-07-08 09:35:18.638826 - gail/main.py:174 - [TRPO] iter = 38000 dist_mean = 0.4945 dist_std = 0.9286 vf_loss = 1.4161 grad_norm = 0.3238 nat_grad_norm = 0.4787 cg_residual = 0.0029 step_size = 0.4756 reward = 0.0000 fps = 7 mse_loss = 1.8016 
2022-07-08 09:35:44.922611 - gail/main.py:174 - [TRPO] iter = 39000 dist_mean = 0.4799 dist_std = 0.9238 vf_loss = 1.4031 grad_norm = 0.4510 nat_grad_norm = 0.5028 cg_residual = 0.0043 step_size = 0.3893 reward = 0.0000 fps = 6 mse_loss = 1.6507 
2022-07-08 09:36:11.904380 - gail/main.py:174 - [TRPO] iter = 40000 dist_mean = 0.5300 dist_std = 0.9142 vf_loss = 1.1524 grad_norm = 0.3842 nat_grad_norm = 0.4908 cg_residual = 0.0048 step_size = 0.4533 reward = 0.0000 fps = 5 mse_loss = 1.6172 
2022-07-08 09:36:12.699988 - gail/main.py:201 - [Discriminator] iter = 40000 loss = -2.6377 grad_norm = 6.1191 grad_penalty = 0.2952 regularization = 0.0000 true_logits = 0.2303 fake_logits = -2.7026 true_prob = 0.5598 fake_prob = 0.0739 
2022-07-08 09:37:02.953717 - gail/main.py:142 - [Evaluate] iter = 40000 episode={ returns = 250.4188 lengths = 134 } discounted_episode={ returns = 230.2201 lengths = 133 } 
2022-07-08 09:37:29.303273 - gail/main.py:174 - [TRPO] iter = 41000 dist_mean = 0.5501 dist_std = 0.9031 vf_loss = 1.0504 grad_norm = 0.3716 nat_grad_norm = 0.4948 cg_residual = 0.0041 step_size = 0.4379 reward = 0.0000 fps = 13 mse_loss = 1.7126 
2022-07-08 09:37:55.746822 - gail/main.py:174 - [TRPO] iter = 42000 dist_mean = 0.5619 dist_std = 0.9066 vf_loss = 0.9332 grad_norm = 0.3522 nat_grad_norm = 0.3779 cg_residual = 0.0018 step_size = 0.5135 reward = 0.0000 fps = 9 mse_loss = 1.6973 
2022-07-08 09:38:22.155634 - gail/main.py:174 - [TRPO] iter = 43000 dist_mean = 0.5328 dist_std = 0.9081 vf_loss = 1.3794 grad_norm = 0.4198 nat_grad_norm = 0.4382 cg_residual = 0.0045 step_size = 0.4462 reward = -0.0000 fps = 7 mse_loss = 1.6167 
2022-07-08 09:38:48.985151 - gail/main.py:174 - [TRPO] iter = 44000 dist_mean = 0.6144 dist_std = 0.9029 vf_loss = 0.6040 grad_norm = 0.3186 nat_grad_norm = 0.4723 cg_residual = 0.0035 step_size = 0.4437 reward = 0.0000 fps = 6 mse_loss = 1.6931 
2022-07-08 09:39:15.811885 - gail/main.py:174 - [TRPO] iter = 45000 dist_mean = 0.5694 dist_std = 0.8995 vf_loss = 0.7297 grad_norm = 0.5190 nat_grad_norm = 0.5932 cg_residual = 0.0071 step_size = 0.3601 reward = 0.0000 fps = 5 mse_loss = 1.7629 
2022-07-08 09:39:16.606785 - gail/main.py:201 - [Discriminator] iter = 45000 loss = -3.0449 grad_norm = 6.4021 grad_penalty = 0.3321 regularization = 0.0000 true_logits = 0.2484 fake_logits = -3.1287 true_prob = 0.5649 fake_prob = 0.0485 
2022-07-08 09:40:04.558355 - gail/main.py:142 - [Evaluate] iter = 45000 episode={ returns = 243.1046 lengths = 129 } discounted_episode={ returns = 226.9982 lengths = 130 } 
2022-07-08 09:40:32.481031 - gail/main.py:174 - [TRPO] iter = 46000 dist_mean = 0.5559 dist_std = 0.8880 vf_loss = 0.7622 grad_norm = 0.3655 nat_grad_norm = 0.4507 cg_residual = 0.0035 step_size = 0.4432 reward = 0.0000 fps = 13 mse_loss = 1.8356 
2022-07-08 09:40:58.319017 - gail/main.py:174 - [TRPO] iter = 47000 dist_mean = 0.5518 dist_std = 0.8836 vf_loss = 0.7803 grad_norm = 0.4393 nat_grad_norm = 0.5134 cg_residual = 0.0044 step_size = 0.3927 reward = 0.0000 fps = 9 mse_loss = 1.7566 
2022-07-08 09:41:25.313625 - gail/main.py:174 - [TRPO] iter = 48000 dist_mean = 0.5760 dist_std = 0.8867 vf_loss = 0.5026 grad_norm = 0.4339 nat_grad_norm = 0.5158 cg_residual = 0.0060 step_size = 0.3907 reward = 0.0000 fps = 7 mse_loss = 1.8524 
2022-07-08 09:41:51.506814 - gail/main.py:174 - [TRPO] iter = 49000 dist_mean = 0.5442 dist_std = 0.8823 vf_loss = 0.4593 grad_norm = 0.4196 nat_grad_norm = 0.4615 cg_residual = 0.0056 step_size = 0.4170 reward = 0.0000 fps = 6 mse_loss = 1.7358 
2022-07-08 09:42:23.752199 - gail/main.py:174 - [TRPO] iter = 50000 dist_mean = 0.5534 dist_std = 0.8813 vf_loss = 0.5494 grad_norm = 0.4639 nat_grad_norm = 0.4865 cg_residual = 0.0072 step_size = 0.4009 reward = -0.0000 fps = 5 mse_loss = 1.9055 
2022-07-08 09:42:24.505657 - gail/main.py:201 - [Discriminator] iter = 50000 loss = -3.3762 grad_norm = 7.5605 grad_penalty = 0.3834 regularization = 0.0000 true_logits = 0.1957 fake_logits = -3.5640 true_prob = 0.5548 fake_prob = 0.0328 
2022-07-08 09:43:02.243811 - gail/main.py:142 - [Evaluate] iter = 50000 episode={ returns = 214.2635 lengths = 109 } discounted_episode={ returns = 197.7473 lengths = 107 } 
2022-07-08 09:43:26.674835 - gail/main.py:174 - [TRPO] iter = 51000 dist_mean = 0.5347 dist_std = 0.8773 vf_loss = 0.5880 grad_norm = 0.4004 nat_grad_norm = 0.4834 cg_residual = 0.0131 step_size = 0.4360 reward = 0.0000 fps = 16 mse_loss = 1.8307 
2022-07-08 09:43:52.530605 - gail/main.py:174 - [TRPO] iter = 52000 dist_mean = 0.5781 dist_std = 0.8766 vf_loss = 0.3203 grad_norm = 0.4989 nat_grad_norm = 0.5311 cg_residual = 0.0050 step_size = 0.3869 reward = 0.0000 fps = 11 mse_loss = 1.9857 
2022-07-08 09:44:18.510757 - gail/main.py:174 - [TRPO] iter = 53000 dist_mean = 0.6064 dist_std = 0.8758 vf_loss = 0.1776 grad_norm = 0.6175 nat_grad_norm = 0.4202 cg_residual = 0.0046 step_size = 0.4468 reward = -0.0000 fps = 8 mse_loss = 1.9704 
2022-07-08 09:44:44.414109 - gail/main.py:174 - [TRPO] iter = 54000 dist_mean = 0.5639 dist_std = 0.8794 vf_loss = 0.2977 grad_norm = 0.4808 nat_grad_norm = 0.4494 cg_residual = 0.0053 step_size = 0.4360 reward = 0.0000 fps = 7 mse_loss = 1.9946 
2022-07-08 09:45:09.750762 - gail/main.py:174 - [TRPO] iter = 55000 dist_mean = 0.5606 dist_std = 0.8761 vf_loss = 0.2280 grad_norm = 0.5505 nat_grad_norm = 0.4431 cg_residual = 0.0064 step_size = 0.4339 reward = 0.0000 fps = 6 mse_loss = 2.0029 
2022-07-08 09:45:10.446763 - gail/main.py:201 - [Discriminator] iter = 55000 loss = -3.5174 grad_norm = 6.2295 grad_penalty = 0.4267 regularization = 0.0000 true_logits = 0.1773 fake_logits = -3.7668 true_prob = 0.5570 fake_prob = 0.0257 
2022-07-08 09:45:43.499086 - gail/main.py:142 - [Evaluate] iter = 55000 episode={ returns = 189.1634 lengths = 95 } discounted_episode={ returns = 180.5252 lengths = 96 } 
2022-07-08 09:46:08.767912 - gail/main.py:174 - [TRPO] iter = 56000 dist_mean = 0.5516 dist_std = 0.8751 vf_loss = 0.3763 grad_norm = 0.4721 nat_grad_norm = 0.4363 cg_residual = 0.0056 step_size = 0.4483 reward = 0.0000 fps = 17 mse_loss = 2.2809 
2022-07-08 09:46:34.472500 - gail/main.py:174 - [TRPO] iter = 57000 dist_mean = 0.5161 dist_std = 0.8778 vf_loss = 0.3892 grad_norm = 0.4546 nat_grad_norm = 0.4558 cg_residual = 0.0099 step_size = 0.4028 reward = -0.0000 fps = 11 mse_loss = 2.2288 
2022-07-08 09:47:00.861738 - gail/main.py:174 - [TRPO] iter = 58000 dist_mean = 0.5011 dist_std = 0.8746 vf_loss = 0.6423 grad_norm = 0.4399 nat_grad_norm = 0.4828 cg_residual = 0.0052 step_size = 0.4073 reward = -0.0000 fps = 9 mse_loss = 2.1972 
2022-07-08 09:47:26.809496 - gail/main.py:174 - [TRPO] iter = 59000 dist_mean = 0.5294 dist_std = 0.8745 vf_loss = 0.4081 grad_norm = 0.5146 nat_grad_norm = 0.5840 cg_residual = 0.0175 step_size = 0.3786 reward = 0.0000 fps = 7 mse_loss = 1.9832 
2022-07-08 09:47:52.384277 - gail/main.py:174 - [TRPO] iter = 60000 dist_mean = 0.5209 dist_std = 0.8710 vf_loss = 0.2710 grad_norm = 0.5567 nat_grad_norm = 0.4683 cg_residual = 0.0070 step_size = 0.3987 reward = 0.0000 fps = 6 mse_loss = 2.1380 
2022-07-08 09:47:53.092382 - gail/main.py:201 - [Discriminator] iter = 60000 loss = -3.6183 grad_norm = 4.4114 grad_penalty = 0.4245 regularization = 0.0000 true_logits = 0.1736 fake_logits = -3.8692 true_prob = 0.5523 fake_prob = 0.0240 
2022-07-08 09:48:25.492152 - gail/main.py:142 - [Evaluate] iter = 60000 episode={ returns = 186.4059 lengths = 95 } discounted_episode={ returns = 177.9951 lengths = 95 } 
2022-07-08 09:48:51.658657 - gail/main.py:174 - [TRPO] iter = 61000 dist_mean = 0.4991 dist_std = 0.8672 vf_loss = 0.3182 grad_norm = 0.5930 nat_grad_norm = 0.5445 cg_residual = 0.0169 step_size = 0.3575 reward = -0.0000 fps = 17 mse_loss = 2.2574 
2022-07-08 09:49:17.155879 - gail/main.py:174 - [TRPO] iter = 62000 dist_mean = 0.4997 dist_std = 0.8642 vf_loss = 0.3056 grad_norm = 0.6281 nat_grad_norm = 0.4128 cg_residual = 0.0071 step_size = 0.4062 reward = -0.0000 fps = 11 mse_loss = 2.2662 
2022-07-08 09:49:44.164678 - gail/main.py:174 - [TRPO] iter = 63000 dist_mean = 0.4993 dist_std = 0.8655 vf_loss = 0.3041 grad_norm = 0.3921 nat_grad_norm = 0.5189 cg_residual = 0.0107 step_size = 0.4171 reward = 0.0000 fps = 9 mse_loss = 2.2965 
2022-07-08 09:50:12.452187 - gail/main.py:174 - [TRPO] iter = 64000 dist_mean = 0.4660 dist_std = 0.8646 vf_loss = 0.4630 grad_norm = 0.6048 nat_grad_norm = 0.4817 cg_residual = 0.0097 step_size = 0.3946 reward = -0.0000 fps = 7 mse_loss = 2.3835 
2022-07-08 09:50:38.619453 - gail/main.py:174 - [TRPO] iter = 65000 dist_mean = 0.4608 dist_std = 0.8624 vf_loss = 0.2944 grad_norm = 0.5998 nat_grad_norm = 0.5032 cg_residual = 0.0147 step_size = 0.3900 reward = -0.0000 fps = 6 mse_loss = 2.5106 
2022-07-08 09:50:39.410491 - gail/main.py:201 - [Discriminator] iter = 65000 loss = -3.7844 grad_norm = 5.3798 grad_penalty = 0.4376 regularization = 0.0000 true_logits = 0.2135 fake_logits = -4.0086 true_prob = 0.5638 fake_prob = 0.0202 
2022-07-08 09:51:04.993254 - gail/main.py:142 - [Evaluate] iter = 65000 episode={ returns = 141.8145 lengths = 75 } discounted_episode={ returns = 138.4724 lengths = 76 } 
2022-07-08 09:51:30.130115 - gail/main.py:174 - [TRPO] iter = 66000 dist_mean = 0.4513 dist_std = 0.8625 vf_loss = 0.2475 grad_norm = 0.5307 nat_grad_norm = 0.5368 cg_residual = 0.0161 step_size = 0.3795 reward = -0.0000 fps = 19 mse_loss = 2.6168 
2022-07-08 09:51:56.313774 - gail/main.py:174 - [TRPO] iter = 67000 dist_mean = 0.4581 dist_std = 0.8634 vf_loss = 0.2635 grad_norm = 0.6299 nat_grad_norm = 0.5561 cg_residual = 0.0142 step_size = 0.3597 reward = 0.0000 fps = 13 mse_loss = 2.4911 
2022-07-08 09:52:28.605943 - gail/main.py:174 - [TRPO] iter = 68000 dist_mean = 0.4496 dist_std = 0.8630 vf_loss = 0.3004 grad_norm = 0.5716 nat_grad_norm = 0.5711 cg_residual = 0.0182 step_size = 0.3833 reward = 0.0000 fps = 9 mse_loss = 2.6944 
2022-07-08 09:52:54.484704 - gail/main.py:174 - [TRPO] iter = 69000 dist_mean = 0.4067 dist_std = 0.8636 vf_loss = 0.3055 grad_norm = 0.5505 nat_grad_norm = 0.5253 cg_residual = 0.0134 step_size = 0.3984 reward = -0.0000 fps = 7 mse_loss = 2.8342 
2022-07-08 09:53:21.333096 - gail/main.py:174 - [TRPO] iter = 70000 dist_mean = 0.4242 dist_std = 0.8574 vf_loss = 0.1783 grad_norm = 0.6409 nat_grad_norm = 0.6867 cg_residual = 0.0231 step_size = 0.2938 reward = 0.0000 fps = 6 mse_loss = 2.8935 
2022-07-08 09:53:22.158310 - gail/main.py:201 - [Discriminator] iter = 70000 loss = -3.8944 grad_norm = 5.8287 grad_penalty = 0.4184 regularization = 0.0000 true_logits = 0.1967 fake_logits = -4.1161 true_prob = 0.5581 fake_prob = 0.0181 
2022-07-08 09:53:45.902063 - gail/main.py:142 - [Evaluate] iter = 70000 episode={ returns = 123.7740 lengths = 67 } discounted_episode={ returns = 119.1177 lengths = 67 } 
2022-07-08 09:54:13.819249 - gail/main.py:174 - [TRPO] iter = 71000 dist_mean = 0.4051 dist_std = 0.8578 vf_loss = 0.3408 grad_norm = 0.5553 nat_grad_norm = 0.6601 cg_residual = 0.0083 step_size = 0.3629 reward = -0.0000 fps = 19 mse_loss = 2.9463 
2022-07-08 09:54:40.470382 - gail/main.py:174 - [TRPO] iter = 72000 dist_mean = 0.3953 dist_std = 0.8628 vf_loss = 0.3040 grad_norm = 0.4352 nat_grad_norm = 0.6024 cg_residual = 0.0167 step_size = 0.3812 reward = 0.0000 fps = 12 mse_loss = 2.9702 
2022-07-08 09:55:06.604395 - gail/main.py:174 - [TRPO] iter = 73000 dist_mean = 0.4461 dist_std = 0.8647 vf_loss = 0.2882 grad_norm = 0.6088 nat_grad_norm = 0.6544 cg_residual = 0.0185 step_size = 0.3480 reward = 0.0000 fps = 9 mse_loss = 2.6759 
2022-07-08 09:55:31.093769 - gail/main.py:174 - [TRPO] iter = 74000 dist_mean = 0.4195 dist_std = 0.8656 vf_loss = 0.2562 grad_norm = 0.5627 nat_grad_norm = 0.6314 cg_residual = 0.0269 step_size = 0.3383 reward = 0.0000 fps = 7 mse_loss = 2.7704 
2022-07-08 09:55:55.566947 - gail/main.py:174 - [TRPO] iter = 75000 dist_mean = 0.4187 dist_std = 0.8649 vf_loss = 0.2774 grad_norm = 0.4309 nat_grad_norm = 0.6733 cg_residual = 0.0184 step_size = 0.3755 reward = 0.0000 fps = 6 mse_loss = 2.8410 
2022-07-08 09:55:56.297704 - gail/main.py:201 - [Discriminator] iter = 75000 loss = -3.9919 grad_norm = 3.7920 grad_penalty = 0.3829 regularization = 0.0000 true_logits = 0.1629 fake_logits = -4.2118 true_prob = 0.5498 fake_prob = 0.0173 
2022-07-08 09:56:18.858569 - gail/main.py:142 - [Evaluate] iter = 75000 episode={ returns = 136.8061 lengths = 72 } discounted_episode={ returns = 135.3354 lengths = 74 } 
2022-07-08 09:56:43.427695 - gail/main.py:174 - [TRPO] iter = 76000 dist_mean = 0.3823 dist_std = 0.8675 vf_loss = 0.3474 grad_norm = 0.4586 nat_grad_norm = 0.5148 cg_residual = 0.0130 step_size = 0.4147 reward = 0.0000 fps = 21 mse_loss = 2.7629 
2022-07-08 09:57:08.078176 - gail/main.py:174 - [TRPO] iter = 77000 dist_mean = 0.3996 dist_std = 0.8676 vf_loss = 0.3020 grad_norm = 0.6971 nat_grad_norm = 0.7249 cg_residual = 0.0170 step_size = 0.3138 reward = -0.0000 fps = 13 mse_loss = 2.7879 
2022-07-08 09:57:32.818580 - gail/main.py:174 - [TRPO] iter = 78000 dist_mean = 0.3885 dist_std = 0.8654 vf_loss = 0.3387 grad_norm = 0.5437 nat_grad_norm = 0.5695 cg_residual = 0.0152 step_size = 0.3784 reward = -0.0000 fps = 10 mse_loss = 2.5379 
2022-07-08 09:57:57.630622 - gail/main.py:174 - [TRPO] iter = 79000 dist_mean = 0.3959 dist_std = 0.8639 vf_loss = 0.3204 grad_norm = 0.6549 nat_grad_norm = 0.5985 cg_residual = 0.0195 step_size = 0.3496 reward = -0.0000 fps = 8 mse_loss = 2.6074 
2022-07-08 09:58:23.042697 - gail/main.py:174 - [TRPO] iter = 80000 dist_mean = 0.3778 dist_std = 0.8605 vf_loss = 0.3614 grad_norm = 0.5861 nat_grad_norm = 0.6286 cg_residual = 0.0210 step_size = 0.3452 reward = 0.0000 fps = 6 mse_loss = 2.6136 
2022-07-08 09:58:23.764652 - gail/main.py:201 - [Discriminator] iter = 80000 loss = -4.0601 grad_norm = 3.4636 grad_penalty = 0.4341 regularization = 0.0000 true_logits = 0.1737 fake_logits = -4.3205 true_prob = 0.5527 fake_prob = 0.0155 
2022-07-08 09:58:52.035258 - gail/main.py:142 - [Evaluate] iter = 80000 episode={ returns = 165.0998 lengths = 82 } discounted_episode={ returns = 153.8501 lengths = 81 } 
2022-07-08 09:59:17.027482 - gail/main.py:174 - [TRPO] iter = 81000 dist_mean = 0.3943 dist_std = 0.8616 vf_loss = 0.3204 grad_norm = 0.5836 nat_grad_norm = 0.5104 cg_residual = 0.0143 step_size = 0.3796 reward = -0.0000 fps = 18 mse_loss = 2.8213 
2022-07-08 09:59:44.218720 - gail/main.py:174 - [TRPO] iter = 82000 dist_mean = 0.3649 dist_std = 0.8602 vf_loss = 0.2410 grad_norm = 0.5690 nat_grad_norm = 0.5801 cg_residual = 0.0206 step_size = 0.3519 reward = 0.0000 fps = 12 mse_loss = 2.6465 
2022-07-08 10:00:11.227268 - gail/main.py:174 - [TRPO] iter = 83000 dist_mean = 0.3682 dist_std = 0.8570 vf_loss = 0.2366 grad_norm = 0.4920 nat_grad_norm = 0.5763 cg_residual = 0.0172 step_size = 0.3676 reward = -0.0000 fps = 9 mse_loss = 3.0122 
2022-07-08 10:00:37.465757 - gail/main.py:174 - [TRPO] iter = 84000 dist_mean = 0.3536 dist_std = 0.8544 vf_loss = 0.3800 grad_norm = 0.5665 nat_grad_norm = 0.6331 cg_residual = 0.0207 step_size = 0.3610 reward = -0.0000 fps = 7 mse_loss = 2.6551 
2022-07-08 10:01:05.205134 - gail/main.py:174 - [TRPO] iter = 85000 dist_mean = 0.3861 dist_std = 0.8550 vf_loss = 0.2803 grad_norm = 0.6356 nat_grad_norm = 0.6278 cg_residual = 0.0243 step_size = 0.3327 reward = -0.0000 fps = 6 mse_loss = 2.5486 
2022-07-08 10:01:05.994448 - gail/main.py:201 - [Discriminator] iter = 85000 loss = -4.0153 grad_norm = 3.6276 grad_penalty = 0.3792 regularization = 0.0000 true_logits = 0.0864 fake_logits = -4.3081 true_prob = 0.5398 fake_prob = 0.0159 
2022-07-08 10:01:37.068764 - gail/main.py:142 - [Evaluate] iter = 85000 episode={ returns = 171.0220 lengths = 84 } discounted_episode={ returns = 161.7468 lengths = 83 } 
2022-07-08 10:02:13.126775 - gail/main.py:174 - [TRPO] iter = 86000 dist_mean = 0.3957 dist_std = 0.8504 vf_loss = 0.4676 grad_norm = 0.4496 nat_grad_norm = 0.6331 cg_residual = 0.0156 step_size = 0.3888 reward = -0.0000 fps = 14 mse_loss = 2.5307 
2022-07-08 10:02:42.246730 - gail/main.py:174 - [TRPO] iter = 87000 dist_mean = 0.3721 dist_std = 0.8471 vf_loss = 0.4274 grad_norm = 0.6691 nat_grad_norm = 0.5247 cg_residual = 0.0202 step_size = 0.3745 reward = 0.0000 fps = 10 mse_loss = 2.6618 
2022-07-08 10:03:12.517964 - gail/main.py:174 - [TRPO] iter = 88000 dist_mean = 0.3682 dist_std = 0.8450 vf_loss = 0.2797 grad_norm = 0.6061 nat_grad_norm = 0.5385 cg_residual = 0.0261 step_size = 0.3800 reward = -0.0000 fps = 7 mse_loss = 3.0665 
2022-07-08 10:03:41.481545 - gail/main.py:174 - [TRPO] iter = 89000 dist_mean = 0.3651 dist_std = 0.8443 vf_loss = 0.3348 grad_norm = 0.5241 nat_grad_norm = 0.5050 cg_residual = 0.0195 step_size = 0.4318 reward = 0.0000 fps = 6 mse_loss = 3.1133 
2022-07-08 10:04:11.446313 - gail/main.py:174 - [TRPO] iter = 90000 dist_mean = 0.3631 dist_std = 0.8404 vf_loss = 0.3198 grad_norm = 0.5399 nat_grad_norm = 0.5427 cg_residual = 0.0204 step_size = 0.3867 reward = -0.0000 fps = 5 mse_loss = 2.8650 
2022-07-08 10:04:12.279263 - gail/main.py:201 - [Discriminator] iter = 90000 loss = -4.1728 grad_norm = 3.6897 grad_penalty = 0.3930 regularization = 0.0000 true_logits = 0.0721 fake_logits = -4.4937 true_prob = 0.5366 fake_prob = 0.0133 
2022-07-08 10:04:45.177685 - gail/main.py:142 - [Evaluate] iter = 90000 episode={ returns = 175.8957 lengths = 85 } discounted_episode={ returns = 165.2300 lengths = 84 } 
2022-07-08 10:05:14.582215 - gail/main.py:174 - [TRPO] iter = 91000 dist_mean = 0.3674 dist_std = 0.8416 vf_loss = 0.3754 grad_norm = 0.5339 nat_grad_norm = 0.5424 cg_residual = 0.0122 step_size = 0.3652 reward = 0.0000 fps = 16 mse_loss = 2.8435 
2022-07-08 10:05:44.847593 - gail/main.py:174 - [TRPO] iter = 92000 dist_mean = 0.3613 dist_std = 0.8354 vf_loss = 0.3730 grad_norm = 0.4825 nat_grad_norm = 0.5527 cg_residual = 0.0261 step_size = 0.3782 reward = 0.0000 fps = 10 mse_loss = 3.1695 
2022-07-08 10:06:14.889216 - gail/main.py:174 - [TRPO] iter = 93000 dist_mean = 0.3627 dist_std = 0.8358 vf_loss = 0.2190 grad_norm = 0.4718 nat_grad_norm = 0.6300 cg_residual = 0.0193 step_size = 0.3581 reward = -0.0000 fps = 8 mse_loss = 3.2938 
2022-07-08 10:06:44.706412 - gail/main.py:174 - [TRPO] iter = 94000 dist_mean = 0.3697 dist_std = 0.8340 vf_loss = 0.3431 grad_norm = 0.5761 nat_grad_norm = 0.6295 cg_residual = 0.0173 step_size = 0.3514 reward = -0.0000 fps = 6 mse_loss = 3.5817 
2022-07-08 10:07:13.862155 - gail/main.py:174 - [TRPO] iter = 95000 dist_mean = 0.3507 dist_std = 0.8327 vf_loss = 0.3317 grad_norm = 0.5692 nat_grad_norm = 0.5461 cg_residual = 0.0101 step_size = 0.3892 reward = -0.0000 fps = 5 mse_loss = 3.4560 
2022-07-08 10:07:14.747218 - gail/main.py:201 - [Discriminator] iter = 95000 loss = -4.1630 grad_norm = 3.3876 grad_penalty = 0.4383 regularization = 0.0000 true_logits = 0.0837 fake_logits = -4.5175 true_prob = 0.5405 fake_prob = 0.0129 
2022-07-08 10:07:45.210223 - gail/main.py:142 - [Evaluate] iter = 95000 episode={ returns = 163.7360 lengths = 82 } discounted_episode={ returns = 153.3929 lengths = 81 } 
2022-07-08 10:08:14.064294 - gail/main.py:174 - [TRPO] iter = 96000 dist_mean = 0.3828 dist_std = 0.8345 vf_loss = 0.2852 grad_norm = 0.5771 nat_grad_norm = 0.5837 cg_residual = 0.0258 step_size = 0.3515 reward = -0.0000 fps = 16 mse_loss = 3.4387 
2022-07-08 10:08:42.944120 - gail/main.py:174 - [TRPO] iter = 97000 dist_mean = 0.3885 dist_std = 0.8365 vf_loss = 0.2468 grad_norm = 0.5762 nat_grad_norm = 0.5267 cg_residual = 0.0186 step_size = 0.3594 reward = -0.0000 fps = 11 mse_loss = 3.2878 
2022-07-08 10:09:12.962815 - gail/main.py:174 - [TRPO] iter = 98000 dist_mean = 0.3666 dist_std = 0.8332 vf_loss = 0.1999 grad_norm = 0.6019 nat_grad_norm = 0.6281 cg_residual = 0.0310 step_size = 0.3549 reward = 0.0000 fps = 8 mse_loss = 3.1300 
2022-07-08 10:09:43.464057 - gail/main.py:174 - [TRPO] iter = 99000 dist_mean = 0.3836 dist_std = 0.8313 vf_loss = 0.2585 grad_norm = 0.5014 nat_grad_norm = 0.6031 cg_residual = 0.0279 step_size = 0.3597 reward = 0.0000 fps = 6 mse_loss = 3.1972 
2022-07-08 10:10:12.387805 - gail/main.py:174 - [TRPO] iter = 100000 dist_mean = 0.3851 dist_std = 0.8291 vf_loss = 0.2069 grad_norm = 0.4602 nat_grad_norm = 0.5958 cg_residual = 0.0262 step_size = 0.3783 reward = 0.0000 fps = 5 mse_loss = 3.1580 
2022-07-08 10:10:13.228172 - gail/main.py:201 - [Discriminator] iter = 100000 loss = -4.2180 grad_norm = 3.4774 grad_penalty = 0.4027 regularization = 0.0000 true_logits = 0.0412 fake_logits = -4.5795 true_prob = 0.5310 fake_prob = 0.0123 
2022-07-08 10:10:51.369281 - gail/main.py:142 - [Evaluate] iter = 100000 episode={ returns = 203.7957 lengths = 98 } discounted_episode={ returns = 193.5462 lengths = 99 } 
2022-07-08 10:11:20.436785 - gail/main.py:174 - [TRPO] iter = 101000 dist_mean = 0.3897 dist_std = 0.8274 vf_loss = 0.2235 grad_norm = 0.6096 nat_grad_norm = 0.5604 cg_residual = 0.0157 step_size = 0.3707 reward = -0.0000 fps = 14 mse_loss = 3.0515 
2022-07-08 10:11:53.622224 - gail/main.py:174 - [TRPO] iter = 102000 dist_mean = 0.3697 dist_std = 0.8242 vf_loss = 0.2162 grad_norm = 0.6242 nat_grad_norm = 0.5187 cg_residual = 0.0200 step_size = 0.3563 reward = 0.0000 fps = 9 mse_loss = 3.0187 
2022-07-08 10:12:27.781539 - gail/main.py:174 - [TRPO] iter = 103000 dist_mean = 0.3866 dist_std = 0.8247 vf_loss = 0.3550 grad_norm = 0.6392 nat_grad_norm = 0.6393 cg_residual = 0.0253 step_size = 0.3266 reward = 0.0000 fps = 7 mse_loss = 3.0201 
2022-07-08 10:12:56.760455 - gail/main.py:174 - [TRPO] iter = 104000 dist_mean = 0.3824 dist_std = 0.8203 vf_loss = 0.2669 grad_norm = 0.5069 nat_grad_norm = 0.5231 cg_residual = 0.0185 step_size = 0.3995 reward = 0.0000 fps = 6 mse_loss = 2.8890 
2022-07-08 10:13:24.738294 - gail/main.py:174 - [TRPO] iter = 105000 dist_mean = 0.3749 dist_std = 0.8186 vf_loss = 0.2051 grad_norm = 0.6377 nat_grad_norm = 0.4761 cg_residual = 0.0184 step_size = 0.3803 reward = -0.0000 fps = 5 mse_loss = 2.7526 
2022-07-08 10:13:25.599603 - gail/main.py:201 - [Discriminator] iter = 105000 loss = -4.2203 grad_norm = 3.0321 grad_penalty = 0.4167 regularization = 0.0000 true_logits = 0.0536 fake_logits = -4.5834 true_prob = 0.5284 fake_prob = 0.0119 
2022-07-08 10:14:07.067082 - gail/main.py:142 - [Evaluate] iter = 105000 episode={ returns = 217.5452 lengths = 104 } discounted_episode={ returns = 207.8049 lengths = 106 } 
2022-07-08 10:14:36.216696 - gail/main.py:174 - [TRPO] iter = 106000 dist_mean = 0.3839 dist_std = 0.8153 vf_loss = 0.1963 grad_norm = 0.6097 nat_grad_norm = 0.5466 cg_residual = 0.0162 step_size = 0.3576 reward = 0.0000 fps = 14 mse_loss = 3.0136 
2022-07-08 10:15:05.780286 - gail/main.py:174 - [TRPO] iter = 107000 dist_mean = 0.3762 dist_std = 0.8112 vf_loss = 0.2606 grad_norm = 0.5647 nat_grad_norm = 0.4821 cg_residual = 0.0154 step_size = 0.3664 reward = 0.0000 fps = 9 mse_loss = 3.0520 
2022-07-08 10:15:35.130579 - gail/main.py:174 - [TRPO] iter = 108000 dist_mean = 0.3723 dist_std = 0.8070 vf_loss = 0.1971 grad_norm = 0.6167 nat_grad_norm = 0.5104 cg_residual = 0.0270 step_size = 0.3639 reward = -0.0000 fps = 7 mse_loss = 2.8441 
2022-07-08 10:16:03.830581 - gail/main.py:174 - [TRPO] iter = 109000 dist_mean = 0.3832 dist_std = 0.8076 vf_loss = 0.2157 grad_norm = 0.6007 nat_grad_norm = 0.4751 cg_residual = 0.0171 step_size = 0.3884 reward = 0.0000 fps = 6 mse_loss = 2.6979 
2022-07-08 10:16:32.961827 - gail/main.py:174 - [TRPO] iter = 110000 dist_mean = 0.3747 dist_std = 0.8044 vf_loss = 0.1642 grad_norm = 0.5643 nat_grad_norm = 0.4738 cg_residual = 0.0225 step_size = 0.3923 reward = 0.0000 fps = 5 mse_loss = 2.7511 
2022-07-08 10:16:33.743439 - gail/main.py:201 - [Discriminator] iter = 110000 loss = -4.2562 grad_norm = 3.0385 grad_penalty = 0.4175 regularization = 0.0000 true_logits = 0.1123 fake_logits = -4.5614 true_prob = 0.5420 fake_prob = 0.0124 
2022-07-08 10:17:14.008465 - gail/main.py:142 - [Evaluate] iter = 110000 episode={ returns = 213.4301 lengths = 101 } discounted_episode={ returns = 203.5067 lengths = 103 } 
2022-07-08 10:17:42.478983 - gail/main.py:174 - [TRPO] iter = 111000 dist_mean = 0.3971 dist_std = 0.7994 vf_loss = 0.2384 grad_norm = 0.5755 nat_grad_norm = 0.5060 cg_residual = 0.0313 step_size = 0.3602 reward = 0.0000 fps = 14 mse_loss = 2.9280 
2022-07-08 10:18:11.048595 - gail/main.py:174 - [TRPO] iter = 112000 dist_mean = 0.4060 dist_std = 0.8010 vf_loss = 0.2287 grad_norm = 0.5319 nat_grad_norm = 0.5322 cg_residual = 0.0267 step_size = 0.3897 reward = 0.0000 fps = 10 mse_loss = 2.9872 
2022-07-08 10:18:38.351965 - gail/main.py:174 - [TRPO] iter = 113000 dist_mean = 0.3932 dist_std = 0.7951 vf_loss = 0.1682 grad_norm = 0.3902 nat_grad_norm = 0.4213 cg_residual = 0.0153 step_size = 0.4647 reward = 0.0000 fps = 8 mse_loss = 3.0019 
2022-07-08 10:19:04.910361 - gail/main.py:174 - [TRPO] iter = 114000 dist_mean = 0.3878 dist_std = 0.7922 vf_loss = 0.1897 grad_norm = 0.5890 nat_grad_norm = 0.5406 cg_residual = 0.0342 step_size = 0.3726 reward = -0.0000 fps = 6 mse_loss = 3.0248 
2022-07-08 10:19:33.713663 - gail/main.py:174 - [TRPO] iter = 115000 dist_mean = 0.3856 dist_std = 0.7894 vf_loss = 0.2516 grad_norm = 0.6271 nat_grad_norm = 0.5653 cg_residual = 0.0362 step_size = 0.3659 reward = 0.0000 fps = 5 mse_loss = 3.1340 
2022-07-08 10:19:34.425518 - gail/main.py:201 - [Discriminator] iter = 115000 loss = -4.2242 grad_norm = 2.6463 grad_penalty = 0.4013 regularization = 0.0000 true_logits = 0.1034 fake_logits = -4.5222 true_prob = 0.5453 fake_prob = 0.0130 
2022-07-08 10:20:18.579673 - gail/main.py:142 - [Evaluate] iter = 115000 episode={ returns = 237.6382 lengths = 112 } discounted_episode={ returns = 223.7269 lengths = 113 } 
2022-07-08 10:20:48.600164 - gail/main.py:174 - [TRPO] iter = 116000 dist_mean = 0.3947 dist_std = 0.7897 vf_loss = 0.2501 grad_norm = 0.5286 nat_grad_norm = 0.4760 cg_residual = 0.0205 step_size = 0.3844 reward = -0.0000 fps = 13 mse_loss = 3.1660 
2022-07-08 10:21:18.003040 - gail/main.py:174 - [TRPO] iter = 117000 dist_mean = 0.3863 dist_std = 0.7834 vf_loss = 0.1981 grad_norm = 0.5628 nat_grad_norm = 0.5122 cg_residual = 0.0313 step_size = 0.3844 reward = 0.0000 fps = 9 mse_loss = 3.1780 
2022-07-08 10:21:51.989146 - gail/main.py:174 - [TRPO] iter = 118000 dist_mean = 0.3844 dist_std = 0.7821 vf_loss = 0.2198 grad_norm = 0.6464 nat_grad_norm = 0.4857 cg_residual = 0.0214 step_size = 0.4081 reward = 0.0000 fps = 7 mse_loss = 3.0939 
2022-07-08 10:22:19.261169 - gail/main.py:174 - [TRPO] iter = 119000 dist_mean = 0.3975 dist_std = 0.7772 vf_loss = 0.2008 grad_norm = 0.6614 nat_grad_norm = 0.4507 cg_residual = 0.0285 step_size = 0.3635 reward = -0.0000 fps = 6 mse_loss = 3.1450 
2022-07-08 10:22:47.853729 - gail/main.py:174 - [TRPO] iter = 120000 dist_mean = 0.3874 dist_std = 0.7757 vf_loss = 0.2024 grad_norm = 0.5508 nat_grad_norm = 0.5169 cg_residual = 0.0199 step_size = 0.3902 reward = -0.0000 fps = 5 mse_loss = 3.2894 
2022-07-08 10:22:48.759463 - gail/main.py:201 - [Discriminator] iter = 120000 loss = -4.2696 grad_norm = 3.4501 grad_penalty = 0.4135 regularization = 0.0000 true_logits = 0.1453 fake_logits = -4.5378 true_prob = 0.5500 fake_prob = 0.0125 
2022-07-08 10:23:30.807054 - gail/main.py:142 - [Evaluate] iter = 120000 episode={ returns = 231.4398 lengths = 109 } discounted_episode={ returns = 220.2825 lengths = 110 } 
2022-07-08 10:23:59.814626 - gail/main.py:174 - [TRPO] iter = 121000 dist_mean = 0.4101 dist_std = 0.7753 vf_loss = 0.2555 grad_norm = 0.6486 nat_grad_norm = 0.4826 cg_residual = 0.0174 step_size = 0.3756 reward = -0.0000 fps = 14 mse_loss = 3.0848 
2022-07-08 10:24:28.869330 - gail/main.py:174 - [TRPO] iter = 122000 dist_mean = 0.4107 dist_std = 0.7696 vf_loss = 0.2040 grad_norm = 0.6191 nat_grad_norm = 0.4967 cg_residual = 0.0288 step_size = 0.3769 reward = 0.0000 fps = 9 mse_loss = 3.3645 
2022-07-08 10:24:57.629452 - gail/main.py:174 - [TRPO] iter = 123000 dist_mean = 0.4114 dist_std = 0.7667 vf_loss = 0.2871 grad_norm = 0.5715 nat_grad_norm = 0.5005 cg_residual = 0.0216 step_size = 0.3608 reward = -0.0000 fps = 7 mse_loss = 3.2863 
2022-07-08 10:25:26.257656 - gail/main.py:174 - [TRPO] iter = 124000 dist_mean = 0.3958 dist_std = 0.7641 vf_loss = 0.1801 grad_norm = 0.6470 nat_grad_norm = 0.4740 cg_residual = 0.0248 step_size = 0.4050 reward = -0.0000 fps = 6 mse_loss = 3.3426 
2022-07-08 10:25:54.172267 - gail/main.py:174 - [TRPO] iter = 125000 dist_mean = 0.3935 dist_std = 0.7576 vf_loss = 0.2563 grad_norm = 0.6306 nat_grad_norm = 0.4622 cg_residual = 0.0326 step_size = 0.3658 reward = -0.0000 fps = 5 mse_loss = 3.5860 
2022-07-08 10:25:54.983269 - gail/main.py:201 - [Discriminator] iter = 125000 loss = -4.1877 grad_norm = 2.9040 grad_penalty = 0.3872 regularization = 0.0000 true_logits = 0.1209 fake_logits = -4.4540 true_prob = 0.5433 fake_prob = 0.0138 
2022-07-08 10:26:41.113409 - gail/main.py:142 - [Evaluate] iter = 125000 episode={ returns = 241.8627 lengths = 113 } discounted_episode={ returns = 225.9562 lengths = 113 } 
2022-07-08 10:27:10.887304 - gail/main.py:174 - [TRPO] iter = 126000 dist_mean = 0.3932 dist_std = 0.7566 vf_loss = 0.2366 grad_norm = 0.5980 nat_grad_norm = 0.4798 cg_residual = 0.0258 step_size = 0.4076 reward = -0.0000 fps = 13 mse_loss = 3.5301 
2022-07-08 10:27:39.244854 - gail/main.py:174 - [TRPO] iter = 127000 dist_mean = 0.3850 dist_std = 0.7566 vf_loss = 0.2691 grad_norm = 0.4998 nat_grad_norm = 0.4630 cg_residual = 0.0267 step_size = 0.3797 reward = -0.0000 fps = 9 mse_loss = 3.2707 
2022-07-08 10:28:07.903452 - gail/main.py:174 - [TRPO] iter = 128000 dist_mean = 0.3885 dist_std = 0.7558 vf_loss = 0.2319 grad_norm = 0.7492 nat_grad_norm = 0.4310 cg_residual = 0.0259 step_size = 0.3991 reward = -0.0000 fps = 7 mse_loss = 3.4096 
2022-07-08 10:28:37.050050 - gail/main.py:174 - [TRPO] iter = 129000 dist_mean = 0.3960 dist_std = 0.7509 vf_loss = 0.2192 grad_norm = 0.6852 nat_grad_norm = 0.4717 cg_residual = 0.0213 step_size = 0.3877 reward = -0.0000 fps = 6 mse_loss = 3.1741 
2022-07-08 10:29:06.715356 - gail/main.py:174 - [TRPO] iter = 130000 dist_mean = 0.4065 dist_std = 0.7517 vf_loss = 0.4089 grad_norm = 0.4807 nat_grad_norm = 0.5813 cg_residual = 0.0590 step_size = 0.4073 reward = -0.0000 fps = 5 mse_loss = 3.5435 
2022-07-08 10:29:07.442451 - gail/main.py:201 - [Discriminator] iter = 130000 loss = -4.1987 grad_norm = 3.5065 grad_penalty = 0.3920 regularization = 0.0000 true_logits = 0.1274 fake_logits = -4.4634 true_prob = 0.5482 fake_prob = 0.0142 
2022-07-08 10:29:54.316226 - gail/main.py:142 - [Evaluate] iter = 130000 episode={ returns = 254.1378 lengths = 116 } discounted_episode={ returns = 226.3574 lengths = 110 } 
2022-07-08 10:30:24.124608 - gail/main.py:174 - [TRPO] iter = 131000 dist_mean = 0.4024 dist_std = 0.7503 vf_loss = 0.3485 grad_norm = 0.5962 nat_grad_norm = 0.4686 cg_residual = 0.0503 step_size = 0.4011 reward = 0.0000 fps = 13 mse_loss = 3.3403 
2022-07-08 10:30:54.362771 - gail/main.py:174 - [TRPO] iter = 132000 dist_mean = 0.3855 dist_std = 0.7480 vf_loss = 0.2211 grad_norm = 0.7497 nat_grad_norm = 0.5594 cg_residual = 0.0569 step_size = 0.3417 reward = -0.0000 fps = 9 mse_loss = 3.6713 
2022-07-08 10:31:25.349507 - gail/main.py:174 - [TRPO] iter = 133000 dist_mean = 0.3869 dist_std = 0.7459 vf_loss = 0.2190 grad_norm = 0.7524 nat_grad_norm = 0.4758 cg_residual = 0.0647 step_size = 0.3865 reward = 0.0000 fps = 7 mse_loss = 3.5379 
2022-07-08 10:32:00.646767 - gail/main.py:174 - [TRPO] iter = 134000 dist_mean = 0.4166 dist_std = 0.7423 vf_loss = 0.3473 grad_norm = 0.6990 nat_grad_norm = 0.5194 cg_residual = 0.0363 step_size = 0.3618 reward = 0.0000 fps = 5 mse_loss = 3.7105 
