2022-07-08 08:45:05.492543 - utils/flags.py:257 - log_dir = logs/gail_w-Walker2d-v2-200-2022-07-08-08-45-05
2022-07-08 08:46:24.767432 - gail/utils/replay_buffer.py:86 - Load dataset from /content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/Walker2d-v2
2022-07-08 08:46:33.270644 - gail/main.py:80 - Expert Reward 5150.674112
2022-07-08 08:46:33.676411 - gail/main.py:84 - Original dataset size 3000
2022-07-08 08:46:33.719479 - gail/main.py:86 - Subsampled dataset size 3000
2022-07-08 08:46:33.721393 - gail/main.py:87 - np random: 864 random : 786
2022-07-08 08:46:33.723308 - gail/main.py:91 - Sampled obs: 0.0531, acs: 0.2269
2022-07-08 08:46:34.761704 - lunzi/nn/flat_param.py:17 - Enabling flattening... ['GaussianMLPPolicy_1/log_std:0', 'GaussianMLPPolicy_1/Variable:0', 'GaussianMLPPolicy_1/Variable_1:0', 'GaussianMLPPolicy_1/Variable_2:0', 'GaussianMLPPolicy_1/Variable_3:0', 'GaussianMLPPolicy_1/Variable_4:0', 'GaussianMLPPolicy_1/Variable_5:0']
2022-07-08 08:46:42.006460 - gail/discriminator/discriminator.py:30 - Use predefined normalization.
2022-07-08 08:46:42.014791 - gail/discriminator/discriminator.py:33 - Normalizer loc:[[ 1.2194959e+00  2.4445076e-01 -7.8132987e-02 -2.6673764e-01
   1.8222688e-01 -9.5077172e-02 -3.3649772e-01  5.3370733e-02
   4.1614923e+00  4.1431887e-03  3.8142569e-02 -2.6013174e-03
  -1.0202496e-02  5.6982285e-01  2.9836079e-02 -1.5763690e-01
   1.7689442e-02]] 
 scale:[[0.06687175 0.23681822 0.23042987 0.33821535 0.664349   0.20301929
  0.42807332 0.7138035  0.986894   0.65049744 2.0363257  2.3816926
  3.7250905  6.026913   2.0511289  4.406521   6.1475325 ]]
2022-07-08 08:46:45.567051 - gail/discriminator/discriminator.py:67 - Discriminator uses Wasserstein distance.
2022-07-08 08:46:45.569257 - gail/discriminator/discriminator.py:68 - [<tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable:0' shape=(23, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_1:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_2:0' shape=(100, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_3:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_4:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_5:0' shape=(1,) dtype=float32_ref>]
2022-07-08 08:46:45.569768 - gail/discriminator/discriminator.py:69 - Use gradient penalty regularization (coef = 10.000000)
2022-07-08 08:46:46.435969 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions_mean]
2022-07-08 08:47:01.678847 - gail/main.py:142 - [Evaluate] iter = 0 episode={ returns = 113.4058 lengths = 135 } discounted_episode={ returns = 135.8490 lengths = 156 } 
2022-07-08 08:47:01.683888 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions]
2022-07-08 08:47:12.406780 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [fake_states fake_actions] => [scaled_neural_reward]
2022-07-08 08:47:12.706405 - lunzi/nn/module.py:71 - [MLPVFunction] is making TensorFlow callables, key = [states] => [values]
2022-07-08 08:47:13.287278 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [] => [sync_old]
2022-07-08 08:47:13.612113 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss flat_grad dist_std mean_kl dist_mean]
2022-07-08 08:47:15.238283 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states tangents actions] => [hessian_vec_prod]
2022-07-08 08:47:18.067910 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [] => [get_flat]
2022-07-08 08:47:18.404026 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [feed_flat] => [set_flat]
2022-07-08 08:47:18.801168 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss mean_kl]
2022-07-08 08:47:19.414776 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [train_vf vf_loss]
2022-07-08 08:47:20.289676 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [vf_loss]
2022-07-08 08:47:20.593876 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states actions_] => [mse_loss]
2022-07-08 08:47:20.909318 - gail/main.py:174 - [TRPO] iter = 1000 dist_mean = 0.0000 dist_std = 1.0000 vf_loss = 0.2127 grad_norm = 0.3176 nat_grad_norm = 0.3986 cg_residual = 0.0000 step_size = 0.5038 reward = 0.0000 fps = 29 mse_loss = 0.4380 
2022-07-08 08:47:28.536131 - gail/main.py:174 - [TRPO] iter = 2000 dist_mean = -0.0148 dist_std = 1.0010 vf_loss = 0.3019 grad_norm = 0.5345 nat_grad_norm = 0.4888 cg_residual = 0.0000 step_size = 0.3417 reward = -0.0000 fps = 23 mse_loss = 0.4759 
2022-07-08 08:47:36.066042 - gail/main.py:174 - [TRPO] iter = 3000 dist_mean = -0.0348 dist_std = 1.0057 vf_loss = 0.1878 grad_norm = 0.5650 nat_grad_norm = 0.4909 cg_residual = 0.0000 step_size = 0.3328 reward = -0.0000 fps = 20 mse_loss = 0.5481 
2022-07-08 08:47:43.076121 - gail/main.py:174 - [TRPO] iter = 4000 dist_mean = -0.0429 dist_std = 1.0060 vf_loss = 0.1870 grad_norm = 0.4669 nat_grad_norm = 0.5075 cg_residual = 0.0000 step_size = 0.3714 reward = 0.0000 fps = 17 mse_loss = 0.6285 
2022-07-08 08:47:50.565746 - gail/main.py:174 - [TRPO] iter = 5000 dist_mean = -0.0506 dist_std = 1.0078 vf_loss = 0.1613 grad_norm = 0.5281 nat_grad_norm = 0.4704 cg_residual = 0.0001 step_size = 0.3728 reward = -0.0000 fps = 15 mse_loss = 0.6875 
2022-07-08 08:47:50.567559 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [true_states true_actions fake_states fake_actions true_masks] => [train loss true_logits fake_logits true_prob fake_prob grad_norm grad_penalty regularization]
2022-07-08 08:47:52.779863 - gail/main.py:201 - [Discriminator] iter = 5000 loss = 1.4121 grad_norm = 9.8276 grad_penalty = 1.0169 regularization = 0.0000 true_logits = 0.3014 fake_logits = 0.6966 true_prob = 0.5739 fake_prob = 0.6630 
2022-07-08 08:47:55.214006 - gail/main.py:142 - [Evaluate] iter = 5000 episode={ returns = 0.5522 lengths = 26 } discounted_episode={ returns = 0.2826 lengths = 26 } 
2022-07-08 08:48:02.233177 - gail/main.py:174 - [TRPO] iter = 6000 dist_mean = -0.0595 dist_std = 1.0085 vf_loss = 0.3100 grad_norm = 0.4358 nat_grad_norm = 0.4758 cg_residual = 0.0001 step_size = 0.4007 reward = 0.0000 fps = 105 mse_loss = 0.6795 
2022-07-08 08:48:08.946473 - gail/main.py:174 - [TRPO] iter = 7000 dist_mean = -0.0571 dist_std = 1.0104 vf_loss = 0.4002 grad_norm = 0.4682 nat_grad_norm = 0.4719 cg_residual = 0.0001 step_size = 0.3677 reward = -0.0000 fps = 61 mse_loss = 0.6821 
2022-07-08 08:48:16.205777 - gail/main.py:174 - [TRPO] iter = 8000 dist_mean = -0.0578 dist_std = 1.0098 vf_loss = 0.3454 grad_norm = 0.4779 nat_grad_norm = 0.4845 cg_residual = 0.0002 step_size = 0.3702 reward = 0.0000 fps = 42 mse_loss = 0.6815 
2022-07-08 08:48:23.653935 - gail/main.py:174 - [TRPO] iter = 9000 dist_mean = -0.0432 dist_std = 1.0079 vf_loss = 0.3692 grad_norm = 0.4299 nat_grad_norm = 0.4747 cg_residual = 0.0002 step_size = 0.4066 reward = 0.0000 fps = 32 mse_loss = 0.6830 
2022-07-08 08:48:31.344849 - gail/main.py:174 - [TRPO] iter = 10000 dist_mean = -0.0477 dist_std = 1.0020 vf_loss = 0.3104 grad_norm = 0.5218 nat_grad_norm = 0.4771 cg_residual = 0.0001 step_size = 0.3738 reward = -0.0000 fps = 25 mse_loss = 0.7671 
2022-07-08 08:48:31.563346 - gail/main.py:201 - [Discriminator] iter = 10000 loss = 0.7476 grad_norm = 7.4934 grad_penalty = 0.6376 regularization = 0.0000 true_logits = 0.3380 fake_logits = 0.4480 true_prob = 0.5823 fake_prob = 0.6059 
2022-07-08 08:48:33.972559 - gail/main.py:142 - [Evaluate] iter = 10000 episode={ returns = -0.5345 lengths = 24 } discounted_episode={ returns = -0.7446 lengths = 24 } 
2022-07-08 08:48:41.201479 - gail/main.py:174 - [TRPO] iter = 11000 dist_mean = -0.0247 dist_std = 1.0060 vf_loss = 0.6413 grad_norm = 0.4589 nat_grad_norm = 0.5094 cg_residual = 0.0001 step_size = 0.3757 reward = -0.0000 fps = 103 mse_loss = 0.7846 
2022-07-08 08:48:48.997375 - gail/main.py:174 - [TRPO] iter = 12000 dist_mean = -0.0201 dist_std = 1.0071 vf_loss = 0.6034 grad_norm = 0.4672 nat_grad_norm = 0.5023 cg_residual = 0.0002 step_size = 0.3789 reward = -0.0000 fps = 57 mse_loss = 0.8191 
2022-07-08 08:48:57.168507 - gail/main.py:174 - [TRPO] iter = 13000 dist_mean = -0.0128 dist_std = 1.0010 vf_loss = 0.4867 grad_norm = 0.4440 nat_grad_norm = 0.5169 cg_residual = 0.0001 step_size = 0.3828 reward = -0.0000 fps = 39 mse_loss = 0.8075 
2022-07-08 08:49:05.166575 - gail/main.py:174 - [TRPO] iter = 14000 dist_mean = 0.0193 dist_std = 0.9993 vf_loss = 0.5215 grad_norm = 0.4364 nat_grad_norm = 0.5285 cg_residual = 0.0002 step_size = 0.4072 reward = 0.0000 fps = 29 mse_loss = 0.7674 
2022-07-08 08:49:12.897575 - gail/main.py:174 - [TRPO] iter = 15000 dist_mean = 0.0343 dist_std = 0.9990 vf_loss = 0.4964 grad_norm = 0.3391 nat_grad_norm = 0.6331 cg_residual = 0.0004 step_size = 0.3883 reward = 0.0000 fps = 24 mse_loss = 0.7814 
2022-07-08 08:49:13.140343 - gail/main.py:201 - [Discriminator] iter = 15000 loss = 0.4732 grad_norm = 6.6826 grad_penalty = 0.5586 regularization = 0.0000 true_logits = 0.3724 fake_logits = 0.2870 true_prob = 0.5901 fake_prob = 0.5679 
2022-07-08 08:49:17.981074 - gail/main.py:142 - [Evaluate] iter = 15000 episode={ returns = -6.3059 lengths = 53 } discounted_episode={ returns = -6.1579 lengths = 53 } 
2022-07-08 08:49:25.428850 - gail/main.py:174 - [TRPO] iter = 16000 dist_mean = 0.0207 dist_std = 0.9953 vf_loss = 0.5158 grad_norm = 0.4636 nat_grad_norm = 0.6354 cg_residual = 0.0003 step_size = 0.3420 reward = 0.0000 fps = 81 mse_loss = 0.8440 
2022-07-08 08:49:32.697846 - gail/main.py:174 - [TRPO] iter = 17000 dist_mean = 0.0293 dist_std = 0.9932 vf_loss = 0.4927 grad_norm = 0.4326 nat_grad_norm = 0.5414 cg_residual = 0.0003 step_size = 0.4098 reward = 0.0000 fps = 51 mse_loss = 0.8344 
2022-07-08 08:49:40.007509 - gail/main.py:174 - [TRPO] iter = 18000 dist_mean = 0.0529 dist_std = 0.9890 vf_loss = 0.5382 grad_norm = 0.4347 nat_grad_norm = 0.5970 cg_residual = 0.0004 step_size = 0.3815 reward = -0.0000 fps = 37 mse_loss = 0.8124 
2022-07-08 08:49:47.600115 - gail/main.py:174 - [TRPO] iter = 19000 dist_mean = 0.0611 dist_std = 0.9899 vf_loss = 0.3538 grad_norm = 0.3922 nat_grad_norm = 0.5491 cg_residual = 0.0003 step_size = 0.4062 reward = -0.0000 fps = 29 mse_loss = 0.8226 
2022-07-08 08:49:55.119014 - gail/main.py:174 - [TRPO] iter = 20000 dist_mean = 0.0638 dist_std = 0.9921 vf_loss = 0.3143 grad_norm = 0.4093 nat_grad_norm = 0.6924 cg_residual = 0.0009 step_size = 0.3587 reward = 0.0000 fps = 23 mse_loss = 0.8542 
2022-07-08 08:49:55.365859 - gail/main.py:201 - [Discriminator] iter = 20000 loss = -0.0558 grad_norm = 6.1154 grad_penalty = 0.4229 regularization = 0.0000 true_logits = 0.4090 fake_logits = -0.0698 true_prob = 0.5979 fake_prob = 0.4851 
2022-07-08 08:49:59.950762 - gail/main.py:142 - [Evaluate] iter = 20000 episode={ returns = -4.0447 lengths = 45 } discounted_episode={ returns = -4.3344 lengths = 46 } 
2022-07-08 08:50:12.891401 - gail/main.py:174 - [TRPO] iter = 21000 dist_mean = 0.0607 dist_std = 0.9961 vf_loss = 0.5308 grad_norm = 0.3524 nat_grad_norm = 0.5610 cg_residual = 0.0004 step_size = 0.4229 reward = 0.0000 fps = 57 mse_loss = 0.8047 
2022-07-08 08:50:20.191616 - gail/main.py:174 - [TRPO] iter = 22000 dist_mean = 0.0775 dist_std = 0.9976 vf_loss = 0.3949 grad_norm = 0.3932 nat_grad_norm = 0.8176 cg_residual = 0.0008 step_size = 0.3378 reward = -0.0000 fps = 40 mse_loss = 0.8143 
2022-07-08 08:50:27.927673 - gail/main.py:174 - [TRPO] iter = 23000 dist_mean = 0.0764 dist_std = 0.9965 vf_loss = 0.3121 grad_norm = 0.4510 nat_grad_norm = 0.6182 cg_residual = 0.0007 step_size = 0.3569 reward = -0.0000 fps = 30 mse_loss = 0.7806 
2022-07-08 08:50:35.662948 - gail/main.py:174 - [TRPO] iter = 24000 dist_mean = 0.0641 dist_std = 1.0050 vf_loss = 0.2684 grad_norm = 0.3781 nat_grad_norm = 0.5631 cg_residual = 0.0005 step_size = 0.4362 reward = 0.0000 fps = 24 mse_loss = 0.7748 
2022-07-08 08:50:42.914497 - gail/main.py:174 - [TRPO] iter = 25000 dist_mean = 0.0575 dist_std = 1.0038 vf_loss = 0.3993 grad_norm = 0.4084 nat_grad_norm = 0.5325 cg_residual = 0.0003 step_size = 0.4159 reward = 0.0000 fps = 21 mse_loss = 0.7954 
2022-07-08 08:50:43.146931 - gail/main.py:201 - [Discriminator] iter = 25000 loss = -0.6814 grad_norm = 6.0971 grad_penalty = 0.4262 regularization = 0.0000 true_logits = 0.4051 fake_logits = -0.7025 true_prob = 0.5973 fake_prob = 0.3458 
2022-07-08 08:50:47.514822 - gail/main.py:142 - [Evaluate] iter = 25000 episode={ returns = -4.1848 lengths = 46 } discounted_episode={ returns = -4.0676 lengths = 46 } 
2022-07-08 08:50:54.619138 - gail/main.py:174 - [TRPO] iter = 26000 dist_mean = 0.0708 dist_std = 0.9952 vf_loss = 0.2992 grad_norm = 0.3350 nat_grad_norm = 0.5684 cg_residual = 0.0006 step_size = 0.4386 reward = -0.0000 fps = 87 mse_loss = 0.8095 
2022-07-08 08:51:02.605619 - gail/main.py:174 - [TRPO] iter = 27000 dist_mean = 0.0808 dist_std = 0.9876 vf_loss = 0.6416 grad_norm = 0.5263 nat_grad_norm = 0.6126 cg_residual = 0.0004 step_size = 0.3557 reward = 0.0000 fps = 51 mse_loss = 0.8210 
2022-07-08 08:51:10.028479 - gail/main.py:174 - [TRPO] iter = 28000 dist_mean = 0.0750 dist_std = 0.9846 vf_loss = 0.4300 grad_norm = 0.4662 nat_grad_norm = 0.5532 cg_residual = 0.0005 step_size = 0.3959 reward = 0.0000 fps = 37 mse_loss = 0.8766 
2022-07-08 08:51:17.365069 - gail/main.py:174 - [TRPO] iter = 29000 dist_mean = 0.0496 dist_std = 0.9820 vf_loss = 0.2634 grad_norm = 0.3508 nat_grad_norm = 0.5377 cg_residual = 0.0004 step_size = 0.4324 reward = -0.0000 fps = 29 mse_loss = 0.8969 
2022-07-08 08:51:24.720073 - gail/main.py:174 - [TRPO] iter = 30000 dist_mean = 0.0837 dist_std = 0.9816 vf_loss = 0.6232 grad_norm = 0.4092 nat_grad_norm = 0.5376 cg_residual = 0.0006 step_size = 0.4242 reward = -0.0000 fps = 24 mse_loss = 0.9464 
2022-07-08 08:51:24.969983 - gail/main.py:201 - [Discriminator] iter = 30000 loss = -1.2253 grad_norm = 6.1716 grad_penalty = 0.3971 regularization = 0.0000 true_logits = 0.4149 fake_logits = -1.2074 true_prob = 0.5997 fake_prob = 0.2488 
2022-07-08 08:51:28.926272 - gail/main.py:142 - [Evaluate] iter = 30000 episode={ returns = -5.3251 lengths = 43 } discounted_episode={ returns = -4.8255 lengths = 43 } 
2022-07-08 08:51:36.272270 - gail/main.py:174 - [TRPO] iter = 31000 dist_mean = 0.0808 dist_std = 0.9832 vf_loss = 0.4438 grad_norm = 0.4137 nat_grad_norm = 0.6252 cg_residual = 0.0008 step_size = 0.3814 reward = -0.0000 fps = 88 mse_loss = 1.0183 
2022-07-08 08:51:43.071655 - gail/main.py:174 - [TRPO] iter = 32000 dist_mean = 0.0765 dist_std = 0.9853 vf_loss = 1.2024 grad_norm = 0.4307 nat_grad_norm = 0.6430 cg_residual = 0.0011 step_size = 0.3620 reward = -0.0000 fps = 55 mse_loss = 0.9503 
2022-07-08 08:51:50.584055 - gail/main.py:174 - [TRPO] iter = 33000 dist_mean = 0.0851 dist_std = 0.9777 vf_loss = 0.3926 grad_norm = 0.3696 nat_grad_norm = 0.5513 cg_residual = 0.0006 step_size = 0.4381 reward = -0.0000 fps = 39 mse_loss = 0.9473 
2022-07-08 08:51:58.022255 - gail/main.py:174 - [TRPO] iter = 34000 dist_mean = 0.0902 dist_std = 0.9760 vf_loss = 0.7552 grad_norm = 0.4280 nat_grad_norm = 0.5816 cg_residual = 0.0012 step_size = 0.3750 reward = 0.0000 fps = 30 mse_loss = 1.0080 
2022-07-08 08:52:05.452686 - gail/main.py:174 - [TRPO] iter = 35000 dist_mean = 0.0716 dist_std = 0.9777 vf_loss = 0.5485 grad_norm = 0.3812 nat_grad_norm = 0.6887 cg_residual = 0.0008 step_size = 0.3734 reward = -0.0000 fps = 24 mse_loss = 0.9314 
2022-07-08 08:52:05.725299 - gail/main.py:201 - [Discriminator] iter = 35000 loss = -1.9746 grad_norm = 6.8495 grad_penalty = 0.4146 regularization = 0.0000 true_logits = 0.4951 fake_logits = -1.8941 true_prob = 0.6183 fake_prob = 0.1569 
2022-07-08 08:52:12.625924 - gail/main.py:142 - [Evaluate] iter = 35000 episode={ returns = 1.0737 lengths = 75 } discounted_episode={ returns = 0.4192 lengths = 76 } 
2022-07-08 08:52:19.540072 - gail/main.py:174 - [TRPO] iter = 36000 dist_mean = 0.0841 dist_std = 0.9776 vf_loss = 0.5749 grad_norm = 0.4295 nat_grad_norm = 0.5811 cg_residual = 0.0007 step_size = 0.3955 reward = -0.0000 fps = 72 mse_loss = 0.9696 
2022-07-08 08:52:26.860133 - gail/main.py:174 - [TRPO] iter = 37000 dist_mean = 0.0804 dist_std = 0.9828 vf_loss = 0.4274 grad_norm = 0.3948 nat_grad_norm = 0.5591 cg_residual = 0.0012 step_size = 0.4050 reward = -0.0000 fps = 47 mse_loss = 1.0068 
2022-07-08 08:52:33.714917 - gail/main.py:174 - [TRPO] iter = 38000 dist_mean = 0.1322 dist_std = 0.9731 vf_loss = 0.9000 grad_norm = 0.3682 nat_grad_norm = 0.5828 cg_residual = 0.0012 step_size = 0.4010 reward = -0.0000 fps = 35 mse_loss = 0.9440 
2022-07-08 08:52:40.937724 - gail/main.py:174 - [TRPO] iter = 39000 dist_mean = 0.1124 dist_std = 0.9731 vf_loss = 0.7508 grad_norm = 0.3424 nat_grad_norm = 0.6417 cg_residual = 0.0012 step_size = 0.4292 reward = -0.0000 fps = 28 mse_loss = 0.9167 
2022-07-08 08:52:47.699670 - gail/main.py:174 - [TRPO] iter = 40000 dist_mean = 0.0933 dist_std = 0.9726 vf_loss = 0.4449 grad_norm = 0.4652 nat_grad_norm = 0.6734 cg_residual = 0.0010 step_size = 0.3512 reward = -0.0000 fps = 23 mse_loss = 0.8471 
2022-07-08 08:52:47.942470 - gail/main.py:201 - [Discriminator] iter = 40000 loss = -2.5910 grad_norm = 7.0262 grad_penalty = 0.4180 regularization = 0.0000 true_logits = 0.5172 fake_logits = -2.4918 true_prob = 0.6239 fake_prob = 0.1018 
2022-07-08 08:52:57.714977 - gail/main.py:142 - [Evaluate] iter = 40000 episode={ returns = 13.6172 lengths = 103 } discounted_episode={ returns = 10.1206 lengths = 116 } 
2022-07-08 08:53:04.739208 - gail/main.py:174 - [TRPO] iter = 41000 dist_mean = 0.0945 dist_std = 0.9731 vf_loss = 0.3788 grad_norm = 0.3621 nat_grad_norm = 0.5873 cg_residual = 0.0005 step_size = 0.4280 reward = 0.0000 fps = 59 mse_loss = 0.8816 
2022-07-08 08:53:11.791768 - gail/main.py:174 - [TRPO] iter = 42000 dist_mean = 0.1059 dist_std = 0.9704 vf_loss = 0.6891 grad_norm = 0.4261 nat_grad_norm = 0.5634 cg_residual = 0.0013 step_size = 0.3911 reward = 0.0000 fps = 41 mse_loss = 0.8912 
2022-07-08 08:53:18.932987 - gail/main.py:174 - [TRPO] iter = 43000 dist_mean = 0.1876 dist_std = 0.9687 vf_loss = 0.4119 grad_norm = 0.4199 nat_grad_norm = 0.5722 cg_residual = 0.0041 step_size = 0.4090 reward = 0.0000 fps = 32 mse_loss = 0.8855 
2022-07-08 08:53:26.706019 - gail/main.py:174 - [TRPO] iter = 44000 dist_mean = 0.1457 dist_std = 0.9708 vf_loss = 0.6276 grad_norm = 0.4092 nat_grad_norm = 0.6689 cg_residual = 0.0035 step_size = 0.3674 reward = 0.0000 fps = 25 mse_loss = 0.8967 
2022-07-08 08:53:34.651990 - gail/main.py:174 - [TRPO] iter = 45000 dist_mean = 0.2067 dist_std = 0.9714 vf_loss = 0.2892 grad_norm = 0.5024 nat_grad_norm = 0.6049 cg_residual = 0.0069 step_size = 0.3422 reward = -0.0000 fps = 21 mse_loss = 0.8880 
2022-07-08 08:53:34.910544 - gail/main.py:201 - [Discriminator] iter = 45000 loss = -2.9693 grad_norm = 6.8531 grad_penalty = 0.4071 regularization = 0.0000 true_logits = 0.5964 fake_logits = -2.7800 true_prob = 0.6416 fake_prob = 0.0801 
2022-07-08 08:53:46.366671 - gail/main.py:142 - [Evaluate] iter = 45000 episode={ returns = 209.4827 lengths = 120 } discounted_episode={ returns = 194.7423 lengths = 122 } 
2022-07-08 08:53:53.517080 - gail/main.py:174 - [TRPO] iter = 46000 dist_mean = 0.1635 dist_std = 0.9728 vf_loss = 0.8451 grad_norm = 0.5590 nat_grad_norm = 0.5555 cg_residual = 0.0027 step_size = 0.3676 reward = -0.0000 fps = 53 mse_loss = 0.8987 
2022-07-08 08:54:00.795668 - gail/main.py:174 - [TRPO] iter = 47000 dist_mean = 0.1606 dist_std = 0.9724 vf_loss = 0.5208 grad_norm = 0.4000 nat_grad_norm = 0.6898 cg_residual = 0.0033 step_size = 0.3670 reward = 0.0000 fps = 38 mse_loss = 0.8770 
2022-07-08 08:54:07.608943 - gail/main.py:174 - [TRPO] iter = 48000 dist_mean = 0.1537 dist_std = 0.9699 vf_loss = 0.4611 grad_norm = 0.4212 nat_grad_norm = 0.5260 cg_residual = 0.0029 step_size = 0.4189 reward = 0.0000 fps = 30 mse_loss = 0.9336 
2022-07-08 08:54:14.727586 - gail/main.py:174 - [TRPO] iter = 49000 dist_mean = 0.1795 dist_std = 0.9704 vf_loss = 0.3398 grad_norm = 0.4876 nat_grad_norm = 0.5540 cg_residual = 0.0050 step_size = 0.3738 reward = 0.0000 fps = 25 mse_loss = 0.9282 
2022-07-08 08:54:21.646682 - gail/main.py:174 - [TRPO] iter = 50000 dist_mean = 0.1712 dist_std = 0.9718 vf_loss = 0.3391 grad_norm = 0.5145 nat_grad_norm = 0.6019 cg_residual = 0.0042 step_size = 0.3648 reward = 0.0000 fps = 21 mse_loss = 0.8871 
2022-07-08 08:54:21.912757 - gail/main.py:201 - [Discriminator] iter = 50000 loss = -3.3414 grad_norm = 7.1683 grad_penalty = 0.4685 regularization = 0.0000 true_logits = 0.5944 fake_logits = -3.2155 true_prob = 0.6391 fake_prob = 0.0590 
2022-07-08 08:54:40.913191 - gail/main.py:142 - [Evaluate] iter = 50000 episode={ returns = 301.5934 lengths = 205 } discounted_episode={ returns = 261.6042 lengths = 201 } 
2022-07-08 08:54:48.185991 - gail/main.py:174 - [TRPO] iter = 51000 dist_mean = 0.1593 dist_std = 0.9666 vf_loss = 0.3671 grad_norm = 0.4422 nat_grad_norm = 0.6572 cg_residual = 0.0031 step_size = 0.3754 reward = 0.0000 fps = 38 mse_loss = 0.9385 
2022-07-08 08:54:56.274044 - gail/main.py:174 - [TRPO] iter = 52000 dist_mean = 0.2076 dist_std = 0.9737 vf_loss = 0.3936 grad_norm = 0.4636 nat_grad_norm = 0.6035 cg_residual = 0.0039 step_size = 0.3853 reward = 0.0000 fps = 29 mse_loss = 0.9047 
2022-07-08 08:55:04.208955 - gail/main.py:174 - [TRPO] iter = 53000 dist_mean = 0.1928 dist_std = 0.9747 vf_loss = 0.2557 grad_norm = 0.4766 nat_grad_norm = 0.5652 cg_residual = 0.0040 step_size = 0.3880 reward = -0.0000 fps = 23 mse_loss = 0.8822 
2022-07-08 08:55:12.411168 - gail/main.py:174 - [TRPO] iter = 54000 dist_mean = 0.1745 dist_std = 0.9800 vf_loss = 0.4214 grad_norm = 0.4095 nat_grad_norm = 0.6188 cg_residual = 0.0025 step_size = 0.4011 reward = 0.0000 fps = 19 mse_loss = 0.9565 
2022-07-08 08:55:19.621719 - gail/main.py:174 - [TRPO] iter = 55000 dist_mean = 0.1637 dist_std = 0.9765 vf_loss = 0.4069 grad_norm = 0.4406 nat_grad_norm = 0.5961 cg_residual = 0.0020 step_size = 0.4192 reward = -0.0000 fps = 17 mse_loss = 0.8732 
2022-07-08 08:55:19.831293 - gail/main.py:201 - [Discriminator] iter = 55000 loss = -3.8434 grad_norm = 7.2907 grad_penalty = 0.4936 regularization = 0.0000 true_logits = 0.6312 fake_logits = -3.7058 true_prob = 0.6472 fake_prob = 0.0401 
2022-07-08 08:56:20.777539 - gail/main.py:142 - [Evaluate] iter = 55000 episode={ returns = 633.8180 lengths = 583 } discounted_episode={ returns = 493.0809 lengths = 696 } 
2022-07-08 08:56:27.653869 - gail/main.py:174 - [TRPO] iter = 56000 dist_mean = 0.2100 dist_std = 0.9743 vf_loss = 0.3729 grad_norm = 0.4018 nat_grad_norm = 0.6548 cg_residual = 0.0026 step_size = 0.3804 reward = 0.0000 fps = 14 mse_loss = 0.9255 
2022-07-08 08:56:35.124360 - gail/main.py:174 - [TRPO] iter = 57000 dist_mean = 0.1941 dist_std = 0.9739 vf_loss = 0.6683 grad_norm = 0.4424 nat_grad_norm = 0.6956 cg_residual = 0.0058 step_size = 0.3824 reward = -0.0000 fps = 13 mse_loss = 0.9132 
2022-07-08 08:56:42.539901 - gail/main.py:174 - [TRPO] iter = 58000 dist_mean = 0.1752 dist_std = 0.9751 vf_loss = 0.3971 grad_norm = 0.4321 nat_grad_norm = 0.5468 cg_residual = 0.0022 step_size = 0.4521 reward = -0.0000 fps = 12 mse_loss = 0.9371 
2022-07-08 08:56:49.469044 - gail/main.py:174 - [TRPO] iter = 59000 dist_mean = 0.2000 dist_std = 0.9722 vf_loss = 0.4621 grad_norm = 0.4532 nat_grad_norm = 0.6619 cg_residual = 0.0046 step_size = 0.3718 reward = 0.0000 fps = 11 mse_loss = 0.8718 
2022-07-08 08:56:56.776881 - gail/main.py:174 - [TRPO] iter = 60000 dist_mean = 0.2191 dist_std = 0.9682 vf_loss = 0.4348 grad_norm = 0.3654 nat_grad_norm = 0.5664 cg_residual = 0.0030 step_size = 0.4346 reward = -0.0000 fps = 10 mse_loss = 0.9349 
2022-07-08 08:56:57.013248 - gail/main.py:201 - [Discriminator] iter = 60000 loss = -4.0519 grad_norm = 6.5735 grad_penalty = 0.5249 regularization = 0.0000 true_logits = 0.5748 fake_logits = -4.0020 true_prob = 0.6346 fake_prob = 0.0343 
2022-07-08 08:58:33.723958 - gail/main.py:142 - [Evaluate] iter = 60000 episode={ returns = 979.3089 lengths = 1000 } discounted_episode={ returns = 610.1385 lengths = 1000 } 
2022-07-08 08:58:41.082888 - gail/main.py:174 - [TRPO] iter = 61000 dist_mean = 0.1761 dist_std = 0.9647 vf_loss = 0.5222 grad_norm = 0.5043 nat_grad_norm = 0.6761 cg_residual = 0.0035 step_size = 0.3571 reward = -0.0000 fps = 9 mse_loss = 0.9076 
2022-07-08 08:58:48.165538 - gail/main.py:174 - [TRPO] iter = 62000 dist_mean = 0.2361 dist_std = 0.9679 vf_loss = 0.5194 grad_norm = 0.5148 nat_grad_norm = 0.6267 cg_residual = 0.0051 step_size = 0.3622 reward = -0.0000 fps = 8 mse_loss = 0.8254 
2022-07-08 08:58:55.208909 - gail/main.py:174 - [TRPO] iter = 63000 dist_mean = 0.2137 dist_std = 0.9624 vf_loss = 0.4210 grad_norm = 0.5061 nat_grad_norm = 0.6520 cg_residual = 0.0065 step_size = 0.3649 reward = 0.0000 fps = 8 mse_loss = 0.8544 
2022-07-08 08:59:02.020470 - gail/main.py:174 - [TRPO] iter = 64000 dist_mean = 0.2148 dist_std = 0.9613 vf_loss = 0.3843 grad_norm = 0.5410 nat_grad_norm = 0.6142 cg_residual = 0.0064 step_size = 0.3455 reward = -0.0000 fps = 8 mse_loss = 0.7693 
2022-07-08 08:59:09.330170 - gail/main.py:174 - [TRPO] iter = 65000 dist_mean = 0.2037 dist_std = 0.9628 vf_loss = 0.2969 grad_norm = 0.4342 nat_grad_norm = 0.6171 cg_residual = 0.0048 step_size = 0.4027 reward = 0.0000 fps = 7 mse_loss = 0.8961 
2022-07-08 08:59:09.614886 - gail/main.py:201 - [Discriminator] iter = 65000 loss = -4.0902 grad_norm = 9.3328 grad_penalty = 0.5762 regularization = 0.0000 true_logits = 0.4626 fake_logits = -4.2038 true_prob = 0.6126 fake_prob = 0.0317 
2022-07-08 08:59:48.992304 - gail/main.py:142 - [Evaluate] iter = 65000 episode={ returns = 525.1393 lengths = 441 } discounted_episode={ returns = 373.0434 lengths = 379 } 
2022-07-08 08:59:56.237631 - gail/main.py:174 - [TRPO] iter = 66000 dist_mean = 0.2132 dist_std = 0.9567 vf_loss = 0.3874 grad_norm = 0.4576 nat_grad_norm = 0.5785 cg_residual = 0.0056 step_size = 0.3939 reward = 0.0000 fps = 21 mse_loss = 0.8303 
2022-07-08 09:00:04.138743 - gail/main.py:174 - [TRPO] iter = 67000 dist_mean = 0.1999 dist_std = 0.9551 vf_loss = 0.2728 grad_norm = 0.4993 nat_grad_norm = 0.6640 cg_residual = 0.0055 step_size = 0.3528 reward = 0.0000 fps = 18 mse_loss = 0.7501 
2022-07-08 09:00:12.656809 - gail/main.py:174 - [TRPO] iter = 68000 dist_mean = 0.1889 dist_std = 0.9544 vf_loss = 0.2162 grad_norm = 0.4592 nat_grad_norm = 0.7452 cg_residual = 0.0044 step_size = 0.3546 reward = -0.0000 fps = 15 mse_loss = 0.6964 
2022-07-08 09:00:21.023826 - gail/main.py:174 - [TRPO] iter = 69000 dist_mean = 0.1716 dist_std = 0.9499 vf_loss = 0.3977 grad_norm = 0.4715 nat_grad_norm = 0.5468 cg_residual = 0.0059 step_size = 0.3992 reward = 0.0000 fps = 14 mse_loss = 0.6751 
2022-07-08 09:00:29.567169 - gail/main.py:174 - [TRPO] iter = 70000 dist_mean = 0.2031 dist_std = 0.9488 vf_loss = 0.3125 grad_norm = 0.4906 nat_grad_norm = 0.6069 cg_residual = 0.0056 step_size = 0.3886 reward = 0.0000 fps = 12 mse_loss = 0.7365 
2022-07-08 09:00:29.953246 - gail/main.py:201 - [Discriminator] iter = 70000 loss = -4.4120 grad_norm = 6.7568 grad_penalty = 0.5008 regularization = 0.0000 true_logits = 0.3385 fake_logits = -4.5744 true_prob = 0.5859 fake_prob = 0.0239 
2022-07-08 09:00:49.957688 - gail/main.py:142 - [Evaluate] iter = 70000 episode={ returns = 250.9397 lengths = 163 } discounted_episode={ returns = 243.8782 lengths = 182 } 
2022-07-08 09:00:56.967048 - gail/main.py:174 - [TRPO] iter = 71000 dist_mean = 0.1927 dist_std = 0.9423 vf_loss = 0.2177 grad_norm = 0.4573 nat_grad_norm = 0.6651 cg_residual = 0.0070 step_size = 0.3751 reward = -0.0000 fps = 37 mse_loss = 0.7699 
2022-07-08 09:01:04.067009 - gail/main.py:174 - [TRPO] iter = 72000 dist_mean = 0.1739 dist_std = 0.9455 vf_loss = 0.4405 grad_norm = 0.4642 nat_grad_norm = 0.6649 cg_residual = 0.0047 step_size = 0.3534 reward = -0.0000 fps = 29 mse_loss = 0.7685 
2022-07-08 09:01:11.391645 - gail/main.py:174 - [TRPO] iter = 73000 dist_mean = 0.1555 dist_std = 0.9472 vf_loss = 0.4347 grad_norm = 0.4727 nat_grad_norm = 0.6425 cg_residual = 0.0043 step_size = 0.3737 reward = 0.0000 fps = 24 mse_loss = 0.7094 
2022-07-08 09:01:18.478085 - gail/main.py:174 - [TRPO] iter = 74000 dist_mean = 0.1991 dist_std = 0.9428 vf_loss = 0.4380 grad_norm = 0.4853 nat_grad_norm = 0.6254 cg_residual = 0.0064 step_size = 0.3615 reward = 0.0000 fps = 20 mse_loss = 0.8192 
2022-07-08 09:01:25.557251 - gail/main.py:174 - [TRPO] iter = 75000 dist_mean = 0.1780 dist_std = 0.9450 vf_loss = 0.2840 grad_norm = 0.5192 nat_grad_norm = 0.6730 cg_residual = 0.0072 step_size = 0.3445 reward = 0.0000 fps = 17 mse_loss = 0.8180 
2022-07-08 09:01:25.793213 - gail/main.py:201 - [Discriminator] iter = 75000 loss = -4.8045 grad_norm = 5.3498 grad_penalty = 0.4941 regularization = 0.0000 true_logits = 0.2715 fake_logits = -5.0271 true_prob = 0.5718 fake_prob = 0.0175 
2022-07-08 09:02:10.368749 - gail/main.py:142 - [Evaluate] iter = 75000 episode={ returns = 553.9214 lengths = 449 } discounted_episode={ returns = 417.8958 lengths = 477 } 
2022-07-08 09:02:17.306512 - gail/main.py:174 - [TRPO] iter = 76000 dist_mean = 0.1979 dist_std = 0.9407 vf_loss = 0.2521 grad_norm = 0.5584 nat_grad_norm = 0.6514 cg_residual = 0.0062 step_size = 0.3342 reward = 0.0000 fps = 19 mse_loss = 0.8586 
2022-07-08 09:02:24.455819 - gail/main.py:174 - [TRPO] iter = 77000 dist_mean = 0.1781 dist_std = 0.9406 vf_loss = 0.3151 grad_norm = 0.5862 nat_grad_norm = 0.6565 cg_residual = 0.0082 step_size = 0.3398 reward = -0.0000 fps = 17 mse_loss = 0.7765 
2022-07-08 09:02:31.351015 - gail/main.py:174 - [TRPO] iter = 78000 dist_mean = 0.2102 dist_std = 0.9366 vf_loss = 0.1778 grad_norm = 0.4645 nat_grad_norm = 0.6504 cg_residual = 0.0064 step_size = 0.3522 reward = -0.0000 fps = 15 mse_loss = 0.8000 
2022-07-08 09:02:38.563232 - gail/main.py:174 - [TRPO] iter = 79000 dist_mean = 0.2086 dist_std = 0.9337 vf_loss = 0.2037 grad_norm = 0.5338 nat_grad_norm = 0.6206 cg_residual = 0.0099 step_size = 0.3810 reward = -0.0000 fps = 13 mse_loss = 0.8078 
2022-07-08 09:02:45.748688 - gail/main.py:174 - [TRPO] iter = 80000 dist_mean = 0.2211 dist_std = 0.9318 vf_loss = 0.2600 grad_norm = 0.4715 nat_grad_norm = 0.6631 cg_residual = 0.0042 step_size = 0.3966 reward = -0.0000 fps = 12 mse_loss = 0.7453 
2022-07-08 09:02:46.050955 - gail/main.py:201 - [Discriminator] iter = 80000 loss = -4.8310 grad_norm = 6.7165 grad_penalty = 0.5316 regularization = 0.0000 true_logits = 0.0964 fake_logits = -5.2662 true_prob = 0.5397 fake_prob = 0.0148 
2022-07-08 09:03:26.287012 - gail/main.py:142 - [Evaluate] iter = 80000 episode={ returns = 435.2746 lengths = 320 } discounted_episode={ returns = 424.8347 lengths = 501 } 
2022-07-08 09:03:34.059295 - gail/main.py:174 - [TRPO] iter = 81000 dist_mean = 0.2519 dist_std = 0.9349 vf_loss = 0.4089 grad_norm = 0.5868 nat_grad_norm = 0.6152 cg_residual = 0.0149 step_size = 0.3375 reward = -0.0000 fps = 20 mse_loss = 0.7763 
2022-07-08 09:03:41.244955 - gail/main.py:174 - [TRPO] iter = 82000 dist_mean = 0.2213 dist_std = 0.9356 vf_loss = 0.3020 grad_norm = 0.5476 nat_grad_norm = 0.6586 cg_residual = 0.0088 step_size = 0.3461 reward = -0.0000 fps = 18 mse_loss = 0.7142 
2022-07-08 09:03:48.690927 - gail/main.py:174 - [TRPO] iter = 83000 dist_mean = 0.2233 dist_std = 0.9302 vf_loss = 0.2157 grad_norm = 0.4910 nat_grad_norm = 0.6464 cg_residual = 0.0107 step_size = 0.3708 reward = -0.0000 fps = 15 mse_loss = 0.8093 
2022-07-08 09:03:56.100857 - gail/main.py:174 - [TRPO] iter = 84000 dist_mean = 0.2102 dist_std = 0.9230 vf_loss = 0.3436 grad_norm = 0.4723 nat_grad_norm = 0.6516 cg_residual = 0.0054 step_size = 0.3710 reward = 0.0000 fps = 14 mse_loss = 0.7759 
2022-07-08 09:04:03.109681 - gail/main.py:174 - [TRPO] iter = 85000 dist_mean = 0.2200 dist_std = 0.9186 vf_loss = 0.5861 grad_norm = 0.3913 nat_grad_norm = 0.5731 cg_residual = 0.0049 step_size = 0.4188 reward = 0.0000 fps = 12 mse_loss = 0.7747 
2022-07-08 09:04:03.344370 - gail/main.py:201 - [Discriminator] iter = 85000 loss = -5.0019 grad_norm = 6.3925 grad_penalty = 0.5884 regularization = 0.0000 true_logits = 0.0131 fake_logits = -5.5773 true_prob = 0.5265 fake_prob = 0.0115 
2022-07-08 09:07:28.873271 - gail/main.py:142 - [Evaluate] iter = 85000 episode={ returns = 964.6305 lengths = 1000 } discounted_episode={ returns = 597.3228 lengths = 1000 } 
2022-07-08 09:07:45.577748 - gail/main.py:174 - [TRPO] iter = 86000 dist_mean = 0.2342 dist_std = 0.9125 vf_loss = 0.3197 grad_norm = 0.5238 nat_grad_norm = 0.6119 cg_residual = 0.0120 step_size = 0.3649 reward = -0.0000 fps = 4 mse_loss = 0.7758 
2022-07-08 09:08:02.501126 - gail/main.py:174 - [TRPO] iter = 87000 dist_mean = 0.2565 dist_std = 0.9114 vf_loss = 0.3986 grad_norm = 0.4685 nat_grad_norm = 0.5137 cg_residual = 0.0128 step_size = 0.4172 reward = -0.0000 fps = 4 mse_loss = 0.7191 
2022-07-08 09:08:21.030647 - gail/main.py:174 - [TRPO] iter = 88000 dist_mean = 0.2835 dist_std = 0.9108 vf_loss = 0.2307 grad_norm = 0.4718 nat_grad_norm = 0.6354 cg_residual = 0.0147 step_size = 0.3674 reward = -0.0000 fps = 3 mse_loss = 0.7032 
2022-07-08 09:08:38.405832 - gail/main.py:174 - [TRPO] iter = 89000 dist_mean = 0.2432 dist_std = 0.9109 vf_loss = 0.3259 grad_norm = 0.4987 nat_grad_norm = 0.6099 cg_residual = 0.0136 step_size = 0.3722 reward = -0.0000 fps = 3 mse_loss = 0.7979 
2022-07-08 09:08:56.098312 - gail/main.py:174 - [TRPO] iter = 90000 dist_mean = 0.2771 dist_std = 0.9056 vf_loss = 0.3093 grad_norm = 0.6256 nat_grad_norm = 0.5325 cg_residual = 0.0102 step_size = 0.4057 reward = 0.0000 fps = 3 mse_loss = 0.7606 
2022-07-08 09:08:56.621652 - gail/main.py:201 - [Discriminator] iter = 90000 loss = -4.5799 grad_norm = 6.0707 grad_penalty = 0.5947 regularization = 0.0000 true_logits = -0.0861 fake_logits = -5.2606 true_prob = 0.5046 fake_prob = 0.0144 
2022-07-08 09:15:33.059929 - gail/main.py:142 - [Evaluate] iter = 90000 episode={ returns = 966.9305 lengths = 1000 } discounted_episode={ returns = 599.5776 lengths = 1000 } 
2022-07-08 09:16:02.306634 - gail/main.py:174 - [TRPO] iter = 91000 dist_mean = 0.2862 dist_std = 0.9036 vf_loss = 0.3160 grad_norm = 0.6423 nat_grad_norm = 0.6385 cg_residual = 0.0153 step_size = 0.3517 reward = -0.0000 fps = 2 mse_loss = 0.7581 
2022-07-08 09:16:30.896541 - gail/main.py:174 - [TRPO] iter = 92000 dist_mean = 0.2754 dist_std = 0.8988 vf_loss = 0.1974 grad_norm = 0.4964 nat_grad_norm = 0.5876 cg_residual = 0.0189 step_size = 0.3819 reward = -0.0000 fps = 2 mse_loss = 0.7735 
2022-07-08 09:17:00.740853 - gail/main.py:174 - [TRPO] iter = 93000 dist_mean = 0.2496 dist_std = 0.8997 vf_loss = 0.3022 grad_norm = 0.5306 nat_grad_norm = 0.5297 cg_residual = 0.0111 step_size = 0.3620 reward = -0.0000 fps = 2 mse_loss = 0.7471 
2022-07-08 09:17:29.773219 - gail/main.py:174 - [TRPO] iter = 94000 dist_mean = 0.2204 dist_std = 0.8936 vf_loss = 0.2345 grad_norm = 0.5520 nat_grad_norm = 0.5854 cg_residual = 0.0109 step_size = 0.3672 reward = -0.0000 fps = 1 mse_loss = 0.6961 
2022-07-08 09:17:58.845642 - gail/main.py:174 - [TRPO] iter = 95000 dist_mean = 0.2774 dist_std = 0.8916 vf_loss = 0.2108 grad_norm = 0.6066 nat_grad_norm = 0.5889 cg_residual = 0.0197 step_size = 0.3550 reward = -0.0000 fps = 1 mse_loss = 0.7321 
2022-07-08 09:17:59.678678 - gail/main.py:201 - [Discriminator] iter = 95000 loss = -4.8817 grad_norm = 7.3253 grad_penalty = 0.5574 regularization = 0.0000 true_logits = -0.1981 fake_logits = -5.6373 true_prob = 0.4836 fake_prob = 0.0109 
2022-07-08 09:24:39.221486 - gail/main.py:142 - [Evaluate] iter = 95000 episode={ returns = 1013.4510 lengths = 995 } discounted_episode={ returns = 613.2265 lengths = 998 } 
2022-07-08 09:25:05.643584 - gail/main.py:174 - [TRPO] iter = 96000 dist_mean = 0.2605 dist_std = 0.8874 vf_loss = 0.2434 grad_norm = 0.5128 nat_grad_norm = 0.5239 cg_residual = 0.0135 step_size = 0.3830 reward = -0.0000 fps = 2 mse_loss = 0.7565 
2022-07-08 09:25:31.940567 - gail/main.py:174 - [TRPO] iter = 97000 dist_mean = 0.2182 dist_std = 0.8794 vf_loss = 0.1877 grad_norm = 0.5693 nat_grad_norm = 0.6152 cg_residual = 0.0129 step_size = 0.3557 reward = 0.0000 fps = 2 mse_loss = 0.7459 
2022-07-08 09:25:59.256722 - gail/main.py:174 - [TRPO] iter = 98000 dist_mean = 0.2953 dist_std = 0.8743 vf_loss = 0.1776 grad_norm = 0.5908 nat_grad_norm = 0.5287 cg_residual = 0.0161 step_size = 0.3471 reward = 0.0000 fps = 2 mse_loss = 0.7592 
2022-07-08 09:26:27.814855 - gail/main.py:174 - [TRPO] iter = 99000 dist_mean = 0.2966 dist_std = 0.8692 vf_loss = 0.2918 grad_norm = 0.9143 nat_grad_norm = 0.5068 cg_residual = 0.0186 step_size = 0.3393 reward = 0.0000 fps = 1 mse_loss = 0.7886 
2022-07-08 09:26:55.017680 - gail/main.py:174 - [TRPO] iter = 100000 dist_mean = 0.2558 dist_std = 0.8684 vf_loss = 0.1888 grad_norm = 0.6282 nat_grad_norm = 0.5473 cg_residual = 0.0174 step_size = 0.3822 reward = 0.0000 fps = 1 mse_loss = 0.8093 
2022-07-08 09:26:55.783240 - gail/main.py:201 - [Discriminator] iter = 100000 loss = -4.7838 grad_norm = 6.6698 grad_penalty = 0.4965 regularization = 0.0000 true_logits = -0.2800 fake_logits = -5.5603 true_prob = 0.4653 fake_prob = 0.0098 
2022-07-08 09:29:32.905697 - gail/main.py:142 - [Evaluate] iter = 100000 episode={ returns = 566.0333 lengths = 433 } discounted_episode={ returns = 420.2587 lengths = 406 } 
2022-07-08 09:30:00.250374 - gail/main.py:174 - [TRPO] iter = 101000 dist_mean = 0.2987 dist_std = 0.8662 vf_loss = 0.2564 grad_norm = 0.5767 nat_grad_norm = 0.5188 cg_residual = 0.0195 step_size = 0.3865 reward = 0.0000 fps = 5 mse_loss = 0.8289 
2022-07-08 09:30:27.447349 - gail/main.py:174 - [TRPO] iter = 102000 dist_mean = 0.2850 dist_std = 0.8624 vf_loss = 0.1947 grad_norm = 0.5526 nat_grad_norm = 0.4970 cg_residual = 0.0200 step_size = 0.4017 reward = 0.0000 fps = 4 mse_loss = 0.7438 
2022-07-08 09:30:54.043889 - gail/main.py:174 - [TRPO] iter = 103000 dist_mean = 0.2801 dist_std = 0.8580 vf_loss = 0.2351 grad_norm = 0.5651 nat_grad_norm = 0.5912 cg_residual = 0.0165 step_size = 0.3484 reward = 0.0000 fps = 4 mse_loss = 0.7899 
2022-07-08 09:31:21.204837 - gail/main.py:174 - [TRPO] iter = 104000 dist_mean = 0.2454 dist_std = 0.8559 vf_loss = 0.1886 grad_norm = 0.5074 nat_grad_norm = 0.5702 cg_residual = 0.0191 step_size = 0.3918 reward = -0.0000 fps = 3 mse_loss = 0.8005 
2022-07-08 09:31:48.732980 - gail/main.py:174 - [TRPO] iter = 105000 dist_mean = 0.2566 dist_std = 0.8573 vf_loss = 0.1788 grad_norm = 0.5298 nat_grad_norm = 0.5869 cg_residual = 0.0223 step_size = 0.3877 reward = -0.0000 fps = 3 mse_loss = 0.9073 
2022-07-08 09:31:49.466039 - gail/main.py:201 - [Discriminator] iter = 105000 loss = -4.9148 grad_norm = 5.5048 grad_penalty = 0.5402 regularization = 0.0000 true_logits = -0.2413 fake_logits = -5.6963 true_prob = 0.4715 fake_prob = 0.0085 
2022-07-08 09:33:22.695138 - gail/main.py:142 - [Evaluate] iter = 105000 episode={ returns = 362.0364 lengths = 236 } discounted_episode={ returns = 309.7403 lengths = 235 } 
2022-07-08 09:33:49.409777 - gail/main.py:174 - [TRPO] iter = 106000 dist_mean = 0.3028 dist_std = 0.8551 vf_loss = 0.2741 grad_norm = 0.5370 nat_grad_norm = 0.5434 cg_residual = 0.0328 step_size = 0.3578 reward = 0.0000 fps = 8 mse_loss = 0.9171 
2022-07-08 09:34:14.570447 - gail/main.py:174 - [TRPO] iter = 107000 dist_mean = 0.2482 dist_std = 0.8550 vf_loss = 0.3102 grad_norm = 0.5925 nat_grad_norm = 0.6433 cg_residual = 0.0266 step_size = 0.3494 reward = 0.0000 fps = 6 mse_loss = 0.9275 
2022-07-08 09:34:41.236799 - gail/main.py:174 - [TRPO] iter = 108000 dist_mean = 0.2792 dist_std = 0.8537 vf_loss = 0.2248 grad_norm = 0.6822 nat_grad_norm = 0.5837 cg_residual = 0.0229 step_size = 0.3408 reward = 0.0000 fps = 5 mse_loss = 0.9593 
2022-07-08 09:35:07.732488 - gail/main.py:174 - [TRPO] iter = 109000 dist_mean = 0.2950 dist_std = 0.8465 vf_loss = 0.2246 grad_norm = 0.6667 nat_grad_norm = 0.5132 cg_residual = 0.0257 step_size = 0.3630 reward = 0.0000 fps = 5 mse_loss = 0.9916 
2022-07-08 09:35:34.008177 - gail/main.py:174 - [TRPO] iter = 110000 dist_mean = 0.2574 dist_std = 0.8409 vf_loss = 0.2883 grad_norm = 0.6164 nat_grad_norm = 0.6044 cg_residual = 0.0301 step_size = 0.3319 reward = -0.0000 fps = 4 mse_loss = 0.9783 
2022-07-08 09:35:34.818741 - gail/main.py:201 - [Discriminator] iter = 110000 loss = -5.1308 grad_norm = 4.8306 grad_penalty = 0.5272 regularization = 0.0000 true_logits = -0.3587 fake_logits = -6.0167 true_prob = 0.4584 fake_prob = 0.0071 
2022-07-08 09:36:51.117270 - gail/main.py:142 - [Evaluate] iter = 110000 episode={ returns = 335.3874 lengths = 207 } discounted_episode={ returns = 300.2520 lengths = 216 } 
2022-07-08 09:37:16.927036 - gail/main.py:174 - [TRPO] iter = 111000 dist_mean = 0.2940 dist_std = 0.8356 vf_loss = 0.2217 grad_norm = 0.5575 nat_grad_norm = 0.5091 cg_residual = 0.0189 step_size = 0.3833 reward = -0.0000 fps = 9 mse_loss = 1.0431 
2022-07-08 09:37:43.269507 - gail/main.py:174 - [TRPO] iter = 112000 dist_mean = 0.2955 dist_std = 0.8347 vf_loss = 0.0713 grad_norm = 0.6570 nat_grad_norm = 0.5453 cg_residual = 0.0397 step_size = 0.3326 reward = 0.0000 fps = 7 mse_loss = 1.1027 
2022-07-08 09:38:09.332993 - gail/main.py:174 - [TRPO] iter = 113000 dist_mean = 0.2897 dist_std = 0.8277 vf_loss = 0.1986 grad_norm = 0.4893 nat_grad_norm = 0.5826 cg_residual = 0.0259 step_size = 0.3779 reward = -0.0000 fps = 6 mse_loss = 1.0800 
2022-07-08 09:38:35.094990 - gail/main.py:174 - [TRPO] iter = 114000 dist_mean = 0.2862 dist_std = 0.8267 vf_loss = 0.2314 grad_norm = 0.6667 nat_grad_norm = 0.4936 cg_residual = 0.0290 step_size = 0.3686 reward = 0.0000 fps = 5 mse_loss = 1.1196 
2022-07-08 09:39:00.596050 - gail/main.py:174 - [TRPO] iter = 115000 dist_mean = 0.3114 dist_std = 0.8268 vf_loss = 0.1009 grad_norm = 0.6161 nat_grad_norm = 0.5595 cg_residual = 0.0302 step_size = 0.3559 reward = 0.0000 fps = 4 mse_loss = 1.1122 
2022-07-08 09:39:01.350497 - gail/main.py:201 - [Discriminator] iter = 115000 loss = -4.5976 grad_norm = 5.3745 grad_penalty = 0.5427 regularization = 0.0000 true_logits = -0.3678 fake_logits = -5.5081 true_prob = 0.4537 fake_prob = 0.0105 
2022-07-08 09:40:17.569086 - gail/main.py:142 - [Evaluate] iter = 115000 episode={ returns = 335.8442 lengths = 211 } discounted_episode={ returns = 293.4187 lengths = 214 } 
2022-07-08 09:40:44.372893 - gail/main.py:174 - [TRPO] iter = 116000 dist_mean = 0.3030 dist_std = 0.8244 vf_loss = 0.1294 grad_norm = 0.5190 nat_grad_norm = 0.5153 cg_residual = 0.0249 step_size = 0.3873 reward = -0.0000 fps = 9 mse_loss = 1.1123 
2022-07-08 09:41:09.255595 - gail/main.py:174 - [TRPO] iter = 117000 dist_mean = 0.2810 dist_std = 0.8224 vf_loss = 0.1461 grad_norm = 0.5790 nat_grad_norm = 0.5687 cg_residual = 0.0250 step_size = 0.3671 reward = -0.0000 fps = 7 mse_loss = 1.0500 
2022-07-08 09:41:36.288984 - gail/main.py:174 - [TRPO] iter = 118000 dist_mean = 0.2946 dist_std = 0.8212 vf_loss = 0.1926 grad_norm = 0.6260 nat_grad_norm = 0.6821 cg_residual = 0.0551 step_size = 0.3228 reward = -0.0000 fps = 6 mse_loss = 1.1344 
2022-07-08 09:42:03.111417 - gail/main.py:174 - [TRPO] iter = 119000 dist_mean = 0.3017 dist_std = 0.8206 vf_loss = 0.0928 grad_norm = 0.5793 nat_grad_norm = 0.5475 cg_residual = 0.0288 step_size = 0.3686 reward = 0.0000 fps = 5 mse_loss = 1.0501 
2022-07-08 09:42:34.195404 - gail/main.py:174 - [TRPO] iter = 120000 dist_mean = 0.2952 dist_std = 0.8181 vf_loss = 0.2279 grad_norm = 0.6660 nat_grad_norm = 0.5859 cg_residual = 0.0387 step_size = 0.3138 reward = -0.0000 fps = 4 mse_loss = 1.0607 
2022-07-08 09:42:34.959258 - gail/main.py:201 - [Discriminator] iter = 120000 loss = -4.9405 grad_norm = 4.1739 grad_penalty = 0.4900 regularization = 0.0000 true_logits = -0.3220 fake_logits = -5.7525 true_prob = 0.4512 fake_prob = 0.0094 
2022-07-08 09:43:51.483972 - gail/main.py:142 - [Evaluate] iter = 120000 episode={ returns = 341.4671 lengths = 225 } discounted_episode={ returns = 295.3946 lengths = 225 } 
2022-07-08 09:44:17.306750 - gail/main.py:174 - [TRPO] iter = 121000 dist_mean = 0.3032 dist_std = 0.8116 vf_loss = 0.2919 grad_norm = 0.5330 nat_grad_norm = 0.5370 cg_residual = 0.0160 step_size = 0.3738 reward = 0.0000 fps = 9 mse_loss = 1.1618 
2022-07-08 09:44:42.810844 - gail/main.py:174 - [TRPO] iter = 122000 dist_mean = 0.2759 dist_std = 0.8084 vf_loss = 0.2132 grad_norm = 0.5960 nat_grad_norm = 0.5760 cg_residual = 0.0309 step_size = 0.3454 reward = 0.0000 fps = 7 mse_loss = 1.1567 
2022-07-08 09:45:08.072824 - gail/main.py:174 - [TRPO] iter = 123000 dist_mean = 0.3236 dist_std = 0.8079 vf_loss = 0.0954 grad_norm = 0.7365 nat_grad_norm = 0.5978 cg_residual = 0.0431 step_size = 0.3074 reward = 0.0000 fps = 6 mse_loss = 1.0927 
2022-07-08 09:45:33.186477 - gail/main.py:174 - [TRPO] iter = 124000 dist_mean = 0.3292 dist_std = 0.8075 vf_loss = 0.2012 grad_norm = 0.6247 nat_grad_norm = 0.5811 cg_residual = 0.0544 step_size = 0.3372 reward = -0.0000 fps = 5 mse_loss = 1.1793 
2022-07-08 09:45:58.408569 - gail/main.py:174 - [TRPO] iter = 125000 dist_mean = 0.3285 dist_std = 0.8063 vf_loss = 0.1570 grad_norm = 0.7226 nat_grad_norm = 0.5485 cg_residual = 0.0579 step_size = 0.3424 reward = 0.0000 fps = 4 mse_loss = 1.2007 
2022-07-08 09:45:59.144839 - gail/main.py:201 - [Discriminator] iter = 125000 loss = -4.9220 grad_norm = 3.8416 grad_penalty = 0.5297 regularization = 0.0000 true_logits = -0.3908 fake_logits = -5.8425 true_prob = 0.4398 fake_prob = 0.0076 
2022-07-08 09:47:15.140668 - gail/main.py:142 - [Evaluate] iter = 125000 episode={ returns = 329.9539 lengths = 220 } discounted_episode={ returns = 284.6577 lengths = 220 } 
2022-07-08 09:47:40.516346 - gail/main.py:174 - [TRPO] iter = 126000 dist_mean = 0.3176 dist_std = 0.8045 vf_loss = 0.1884 grad_norm = 0.7268 nat_grad_norm = 0.5152 cg_residual = 0.0367 step_size = 0.3420 reward = 0.0000 fps = 9 mse_loss = 1.0567 
2022-07-08 09:48:06.716903 - gail/main.py:174 - [TRPO] iter = 127000 dist_mean = 0.2888 dist_std = 0.8008 vf_loss = 0.2509 grad_norm = 0.7985 nat_grad_norm = 0.5137 cg_residual = 0.0432 step_size = 0.3680 reward = -0.0000 fps = 7 mse_loss = 1.1944 
2022-07-08 09:48:31.386654 - gail/main.py:174 - [TRPO] iter = 128000 dist_mean = 0.2744 dist_std = 0.7997 vf_loss = 0.2632 grad_norm = 0.7533 nat_grad_norm = 0.5191 cg_residual = 0.0345 step_size = 0.3725 reward = 0.0000 fps = 6 mse_loss = 1.2017 
2022-07-08 09:48:56.357843 - gail/main.py:174 - [TRPO] iter = 129000 dist_mean = 0.3094 dist_std = 0.7972 vf_loss = 0.1798 grad_norm = 0.6449 nat_grad_norm = 0.4960 cg_residual = 0.0319 step_size = 0.3860 reward = 0.0000 fps = 5 mse_loss = 1.1731 
2022-07-08 09:49:21.964560 - gail/main.py:174 - [TRPO] iter = 130000 dist_mean = 0.2802 dist_std = 0.8015 vf_loss = 0.1232 grad_norm = 0.6634 nat_grad_norm = 0.4818 cg_residual = 0.0605 step_size = 0.3669 reward = -0.0000 fps = 4 mse_loss = 1.2260 
2022-07-08 09:49:22.646769 - gail/main.py:201 - [Discriminator] iter = 130000 loss = -4.8713 grad_norm = 4.3611 grad_penalty = 0.5539 regularization = 0.0000 true_logits = -0.5026 fake_logits = -5.9277 true_prob = 0.4206 fake_prob = 0.0086 
2022-07-08 09:50:34.906967 - gail/main.py:142 - [Evaluate] iter = 130000 episode={ returns = 315.1688 lengths = 193 } discounted_episode={ returns = 277.9448 lengths = 193 } 
2022-07-08 09:50:59.491423 - gail/main.py:174 - [TRPO] iter = 131000 dist_mean = 0.3145 dist_std = 0.7947 vf_loss = 0.1652 grad_norm = 0.7473 nat_grad_norm = 0.5432 cg_residual = 0.0443 step_size = 0.3053 reward = 0.0000 fps = 10 mse_loss = 1.3019 
2022-07-08 09:51:24.081926 - gail/main.py:174 - [TRPO] iter = 132000 dist_mean = 0.3090 dist_std = 0.7861 vf_loss = 0.1835 grad_norm = 0.5354 nat_grad_norm = 0.5640 cg_residual = 0.0340 step_size = 0.3856 reward = -0.0000 fps = 8 mse_loss = 1.1499 
2022-07-08 09:51:48.866856 - gail/main.py:174 - [TRPO] iter = 133000 dist_mean = 0.3009 dist_std = 0.7861 vf_loss = 0.2120 grad_norm = 0.8068 nat_grad_norm = 0.5662 cg_residual = 0.0417 step_size = 0.3395 reward = -0.0000 fps = 6 mse_loss = 1.2037 
2022-07-08 09:52:21.452417 - gail/main.py:174 - [TRPO] iter = 134000 dist_mean = 0.3004 dist_std = 0.7858 vf_loss = 0.2663 grad_norm = 0.6438 nat_grad_norm = 0.5355 cg_residual = 0.0244 step_size = 0.3680 reward = 0.0000 fps = 5 mse_loss = 1.1707 
2022-07-08 09:52:46.418753 - gail/main.py:174 - [TRPO] iter = 135000 dist_mean = 0.2975 dist_std = 0.7851 vf_loss = 0.2047 grad_norm = 0.7131 nat_grad_norm = 0.5759 cg_residual = 0.0365 step_size = 0.3298 reward = 0.0000 fps = 4 mse_loss = 1.3459 
2022-07-08 09:52:47.264101 - gail/main.py:201 - [Discriminator] iter = 135000 loss = -4.8192 grad_norm = 4.3082 grad_penalty = 0.4730 regularization = 0.0000 true_logits = -0.4312 fake_logits = -5.7234 true_prob = 0.4387 fake_prob = 0.0086 
2022-07-08 09:53:44.807233 - gail/main.py:142 - [Evaluate] iter = 135000 episode={ returns = 282.5738 lengths = 160 } discounted_episode={ returns = 266.2205 lengths = 167 } 
2022-07-08 09:54:12.134099 - gail/main.py:174 - [TRPO] iter = 136000 dist_mean = 0.2364 dist_std = 0.7858 vf_loss = 0.2569 grad_norm = 0.5941 nat_grad_norm = 0.5838 cg_residual = 0.0484 step_size = 0.3491 reward = 0.0000 fps = 11 mse_loss = 1.2229 
2022-07-08 09:54:38.646278 - gail/main.py:174 - [TRPO] iter = 137000 dist_mean = 0.3113 dist_std = 0.7845 vf_loss = 0.1393 grad_norm = 0.6602 nat_grad_norm = 0.5406 cg_residual = 0.0405 step_size = 0.3545 reward = -0.0000 fps = 8 mse_loss = 1.2020 
2022-07-08 09:55:04.544703 - gail/main.py:174 - [TRPO] iter = 138000 dist_mean = 0.2889 dist_std = 0.7839 vf_loss = 0.2588 grad_norm = 0.6920 nat_grad_norm = 0.5358 cg_residual = 0.0282 step_size = 0.3776 reward = -0.0000 fps = 7 mse_loss = 1.1870 
2022-07-08 09:55:29.341237 - gail/main.py:174 - [TRPO] iter = 139000 dist_mean = 0.2863 dist_std = 0.7824 vf_loss = 0.2925 grad_norm = 0.7013 nat_grad_norm = 0.4549 cg_residual = 0.0308 step_size = 0.3942 reward = 0.0000 fps = 6 mse_loss = 1.2128 
2022-07-08 09:55:53.362723 - gail/main.py:174 - [TRPO] iter = 140000 dist_mean = 0.3000 dist_std = 0.7803 vf_loss = 0.2001 grad_norm = 0.5558 nat_grad_norm = 0.5196 cg_residual = 0.0302 step_size = 0.3649 reward = 0.0000 fps = 5 mse_loss = 1.2078 
2022-07-08 09:55:54.100242 - gail/main.py:201 - [Discriminator] iter = 140000 loss = -4.9941 grad_norm = 4.9615 grad_penalty = 0.5300 regularization = 0.0000 true_logits = -0.4461 fake_logits = -5.9702 true_prob = 0.4352 fake_prob = 0.0064 
2022-07-08 09:56:52.596561 - gail/main.py:142 - [Evaluate] iter = 140000 episode={ returns = 315.9703 lengths = 186 } discounted_episode={ returns = 276.0738 lengths = 184 } 
2022-07-08 09:57:16.901118 - gail/main.py:174 - [TRPO] iter = 141000 dist_mean = 0.3072 dist_std = 0.7769 vf_loss = 0.1729 grad_norm = 0.5701 nat_grad_norm = 0.5135 cg_residual = 0.0431 step_size = 0.3624 reward = -0.0000 fps = 12 mse_loss = 1.2093 
2022-07-08 09:57:40.378759 - gail/main.py:174 - [TRPO] iter = 142000 dist_mean = 0.2479 dist_std = 0.7777 vf_loss = 0.1960 grad_norm = 0.7604 nat_grad_norm = 0.6311 cg_residual = 0.0389 step_size = 0.3276 reward = 0.0000 fps = 9 mse_loss = 1.2713 
2022-07-08 09:58:04.540666 - gail/main.py:174 - [TRPO] iter = 143000 dist_mean = 0.3093 dist_std = 0.7742 vf_loss = 0.1583 grad_norm = 0.6786 nat_grad_norm = 0.5350 cg_residual = 0.0458 step_size = 0.3300 reward = 0.0000 fps = 7 mse_loss = 1.4975 
2022-07-08 09:58:30.548602 - gail/main.py:174 - [TRPO] iter = 144000 dist_mean = 0.2929 dist_std = 0.7685 vf_loss = 0.1887 grad_norm = 0.5960 nat_grad_norm = 0.4961 cg_residual = 0.0287 step_size = 0.3663 reward = -0.0000 fps = 6 mse_loss = 1.3470 
2022-07-08 09:58:57.018227 - gail/main.py:174 - [TRPO] iter = 145000 dist_mean = 0.3133 dist_std = 0.7660 vf_loss = 0.1395 grad_norm = 0.8211 nat_grad_norm = 0.5699 cg_residual = 0.0596 step_size = 0.3239 reward = 0.0000 fps = 5 mse_loss = 1.3098 
2022-07-08 09:58:57.774845 - gail/main.py:201 - [Discriminator] iter = 145000 loss = -4.6539 grad_norm = 4.9911 grad_penalty = 0.4862 regularization = 0.0000 true_logits = -0.5068 fake_logits = -5.6469 true_prob = 0.4273 fake_prob = 0.0090 
2022-07-08 10:00:00.366082 - gail/main.py:142 - [Evaluate] iter = 145000 episode={ returns = 237.9717 lengths = 178 } discounted_episode={ returns = 200.2460 lengths = 175 } 
2022-07-08 10:00:26.927227 - gail/main.py:174 - [TRPO] iter = 146000 dist_mean = 0.3038 dist_std = 0.7647 vf_loss = 0.1541 grad_norm = 0.8475 nat_grad_norm = 0.5185 cg_residual = 0.0420 step_size = 0.3502 reward = -0.0000 fps = 11 mse_loss = 1.3428 
2022-07-08 10:00:53.901305 - gail/main.py:174 - [TRPO] iter = 147000 dist_mean = 0.2879 dist_std = 0.7645 vf_loss = 0.1755 grad_norm = 0.7582 nat_grad_norm = 0.4343 cg_residual = 0.0468 step_size = 0.3628 reward = -0.0000 fps = 8 mse_loss = 1.3653 
2022-07-08 10:01:20.058248 - gail/main.py:174 - [TRPO] iter = 148000 dist_mean = 0.2992 dist_std = 0.7614 vf_loss = 0.1581 grad_norm = 0.7213 nat_grad_norm = 0.5377 cg_residual = 0.0481 step_size = 0.3234 reward = 0.0000 fps = 7 mse_loss = 1.4517 
2022-07-08 10:01:48.684841 - gail/main.py:174 - [TRPO] iter = 149000 dist_mean = 0.3126 dist_std = 0.7544 vf_loss = 0.1360 grad_norm = 0.7086 nat_grad_norm = 0.4783 cg_residual = 0.0503 step_size = 0.3673 reward = -0.0000 fps = 5 mse_loss = 1.5013 
2022-07-08 10:02:24.661001 - gail/main.py:174 - [TRPO] iter = 150000 dist_mean = 0.2983 dist_std = 0.7489 vf_loss = 0.1157 grad_norm = 0.7242 nat_grad_norm = 0.4441 cg_residual = 0.0632 step_size = 0.3838 reward = -0.0000 fps = 4 mse_loss = 1.5474 
2022-07-08 10:02:25.417726 - gail/main.py:201 - [Discriminator] iter = 150000 loss = -5.0318 grad_norm = 5.1302 grad_penalty = 0.5527 regularization = 0.0000 true_logits = -0.4447 fake_logits = -6.0292 true_prob = 0.4303 fake_prob = 0.0053 
2022-07-08 10:03:36.970095 - gail/main.py:142 - [Evaluate] iter = 150000 episode={ returns = 212.9091 lengths = 181 } discounted_episode={ returns = 191.6898 lengths = 182 } 
2022-07-08 10:04:07.329702 - gail/main.py:174 - [TRPO] iter = 151000 dist_mean = 0.2934 dist_std = 0.7482 vf_loss = 0.2485 grad_norm = 0.6453 nat_grad_norm = 0.5276 cg_residual = 0.0435 step_size = 0.3294 reward = -0.0000 fps = 9 mse_loss = 1.3915 
2022-07-08 10:04:36.636624 - gail/main.py:174 - [TRPO] iter = 152000 dist_mean = 0.3115 dist_std = 0.7448 vf_loss = 0.2093 grad_norm = 0.8539 nat_grad_norm = 0.4318 cg_residual = 0.0399 step_size = 0.3936 reward = 0.0000 fps = 7 mse_loss = 1.4499 
2022-07-08 10:05:04.971504 - gail/main.py:174 - [TRPO] iter = 153000 dist_mean = 0.3156 dist_std = 0.7388 vf_loss = 0.1910 grad_norm = 0.7152 nat_grad_norm = 0.5456 cg_residual = 0.0340 step_size = 0.3494 reward = 0.0000 fps = 6 mse_loss = 1.4479 
2022-07-08 10:05:34.042736 - gail/main.py:174 - [TRPO] iter = 154000 dist_mean = 0.3017 dist_std = 0.7383 vf_loss = 0.2492 grad_norm = 0.9498 nat_grad_norm = 0.4253 cg_residual = 0.0524 step_size = 0.3729 reward = 0.0000 fps = 5 mse_loss = 1.5094 
2022-07-08 10:06:03.263091 - gail/main.py:174 - [TRPO] iter = 155000 dist_mean = 0.2888 dist_std = 0.7405 vf_loss = 0.2726 grad_norm = 0.5738 nat_grad_norm = 0.5408 cg_residual = 0.0382 step_size = 0.3572 reward = 0.0000 fps = 4 mse_loss = 1.3734 
2022-07-08 10:06:04.143256 - gail/main.py:201 - [Discriminator] iter = 155000 loss = -4.8756 grad_norm = 6.3019 grad_penalty = 0.4810 regularization = 0.0000 true_logits = -0.3455 fake_logits = -5.7021 true_prob = 0.4555 fake_prob = 0.0091 
2022-07-08 10:07:15.668723 - gail/main.py:142 - [Evaluate] iter = 155000 episode={ returns = 328.1398 lengths = 183 } discounted_episode={ returns = 294.4731 lengths = 181 } 
2022-07-08 10:07:43.670825 - gail/main.py:174 - [TRPO] iter = 156000 dist_mean = 0.2944 dist_std = 0.7397 vf_loss = 0.1898 grad_norm = 0.6643 nat_grad_norm = 0.5178 cg_residual = 0.0457 step_size = 0.3467 reward = 0.0000 fps = 10 mse_loss = 1.4486 
2022-07-08 10:08:11.667362 - gail/main.py:174 - [TRPO] iter = 157000 dist_mean = 0.2791 dist_std = 0.7363 vf_loss = 0.1507 grad_norm = 0.6779 nat_grad_norm = 0.5462 cg_residual = 0.0412 step_size = 0.3423 reward = 0.0000 fps = 7 mse_loss = 1.4281 
2022-07-08 10:08:39.857066 - gail/main.py:174 - [TRPO] iter = 158000 dist_mean = 0.3047 dist_std = 0.7359 vf_loss = 0.1702 grad_norm = 0.6777 nat_grad_norm = 0.6383 cg_residual = 0.0538 step_size = 0.3128 reward = -0.0000 fps = 6 mse_loss = 1.3822 
2022-07-08 10:09:09.476175 - gail/main.py:174 - [TRPO] iter = 159000 dist_mean = 0.3082 dist_std = 0.7280 vf_loss = 0.1362 grad_norm = 0.6845 nat_grad_norm = 0.5099 cg_residual = 0.0452 step_size = 0.3553 reward = -0.0000 fps = 5 mse_loss = 1.5246 
2022-07-08 10:09:39.562833 - gail/main.py:174 - [TRPO] iter = 160000 dist_mean = 0.2927 dist_std = 0.7232 vf_loss = 0.1662 grad_norm = 0.5815 nat_grad_norm = 0.5947 cg_residual = 0.0502 step_size = 0.3555 reward = -0.0000 fps = 4 mse_loss = 1.4946 
2022-07-08 10:09:40.417076 - gail/main.py:201 - [Discriminator] iter = 160000 loss = -5.0289 grad_norm = 6.6806 grad_penalty = 0.5587 regularization = 0.0000 true_logits = -0.3471 fake_logits = -5.9347 true_prob = 0.4596 fake_prob = 0.0072 
2022-07-08 10:10:44.639454 - gail/main.py:142 - [Evaluate] iter = 160000 episode={ returns = 308.4750 lengths = 166 } discounted_episode={ returns = 277.6111 lengths = 167 } 
2022-07-08 10:11:14.121712 - gail/main.py:174 - [TRPO] iter = 161000 dist_mean = 0.2913 dist_std = 0.7231 vf_loss = 0.1396 grad_norm = 0.7156 nat_grad_norm = 0.4640 cg_residual = 0.0305 step_size = 0.3770 reward = -0.0000 fps = 10 mse_loss = 1.4543 
2022-07-08 10:11:44.076896 - gail/main.py:174 - [TRPO] iter = 162000 dist_mean = 0.2993 dist_std = 0.7172 vf_loss = 0.1601 grad_norm = 0.8065 nat_grad_norm = 0.4744 cg_residual = 0.0370 step_size = 0.3778 reward = -0.0000 fps = 8 mse_loss = 1.5074 
2022-07-08 10:12:20.240631 - gail/main.py:174 - [TRPO] iter = 163000 dist_mean = 0.3005 dist_std = 0.7130 vf_loss = 0.1618 grad_norm = 0.9413 nat_grad_norm = 0.4401 cg_residual = 0.0656 step_size = 0.3732 reward = 0.0000 fps = 6 mse_loss = 1.5541 
2022-07-08 10:12:50.004922 - gail/main.py:174 - [TRPO] iter = 164000 dist_mean = 0.3221 dist_std = 0.7110 vf_loss = 0.1439 grad_norm = 0.7295 nat_grad_norm = 0.4929 cg_residual = 0.0958 step_size = 0.3564 reward = -0.0000 fps = 5 mse_loss = 1.5861 
2022-07-08 10:13:18.265099 - gail/main.py:174 - [TRPO] iter = 165000 dist_mean = 0.3117 dist_std = 0.7066 vf_loss = 0.1359 grad_norm = 0.7909 nat_grad_norm = 0.4597 cg_residual = 0.0543 step_size = 0.3513 reward = 0.0000 fps = 4 mse_loss = 1.4502 
2022-07-08 10:13:19.100913 - gail/main.py:201 - [Discriminator] iter = 165000 loss = -4.7708 grad_norm = 4.0072 grad_penalty = 0.5033 regularization = 0.0000 true_logits = -0.2359 fake_logits = -5.5099 true_prob = 0.4706 fake_prob = 0.0096 
2022-07-08 10:14:27.877508 - gail/main.py:142 - [Evaluate] iter = 165000 episode={ returns = 346.1606 lengths = 184 } discounted_episode={ returns = 308.1932 lengths = 183 } 
2022-07-08 10:14:57.416539 - gail/main.py:174 - [TRPO] iter = 166000 dist_mean = 0.2963 dist_std = 0.7040 vf_loss = 0.1569 grad_norm = 0.6449 nat_grad_norm = 0.4896 cg_residual = 0.0564 step_size = 0.3822 reward = 0.0000 fps = 10 mse_loss = 1.4759 
2022-07-08 10:15:25.800661 - gail/main.py:174 - [TRPO] iter = 167000 dist_mean = 0.2897 dist_std = 0.7027 vf_loss = 0.1903 grad_norm = 0.8254 nat_grad_norm = 0.5182 cg_residual = 0.0926 step_size = 0.3401 reward = -0.0000 fps = 7 mse_loss = 1.5612 
2022-07-08 10:15:54.470592 - gail/main.py:174 - [TRPO] iter = 168000 dist_mean = 0.2879 dist_std = 0.7008 vf_loss = 0.1950 grad_norm = 0.8947 nat_grad_norm = 0.6076 cg_residual = 0.0720 step_size = 0.3109 reward = -0.0000 fps = 6 mse_loss = 1.5088 
2022-07-08 10:16:23.365274 - gail/main.py:174 - [TRPO] iter = 169000 dist_mean = 0.2734 dist_std = 0.6964 vf_loss = 0.1946 grad_norm = 0.8172 nat_grad_norm = 0.4987 cg_residual = 0.0561 step_size = 0.3257 reward = -0.0000 fps = 5 mse_loss = 1.4632 
2022-07-08 10:16:52.405312 - gail/main.py:174 - [TRPO] iter = 170000 dist_mean = 0.2871 dist_std = 0.6942 vf_loss = 0.2381 grad_norm = 0.7625 nat_grad_norm = 0.4981 cg_residual = 0.0742 step_size = 0.3445 reward = -0.0000 fps = 4 mse_loss = 1.4341 
2022-07-08 10:16:53.242019 - gail/main.py:201 - [Discriminator] iter = 170000 loss = -4.9546 grad_norm = 3.2557 grad_penalty = 0.5055 regularization = 0.0000 true_logits = -0.2176 fake_logits = -5.6777 true_prob = 0.4780 fake_prob = 0.0086 
2022-07-08 10:18:07.189375 - gail/main.py:142 - [Evaluate] iter = 170000 episode={ returns = 396.7463 lengths = 199 } discounted_episode={ returns = 339.7739 lengths = 194 } 
2022-07-08 10:18:35.551663 - gail/main.py:174 - [TRPO] iter = 171000 dist_mean = 0.2690 dist_std = 0.6879 vf_loss = 0.2992 grad_norm = 0.6315 nat_grad_norm = 0.4764 cg_residual = 0.0427 step_size = 0.3847 reward = -0.0000 fps = 9 mse_loss = 1.7250 
2022-07-08 10:19:01.805145 - gail/main.py:174 - [TRPO] iter = 172000 dist_mean = 0.2904 dist_std = 0.6865 vf_loss = 0.1444 grad_norm = 0.7210 nat_grad_norm = 0.4441 cg_residual = 0.0552 step_size = 0.3699 reward = 0.0000 fps = 7 mse_loss = 1.5185 
2022-07-08 10:19:30.166350 - gail/main.py:174 - [TRPO] iter = 173000 dist_mean = 0.2966 dist_std = 0.6851 vf_loss = 0.0940 grad_norm = 0.8552 nat_grad_norm = 0.5359 cg_residual = 0.0822 step_size = 0.3002 reward = 0.0000 fps = 6 mse_loss = 1.4316 
2022-07-08 10:19:58.993778 - gail/main.py:174 - [TRPO] iter = 174000 dist_mean = 0.2981 dist_std = 0.6819 vf_loss = 0.1849 grad_norm = 0.7482 nat_grad_norm = 0.5054 cg_residual = 0.0644 step_size = 0.3607 reward = 0.0000 fps = 5 mse_loss = 1.4328 
2022-07-08 10:20:27.131359 - gail/main.py:174 - [TRPO] iter = 175000 dist_mean = 0.3050 dist_std = 0.6807 vf_loss = 0.2270 grad_norm = 0.9423 nat_grad_norm = 0.5395 cg_residual = 0.0871 step_size = 0.3359 reward = -0.0000 fps = 4 mse_loss = 1.4341 
2022-07-08 10:20:28.177253 - gail/main.py:201 - [Discriminator] iter = 175000 loss = -4.9075 grad_norm = 3.3892 grad_penalty = 0.5286 regularization = 0.0000 true_logits = -0.2200 fake_logits = -5.6561 true_prob = 0.4779 fake_prob = 0.0091 
2022-07-08 10:21:44.452967 - gail/main.py:142 - [Evaluate] iter = 175000 episode={ returns = 375.8166 lengths = 189 } discounted_episode={ returns = 327.0672 lengths = 187 } 
2022-07-08 10:22:13.579424 - gail/main.py:174 - [TRPO] iter = 176000 dist_mean = 0.2938 dist_std = 0.6825 vf_loss = 0.1766 grad_norm = 0.7244 nat_grad_norm = 0.5752 cg_residual = 0.0579 step_size = 0.3156 reward = 0.0000 fps = 9 mse_loss = 1.4388 
2022-07-08 10:22:41.691106 - gail/main.py:174 - [TRPO] iter = 177000 dist_mean = 0.3114 dist_std = 0.6786 vf_loss = 0.2671 grad_norm = 0.5918 nat_grad_norm = 0.5038 cg_residual = 0.0411 step_size = 0.3588 reward = 0.0000 fps = 7 mse_loss = 1.4272 
2022-07-08 10:23:10.101766 - gail/main.py:174 - [TRPO] iter = 178000 dist_mean = 0.2850 dist_std = 0.6809 vf_loss = 0.2603 grad_norm = 0.7428 nat_grad_norm = 0.4754 cg_residual = 0.0371 step_size = 0.3337 reward = 0.0000 fps = 6 mse_loss = 1.4562 
2022-07-08 10:23:38.553851 - gail/main.py:174 - [TRPO] iter = 179000 dist_mean = 0.2546 dist_std = 0.6785 vf_loss = 0.1872 grad_norm = 0.8639 nat_grad_norm = 0.5096 cg_residual = 0.0352 step_size = 0.3518 reward = -0.0000 fps = 5 mse_loss = 1.5465 
2022-07-08 10:24:07.059291 - gail/main.py:174 - [TRPO] iter = 180000 dist_mean = 0.2828 dist_std = 0.6778 vf_loss = 0.2316 grad_norm = 0.8349 nat_grad_norm = 0.5418 cg_residual = 0.1116 step_size = 0.3338 reward = -0.0000 fps = 4 mse_loss = 1.4694 
2022-07-08 10:24:07.867013 - gail/main.py:201 - [Discriminator] iter = 180000 loss = -4.6366 grad_norm = 3.0964 grad_penalty = 0.4779 regularization = 0.0000 true_logits = -0.2495 fake_logits = -5.3640 true_prob = 0.4779 fake_prob = 0.0110 
2022-07-08 10:25:16.332905 - gail/main.py:142 - [Evaluate] iter = 180000 episode={ returns = 366.2103 lengths = 181 } discounted_episode={ returns = 323.9316 lengths = 181 } 
2022-07-08 10:25:44.635357 - gail/main.py:174 - [TRPO] iter = 181000 dist_mean = 0.2924 dist_std = 0.6735 vf_loss = 0.1557 grad_norm = 0.8048 nat_grad_norm = 0.4537 cg_residual = 0.0721 step_size = 0.3477 reward = 0.0000 fps = 10 mse_loss = 1.5279 
2022-07-08 10:26:14.163080 - gail/main.py:174 - [TRPO] iter = 182000 dist_mean = 0.2934 dist_std = 0.6722 vf_loss = 0.1922 grad_norm = 0.6692 nat_grad_norm = 0.5000 cg_residual = 0.0694 step_size = 0.3507 reward = 0.0000 fps = 7 mse_loss = 1.5018 
2022-07-08 10:26:43.915596 - gail/main.py:174 - [TRPO] iter = 183000 dist_mean = 0.2901 dist_std = 0.6668 vf_loss = 0.1503 grad_norm = 0.6644 nat_grad_norm = 0.4932 cg_residual = 0.0474 step_size = 0.3568 reward = -0.0000 fps = 6 mse_loss = 1.6398 
2022-07-08 10:27:12.938175 - gail/main.py:174 - [TRPO] iter = 184000 dist_mean = 0.2935 dist_std = 0.6623 vf_loss = 0.1521 grad_norm = 0.8000 nat_grad_norm = 0.5016 cg_residual = 0.0490 step_size = 0.3448 reward = -0.0000 fps = 5 mse_loss = 1.4395 
2022-07-08 10:27:42.264328 - gail/main.py:174 - [TRPO] iter = 185000 dist_mean = 0.2927 dist_std = 0.6612 vf_loss = 0.1510 grad_norm = 0.7853 nat_grad_norm = 0.5306 cg_residual = 0.1327 step_size = 0.3429 reward = 0.0000 fps = 4 mse_loss = 1.4958 
2022-07-08 10:27:43.134469 - gail/main.py:201 - [Discriminator] iter = 185000 loss = -4.7163 grad_norm = 2.8854 grad_penalty = 0.4767 regularization = 0.0000 true_logits = -0.1972 fake_logits = -5.3902 true_prob = 0.4792 fake_prob = 0.0088 
2022-07-08 10:29:01.819453 - gail/main.py:142 - [Evaluate] iter = 185000 episode={ returns = 407.7731 lengths = 200 } discounted_episode={ returns = 357.3699 lengths = 196 } 
2022-07-08 10:29:31.418366 - gail/main.py:174 - [TRPO] iter = 186000 dist_mean = 0.2904 dist_std = 0.6543 vf_loss = 0.1647 grad_norm = 0.5707 nat_grad_norm = 0.5083 cg_residual = 0.0782 step_size = 0.3792 reward = 0.0000 fps = 9 mse_loss = 1.4684 
2022-07-08 10:30:01.774725 - gail/main.py:174 - [TRPO] iter = 187000 dist_mean = 0.2931 dist_std = 0.6468 vf_loss = 0.1567 grad_norm = 0.6457 nat_grad_norm = 0.4440 cg_residual = 0.0567 step_size = 0.3920 reward = -0.0000 fps = 7 mse_loss = 1.6057 
2022-07-08 10:30:32.150154 - gail/main.py:174 - [TRPO] iter = 188000 dist_mean = 0.2870 dist_std = 0.6471 vf_loss = 0.2453 grad_norm = 0.6192 nat_grad_norm = 0.4418 cg_residual = 0.0620 step_size = 0.3734 reward = -0.0000 fps = 5 mse_loss = 1.5572 
2022-07-08 10:31:02.545753 - gail/main.py:174 - [TRPO] iter = 189000 dist_mean = 0.2846 dist_std = 0.6459 vf_loss = 0.2590 grad_norm = 0.7336 nat_grad_norm = 0.4441 cg_residual = 0.0628 step_size = 0.3738 reward = -0.0000 fps = 5 mse_loss = 1.5070 
2022-07-08 10:31:39.034935 - gail/main.py:174 - [TRPO] iter = 190000 dist_mean = 0.2996 dist_std = 0.6428 vf_loss = 0.2277 grad_norm = 0.7637 nat_grad_norm = 0.5320 cg_residual = 0.0896 step_size = 0.3296 reward = 0.0000 fps = 4 mse_loss = 1.4054 
2022-07-08 10:31:39.878837 - gail/main.py:201 - [Discriminator] iter = 190000 loss = -4.7907 grad_norm = 2.9656 grad_penalty = 0.4838 regularization = 0.0000 true_logits = -0.1914 fake_logits = -5.4659 true_prob = 0.4859 fake_prob = 0.0090 
2022-07-08 10:32:24.727880 - gail/main.py:142 - [Evaluate] iter = 190000 episode={ returns = 485.9811 lengths = 239 } discounted_episode={ returns = 457.8961 lengths = 257 } 
2022-07-08 10:32:34.464491 - gail/main.py:174 - [TRPO] iter = 191000 dist_mean = 0.3103 dist_std = 0.6400 vf_loss = 0.1658 grad_norm = 0.8926 nat_grad_norm = 0.4498 cg_residual = 0.1009 step_size = 0.3680 reward = 0.0000 fps = 18 mse_loss = 1.4567 
2022-07-08 10:32:44.725589 - gail/main.py:174 - [TRPO] iter = 192000 dist_mean = 0.2973 dist_std = 0.6380 vf_loss = 0.2678 grad_norm = 0.7129 nat_grad_norm = 0.4277 cg_residual = 0.0889 step_size = 0.3735 reward = -0.0000 fps = 15 mse_loss = 1.3938 
