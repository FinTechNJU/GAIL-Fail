2022-07-08 09:09:51.124991 - utils/flags.py:257 - log_dir = logs/gail_w-HalfCheetah-v2-200-2022-07-08-09-09-50
2022-07-08 09:10:25.355429 - gail/utils/replay_buffer.py:86 - Load dataset from /content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/HalfCheetah-v2
2022-07-08 09:10:54.281760 - gail/main.py:80 - Expert Reward 6594.485577
2022-07-08 09:10:55.569685 - gail/main.py:84 - Original dataset size 3000
2022-07-08 09:10:55.699447 - gail/main.py:86 - Subsampled dataset size 3000
2022-07-08 09:10:55.726314 - gail/main.py:87 - np random: 864 random : 786
2022-07-08 09:10:55.728378 - gail/main.py:91 - Sampled obs: 0.6422, acs: 0.2570
2022-07-08 09:10:59.645053 - lunzi/nn/flat_param.py:17 - Enabling flattening... ['GaussianMLPPolicy_1/log_std:0', 'GaussianMLPPolicy_1/Variable:0', 'GaussianMLPPolicy_1/Variable_1:0', 'GaussianMLPPolicy_1/Variable_2:0', 'GaussianMLPPolicy_1/Variable_3:0', 'GaussianMLPPolicy_1/Variable_4:0', 'GaussianMLPPolicy_1/Variable_5:0']
2022-07-08 09:11:41.801397 - gail/discriminator/discriminator.py:30 - Use predefined normalization.
2022-07-08 09:11:41.827629 - gail/discriminator/discriminator.py:33 - Normalizer loc:[[-1.0330443e-01 -9.3026832e-03  2.9791275e-01  8.1040427e-02
   1.7489190e-01  4.0859219e-01 -3.0232381e-02 -9.5055901e-02
   7.1118622e+00  5.5424552e-03 -1.8441556e-01  3.1117105e-01
  -2.7393317e-01  2.6658252e-01  4.0665963e-01 -2.8198040e-01
   2.5822452e-01]] 
 scale:[[ 0.03102476  0.07177044  0.37663096  0.5276757   0.46577376  0.12057849
   0.35446092  0.3551628   1.290817    0.4731574   1.4939526  12.314076
  14.637607   14.150712    3.6817648  11.124868    9.962843  ]]
2022-07-08 09:11:59.694286 - gail/discriminator/discriminator.py:67 - Discriminator uses Wasserstein distance.
2022-07-08 09:11:59.699361 - gail/discriminator/discriminator.py:68 - [<tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable:0' shape=(23, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_1:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_2:0' shape=(100, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_3:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_4:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_5:0' shape=(1,) dtype=float32_ref>]
2022-07-08 09:11:59.708911 - gail/discriminator/discriminator.py:69 - Use gradient penalty regularization (coef = 10.000000)
2022-07-08 09:12:03.712244 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions_mean]
2022-07-08 09:16:46.517174 - gail/main.py:142 - [Evaluate] iter = 0 episode={ returns = -0.6418 lengths = 1000 } discounted_episode={ returns = 0.1100 lengths = 1000 } 
2022-07-08 09:16:46.521952 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions]
2022-07-08 09:17:19.071916 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [fake_states fake_actions] => [scaled_neural_reward]
2022-07-08 09:17:20.211059 - lunzi/nn/module.py:71 - [MLPVFunction] is making TensorFlow callables, key = [states] => [values]
2022-07-08 09:17:22.126776 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [] => [sync_old]
2022-07-08 09:17:23.069970 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss flat_grad dist_std mean_kl dist_mean]
2022-07-08 09:17:29.360221 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states tangents actions] => [hessian_vec_prod]
2022-07-08 09:17:40.539680 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [] => [get_flat]
2022-07-08 09:17:41.606125 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [feed_flat] => [set_flat]
2022-07-08 09:17:42.728207 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss mean_kl]
2022-07-08 09:17:44.487062 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [train_vf vf_loss]
2022-07-08 09:17:47.537895 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [vf_loss]
2022-07-08 09:17:48.514052 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states actions_] => [mse_loss]
2022-07-08 09:17:49.510114 - gail/main.py:174 - [TRPO] iter = 1000 dist_mean = -0.0000 dist_std = 1.0000 vf_loss = 0.4141 grad_norm = 0.2861 nat_grad_norm = 0.5188 cg_residual = 0.0000 step_size = 0.4310 reward = -0.0000 fps = 2 mse_loss = 0.9358 
2022-07-08 09:18:12.341573 - gail/main.py:174 - [TRPO] iter = 2000 dist_mean = 0.0087 dist_std = 1.0053 vf_loss = 1.6372 grad_norm = 0.3517 nat_grad_norm = 0.5130 cg_residual = 0.0000 step_size = 0.3746 reward = -0.0000 fps = 2 mse_loss = 0.9822 
2022-07-08 09:18:34.787463 - gail/main.py:174 - [TRPO] iter = 3000 dist_mean = 0.0030 dist_std = 1.0018 vf_loss = 2.1472 grad_norm = 0.3032 nat_grad_norm = 0.4694 cg_residual = 0.0000 step_size = 0.4389 reward = 0.0000 fps = 2 mse_loss = 0.9519 
2022-07-08 09:18:57.788841 - gail/main.py:174 - [TRPO] iter = 4000 dist_mean = -0.0095 dist_std = 1.0049 vf_loss = 2.1587 grad_norm = 0.3233 nat_grad_norm = 0.5197 cg_residual = 0.0000 step_size = 0.3919 reward = -0.0000 fps = 2 mse_loss = 0.9790 
2022-07-08 09:19:19.898690 - gail/main.py:174 - [TRPO] iter = 5000 dist_mean = -0.0356 dist_std = 1.0034 vf_loss = 2.8862 grad_norm = 0.3388 nat_grad_norm = 0.5097 cg_residual = 0.0000 step_size = 0.4144 reward = 0.0000 fps = 2 mse_loss = 1.0049 
2022-07-08 09:19:19.900655 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [true_states true_actions fake_states fake_actions true_masks] => [train loss true_logits fake_logits true_prob fake_prob grad_norm grad_penalty regularization]
2022-07-08 09:19:29.013251 - gail/main.py:201 - [Discriminator] iter = 5000 loss = 3.2285 grad_norm = 59.2099 grad_penalty = 3.0606 regularization = 0.0000 true_logits = 0.3494 fake_logits = 0.5172 true_prob = 0.5860 fake_prob = 0.6230 
2022-07-08 09:24:07.457780 - gail/main.py:142 - [Evaluate] iter = 5000 episode={ returns = -4.6817 lengths = 1000 } discounted_episode={ returns = -3.0303 lengths = 1000 } 
2022-07-08 09:24:29.345963 - gail/main.py:174 - [TRPO] iter = 6000 dist_mean = -0.0240 dist_std = 1.0106 vf_loss = 1.3900 grad_norm = 0.2931 nat_grad_norm = 0.6099 cg_residual = 0.0000 step_size = 0.3823 reward = 0.0000 fps = 3 mse_loss = 0.9997 
2022-07-08 09:24:50.689269 - gail/main.py:174 - [TRPO] iter = 7000 dist_mean = -0.0243 dist_std = 1.0100 vf_loss = 2.5572 grad_norm = 0.3050 nat_grad_norm = 0.6084 cg_residual = 0.0000 step_size = 0.3777 reward = -0.0000 fps = 3 mse_loss = 0.9796 
2022-07-08 09:25:11.612692 - gail/main.py:174 - [TRPO] iter = 8000 dist_mean = -0.0264 dist_std = 1.0076 vf_loss = 3.1389 grad_norm = 0.3330 nat_grad_norm = 0.6053 cg_residual = 0.0000 step_size = 0.3607 reward = 0.0000 fps = 2 mse_loss = 0.9699 
2022-07-08 09:25:33.720400 - gail/main.py:174 - [TRPO] iter = 9000 dist_mean = -0.0240 dist_std = 1.0043 vf_loss = 1.0427 grad_norm = 0.2558 nat_grad_norm = 0.5970 cg_residual = 0.0000 step_size = 0.4260 reward = 0.0000 fps = 2 mse_loss = 0.9951 
2022-07-08 09:25:56.776987 - gail/main.py:174 - [TRPO] iter = 10000 dist_mean = -0.0277 dist_std = 1.0014 vf_loss = 1.8329 grad_norm = 0.2910 nat_grad_norm = 0.5550 cg_residual = 0.0000 step_size = 0.4127 reward = -0.0000 fps = 2 mse_loss = 0.9423 
2022-07-08 09:25:57.698247 - gail/main.py:201 - [Discriminator] iter = 10000 loss = 2.6530 grad_norm = 88.6811 grad_penalty = 2.4638 regularization = 0.0000 true_logits = 0.3660 fake_logits = 0.5552 true_prob = 0.5900 fake_prob = 0.6121 
2022-07-08 09:30:19.892756 - gail/main.py:142 - [Evaluate] iter = 10000 episode={ returns = -6.8716 lengths = 1000 } discounted_episode={ returns = -4.4656 lengths = 1000 } 
2022-07-08 09:30:41.326409 - gail/main.py:174 - [TRPO] iter = 11000 dist_mean = -0.0260 dist_std = 1.0015 vf_loss = 0.4262 grad_norm = 0.3406 nat_grad_norm = 0.8236 cg_residual = 0.0000 step_size = 0.3187 reward = -0.0000 fps = 3 mse_loss = 0.9629 
2022-07-08 09:31:03.521900 - gail/main.py:174 - [TRPO] iter = 12000 dist_mean = -0.0410 dist_std = 1.0010 vf_loss = 0.2303 grad_norm = 0.3133 nat_grad_norm = 0.6312 cg_residual = 0.0000 step_size = 0.3747 reward = -0.0000 fps = 3 mse_loss = 1.0848 
2022-07-08 09:31:25.042734 - gail/main.py:174 - [TRPO] iter = 13000 dist_mean = -0.0427 dist_std = 1.0019 vf_loss = 0.2669 grad_norm = 0.2932 nat_grad_norm = 0.5889 cg_residual = 0.0000 step_size = 0.4030 reward = -0.0000 fps = 3 mse_loss = 1.1235 
2022-07-08 09:31:47.178581 - gail/main.py:174 - [TRPO] iter = 14000 dist_mean = -0.0660 dist_std = 0.9986 vf_loss = 1.0967 grad_norm = 0.3189 nat_grad_norm = 0.5316 cg_residual = 0.0000 step_size = 0.4204 reward = 0.0000 fps = 2 mse_loss = 1.1412 
2022-07-08 09:32:09.756564 - gail/main.py:174 - [TRPO] iter = 15000 dist_mean = -0.0320 dist_std = 1.0015 vf_loss = 0.2936 grad_norm = 0.3352 nat_grad_norm = 0.5151 cg_residual = 0.0000 step_size = 0.4181 reward = 0.0000 fps = 2 mse_loss = 1.1984 
2022-07-08 09:32:11.086459 - gail/main.py:201 - [Discriminator] iter = 15000 loss = -0.8407 grad_norm = 51.4457 grad_penalty = 2.0545 regularization = 0.0000 true_logits = 0.3722 fake_logits = -2.5229 true_prob = 0.5914 fake_prob = 0.0960 
2022-07-08 09:36:27.280657 - gail/main.py:142 - [Evaluate] iter = 15000 episode={ returns = -60.8179 lengths = 1000 } discounted_episode={ returns = -40.2974 lengths = 1000 } 
2022-07-08 09:36:47.846713 - gail/main.py:174 - [TRPO] iter = 16000 dist_mean = -0.0864 dist_std = 1.0041 vf_loss = 1.8560 grad_norm = 0.3647 nat_grad_norm = 0.6201 cg_residual = 0.0006 step_size = 0.3678 reward = 0.0000 fps = 3 mse_loss = 1.2406 
2022-07-08 09:37:09.084806 - gail/main.py:174 - [TRPO] iter = 17000 dist_mean = -0.0365 dist_std = 1.0079 vf_loss = 0.6551 grad_norm = 0.3346 nat_grad_norm = 0.6941 cg_residual = 0.0000 step_size = 0.3407 reward = -0.0000 fps = 3 mse_loss = 1.1828 
2022-07-08 09:37:29.143423 - gail/main.py:174 - [TRPO] iter = 18000 dist_mean = -0.0559 dist_std = 1.0092 vf_loss = 1.8232 grad_norm = 0.3336 nat_grad_norm = 0.6110 cg_residual = 0.0000 step_size = 0.3796 reward = -0.0000 fps = 3 mse_loss = 1.1187 
2022-07-08 09:37:50.025779 - gail/main.py:174 - [TRPO] iter = 19000 dist_mean = -0.0289 dist_std = 1.0100 vf_loss = 1.4518 grad_norm = 0.3555 nat_grad_norm = 0.8006 cg_residual = 0.0000 step_size = 0.2980 reward = 0.0000 fps = 2 mse_loss = 1.1197 
2022-07-08 09:38:10.445933 - gail/main.py:174 - [TRPO] iter = 20000 dist_mean = -0.0733 dist_std = 1.0085 vf_loss = 3.8328 grad_norm = 0.3101 nat_grad_norm = 0.6035 cg_residual = 0.0000 step_size = 0.4160 reward = 0.0000 fps = 2 mse_loss = 1.1207 
2022-07-08 09:38:11.179118 - gail/main.py:201 - [Discriminator] iter = 20000 loss = -0.5113 grad_norm = 27.3569 grad_penalty = 1.6425 regularization = 0.0000 true_logits = 0.4592 fake_logits = -1.6946 true_prob = 0.6119 fake_prob = 0.2288 
2022-07-08 09:42:25.756739 - gail/main.py:142 - [Evaluate] iter = 20000 episode={ returns = -25.3386 lengths = 1000 } discounted_episode={ returns = -15.5967 lengths = 1000 } 
2022-07-08 09:42:45.990445 - gail/main.py:174 - [TRPO] iter = 21000 dist_mean = -0.0301 dist_std = 1.0128 vf_loss = 2.0585 grad_norm = 0.3148 nat_grad_norm = 0.5895 cg_residual = 0.0000 step_size = 0.3921 reward = -0.0000 fps = 3 mse_loss = 1.0922 
2022-07-08 09:43:05.676541 - gail/main.py:174 - [TRPO] iter = 22000 dist_mean = -0.0699 dist_std = 1.0152 vf_loss = 4.8085 grad_norm = 0.3111 nat_grad_norm = 0.5994 cg_residual = 0.0000 step_size = 0.4035 reward = 0.0000 fps = 3 mse_loss = 1.1281 
2022-07-08 09:43:25.368748 - gail/main.py:174 - [TRPO] iter = 23000 dist_mean = -0.0174 dist_std = 1.0155 vf_loss = 1.1725 grad_norm = 0.3290 nat_grad_norm = 0.7889 cg_residual = 0.0000 step_size = 0.3398 reward = 0.0000 fps = 3 mse_loss = 1.0657 
2022-07-08 09:43:45.102940 - gail/main.py:174 - [TRPO] iter = 24000 dist_mean = -0.0662 dist_std = 1.0166 vf_loss = 3.5571 grad_norm = 0.3203 nat_grad_norm = 0.5250 cg_residual = 0.0000 step_size = 0.4207 reward = 0.0000 fps = 2 mse_loss = 1.0829 
2022-07-08 09:44:05.551179 - gail/main.py:174 - [TRPO] iter = 25000 dist_mean = -0.0794 dist_std = 1.0126 vf_loss = 7.0886 grad_norm = 0.3970 nat_grad_norm = 0.5752 cg_residual = 0.0001 step_size = 0.3858 reward = 0.0000 fps = 2 mse_loss = 1.0767 
2022-07-08 09:44:06.346961 - gail/main.py:201 - [Discriminator] iter = 25000 loss = -0.4800 grad_norm = 20.0757 grad_penalty = 1.2794 regularization = 0.0000 true_logits = 0.5141 fake_logits = -1.2453 true_prob = 0.6246 fake_prob = 0.2872 
2022-07-08 09:48:07.088322 - gail/main.py:142 - [Evaluate] iter = 25000 episode={ returns = -56.8373 lengths = 1000 } discounted_episode={ returns = -35.3050 lengths = 1000 } 
2022-07-08 09:48:26.372554 - gail/main.py:174 - [TRPO] iter = 26000 dist_mean = -0.0369 dist_std = 1.0160 vf_loss = 4.8016 grad_norm = 0.3486 nat_grad_norm = 0.6777 cg_residual = 0.0000 step_size = 0.3624 reward = 0.0000 fps = 3 mse_loss = 1.1096 
2022-07-08 09:48:46.500121 - gail/main.py:174 - [TRPO] iter = 27000 dist_mean = -0.1170 dist_std = 1.0130 vf_loss = 9.8445 grad_norm = 0.3750 nat_grad_norm = 0.5894 cg_residual = 0.0002 step_size = 0.3764 reward = 0.0000 fps = 3 mse_loss = 1.1774 
2022-07-08 09:49:06.534187 - gail/main.py:174 - [TRPO] iter = 28000 dist_mean = -0.1107 dist_std = 1.0162 vf_loss = 3.4319 grad_norm = 0.2922 nat_grad_norm = 0.5508 cg_residual = 0.0004 step_size = 0.4436 reward = 0.0000 fps = 3 mse_loss = 1.0981 
2022-07-08 09:49:27.359881 - gail/main.py:174 - [TRPO] iter = 29000 dist_mean = -0.1238 dist_std = 1.0147 vf_loss = 5.0379 grad_norm = 0.3054 nat_grad_norm = 0.5759 cg_residual = 0.0004 step_size = 0.4317 reward = 0.0000 fps = 3 mse_loss = 1.0942 
2022-07-08 09:49:49.937907 - gail/main.py:174 - [TRPO] iter = 30000 dist_mean = -0.0070 dist_std = 1.0192 vf_loss = 3.3210 grad_norm = 0.3411 nat_grad_norm = 0.6142 cg_residual = 0.0000 step_size = 0.3785 reward = -0.0000 fps = 2 mse_loss = 1.0909 
2022-07-08 09:49:50.812203 - gail/main.py:201 - [Discriminator] iter = 30000 loss = -1.9315 grad_norm = 25.5681 grad_penalty = 1.2411 regularization = 0.0000 true_logits = 0.5777 fake_logits = -2.5949 true_prob = 0.6390 fake_prob = 0.0899 
2022-07-08 09:54:04.081518 - gail/main.py:142 - [Evaluate] iter = 30000 episode={ returns = 17.0251 lengths = 1000 } discounted_episode={ returns = 10.5760 lengths = 1000 } 
2022-07-08 09:54:25.755751 - gail/main.py:174 - [TRPO] iter = 31000 dist_mean = -0.0150 dist_std = 1.0270 vf_loss = 3.1839 grad_norm = 0.3266 nat_grad_norm = 0.5777 cg_residual = 0.0000 step_size = 0.3901 reward = -0.0000 fps = 3 mse_loss = 1.1227 
2022-07-08 09:54:46.809200 - gail/main.py:174 - [TRPO] iter = 32000 dist_mean = -0.0330 dist_std = 1.0269 vf_loss = 5.5782 grad_norm = 0.3140 nat_grad_norm = 0.5685 cg_residual = 0.0000 step_size = 0.4239 reward = -0.0000 fps = 3 mse_loss = 1.0809 
2022-07-08 09:55:07.644380 - gail/main.py:174 - [TRPO] iter = 33000 dist_mean = -0.0992 dist_std = 1.0289 vf_loss = 4.8228 grad_norm = 0.3678 nat_grad_norm = 0.5714 cg_residual = 0.0002 step_size = 0.4058 reward = 0.0000 fps = 3 mse_loss = 1.1236 
2022-07-08 09:55:27.110428 - gail/main.py:174 - [TRPO] iter = 34000 dist_mean = -0.0510 dist_std = 1.0302 vf_loss = 2.0687 grad_norm = 0.3693 nat_grad_norm = 0.6430 cg_residual = 0.0004 step_size = 0.3805 reward = 0.0000 fps = 2 mse_loss = 1.0947 
2022-07-08 09:55:46.383993 - gail/main.py:174 - [TRPO] iter = 35000 dist_mean = -0.0897 dist_std = 1.0241 vf_loss = 3.7127 grad_norm = 0.3298 nat_grad_norm = 0.5750 cg_residual = 0.0001 step_size = 0.4112 reward = 0.0000 fps = 2 mse_loss = 1.1487 
2022-07-08 09:55:47.133275 - gail/main.py:201 - [Discriminator] iter = 35000 loss = 0.0243 grad_norm = 30.2598 grad_penalty = 1.4084 regularization = 0.0000 true_logits = 0.6174 fake_logits = -0.7667 true_prob = 0.6478 fake_prob = 0.3430 
2022-07-08 09:59:39.786010 - gail/main.py:142 - [Evaluate] iter = 35000 episode={ returns = 162.1926 lengths = 1000 } discounted_episode={ returns = 96.1086 lengths = 1000 } 
2022-07-08 10:00:00.099564 - gail/main.py:174 - [TRPO] iter = 36000 dist_mean = -0.0271 dist_std = 1.0272 vf_loss = 2.6562 grad_norm = 0.3226 nat_grad_norm = 0.5643 cg_residual = 0.0000 step_size = 0.4140 reward = 0.0000 fps = 3 mse_loss = 1.1758 
2022-07-08 10:00:20.857428 - gail/main.py:174 - [TRPO] iter = 37000 dist_mean = -0.0524 dist_std = 1.0254 vf_loss = 1.4240 grad_norm = 0.3810 nat_grad_norm = 0.7172 cg_residual = 0.0006 step_size = 0.3378 reward = 0.0000 fps = 3 mse_loss = 1.2621 
2022-07-08 10:00:41.624396 - gail/main.py:174 - [TRPO] iter = 38000 dist_mean = 0.0201 dist_std = 1.0314 vf_loss = 2.1474 grad_norm = 0.3741 nat_grad_norm = 0.7603 cg_residual = 0.0000 step_size = 0.3179 reward = 0.0000 fps = 3 mse_loss = 1.2820 
2022-07-08 10:01:02.997074 - gail/main.py:174 - [TRPO] iter = 39000 dist_mean = 0.0285 dist_std = 1.0295 vf_loss = 0.6608 grad_norm = 0.4024 nat_grad_norm = 0.7745 cg_residual = 0.0001 step_size = 0.3127 reward = -0.0000 fps = 3 mse_loss = 1.2558 
2022-07-08 10:01:23.590074 - gail/main.py:174 - [TRPO] iter = 40000 dist_mean = -0.0507 dist_std = 1.0329 vf_loss = 2.5836 grad_norm = 0.3553 nat_grad_norm = 0.5985 cg_residual = 0.0000 step_size = 0.3932 reward = 0.0000 fps = 2 mse_loss = 1.2950 
2022-07-08 10:01:24.397333 - gail/main.py:201 - [Discriminator] iter = 40000 loss = -1.1525 grad_norm = 21.9718 grad_penalty = 1.3343 regularization = 0.0000 true_logits = 0.6464 fake_logits = -1.8405 true_prob = 0.6537 fake_prob = 0.1989 
2022-07-08 10:06:11.775064 - gail/main.py:142 - [Evaluate] iter = 40000 episode={ returns = 356.5229 lengths = 1000 } discounted_episode={ returns = 223.8941 lengths = 1000 } 
2022-07-08 10:06:35.546436 - gail/main.py:174 - [TRPO] iter = 41000 dist_mean = -0.0245 dist_std = 1.0384 vf_loss = 5.7357 grad_norm = 0.3393 nat_grad_norm = 0.6239 cg_residual = 0.0000 step_size = 0.3883 reward = -0.0000 fps = 3 mse_loss = 1.3906 
2022-07-08 10:06:59.381882 - gail/main.py:174 - [TRPO] iter = 42000 dist_mean = -0.0526 dist_std = 1.0384 vf_loss = 9.1997 grad_norm = 0.3113 nat_grad_norm = 0.5990 cg_residual = 0.0000 step_size = 0.4148 reward = 0.0000 fps = 2 mse_loss = 1.4081 
2022-07-08 10:07:22.815166 - gail/main.py:174 - [TRPO] iter = 43000 dist_mean = -0.0132 dist_std = 1.0334 vf_loss = 4.0203 grad_norm = 0.2786 nat_grad_norm = 0.5468 cg_residual = 0.0000 step_size = 0.4816 reward = 0.0000 fps = 2 mse_loss = 1.4938 
2022-07-08 10:07:45.294162 - gail/main.py:174 - [TRPO] iter = 44000 dist_mean = 0.0170 dist_std = 1.0367 vf_loss = 2.4584 grad_norm = 0.3049 nat_grad_norm = 0.6305 cg_residual = 0.0011 step_size = 0.4001 reward = 0.0000 fps = 2 mse_loss = 1.5395 
2022-07-08 10:08:06.862562 - gail/main.py:174 - [TRPO] iter = 45000 dist_mean = 0.0237 dist_std = 1.0340 vf_loss = 6.1691 grad_norm = 0.3537 nat_grad_norm = 0.6847 cg_residual = 0.0000 step_size = 0.3759 reward = 0.0000 fps = 2 mse_loss = 1.5703 
2022-07-08 10:08:07.647806 - gail/main.py:201 - [Discriminator] iter = 45000 loss = -2.2016 grad_norm = 20.3683 grad_penalty = 1.1765 regularization = 0.0000 true_logits = 0.7290 fake_logits = -2.6492 true_prob = 0.6714 fake_prob = 0.1091 
2022-07-08 10:12:56.271671 - gail/main.py:142 - [Evaluate] iter = 45000 episode={ returns = 306.8668 lengths = 1000 } discounted_episode={ returns = 194.8634 lengths = 1000 } 
2022-07-08 10:13:18.488959 - gail/main.py:174 - [TRPO] iter = 46000 dist_mean = -0.0548 dist_std = 1.0311 vf_loss = 8.3197 grad_norm = 0.3244 nat_grad_norm = 0.5766 cg_residual = 0.0001 step_size = 0.4250 reward = 0.0000 fps = 3 mse_loss = 1.6330 
2022-07-08 10:13:40.568428 - gail/main.py:174 - [TRPO] iter = 47000 dist_mean = -0.0369 dist_std = 1.0385 vf_loss = 2.8471 grad_norm = 0.3710 nat_grad_norm = 0.6975 cg_residual = 0.0011 step_size = 0.3383 reward = 0.0000 fps = 3 mse_loss = 1.7711 
2022-07-08 10:14:04.016191 - gail/main.py:174 - [TRPO] iter = 48000 dist_mean = -0.0529 dist_std = 1.0382 vf_loss = 1.4429 grad_norm = 0.3577 nat_grad_norm = 0.6201 cg_residual = 0.0001 step_size = 0.3957 reward = 0.0000 fps = 2 mse_loss = 1.8224 
2022-07-08 10:14:27.207137 - gail/main.py:174 - [TRPO] iter = 49000 dist_mean = 0.0436 dist_std = 1.0404 vf_loss = 2.1715 grad_norm = 0.3275 nat_grad_norm = 0.6237 cg_residual = 0.0000 step_size = 0.3895 reward = 0.0000 fps = 2 mse_loss = 1.8796 
2022-07-08 10:14:50.280324 - gail/main.py:174 - [TRPO] iter = 50000 dist_mean = 0.0228 dist_std = 1.0384 vf_loss = 3.1236 grad_norm = 0.3255 nat_grad_norm = 0.6122 cg_residual = 0.0000 step_size = 0.4129 reward = 0.0000 fps = 2 mse_loss = 1.7930 
2022-07-08 10:14:51.157044 - gail/main.py:201 - [Discriminator] iter = 50000 loss = -2.5619 grad_norm = 15.2932 grad_penalty = 0.9944 regularization = 0.0000 true_logits = 0.8209 fake_logits = -2.7354 true_prob = 0.6908 fake_prob = 0.1124 
2022-07-08 10:19:21.691947 - gail/main.py:142 - [Evaluate] iter = 50000 episode={ returns = -10.5158 lengths = 1000 } discounted_episode={ returns = -5.4559 lengths = 1000 } 
2022-07-08 10:19:44.713749 - gail/main.py:174 - [TRPO] iter = 51000 dist_mean = -0.0143 dist_std = 1.0385 vf_loss = 5.6458 grad_norm = 0.4316 nat_grad_norm = 0.6731 cg_residual = 0.0001 step_size = 0.3445 reward = 0.0000 fps = 3 mse_loss = 1.8708 
2022-07-08 10:20:07.209037 - gail/main.py:174 - [TRPO] iter = 52000 dist_mean = 0.0335 dist_std = 1.0476 vf_loss = 8.3126 grad_norm = 0.3674 nat_grad_norm = 0.7518 cg_residual = 0.0000 step_size = 0.3380 reward = 0.0000 fps = 3 mse_loss = 1.8890 
2022-07-08 10:20:30.073823 - gail/main.py:174 - [TRPO] iter = 53000 dist_mean = 0.0323 dist_std = 1.0494 vf_loss = 1.6635 grad_norm = 0.3905 nat_grad_norm = 0.6819 cg_residual = 0.0002 step_size = 0.3456 reward = -0.0000 fps = 2 mse_loss = 1.9585 
2022-07-08 10:20:52.679438 - gail/main.py:174 - [TRPO] iter = 54000 dist_mean = 0.0298 dist_std = 1.0503 vf_loss = 1.2810 grad_norm = 0.3399 nat_grad_norm = 0.6250 cg_residual = 0.0003 step_size = 0.3728 reward = -0.0000 fps = 2 mse_loss = 2.1630 
2022-07-08 10:21:15.729946 - gail/main.py:174 - [TRPO] iter = 55000 dist_mean = -0.0300 dist_std = 1.0508 vf_loss = 2.9118 grad_norm = 0.3723 nat_grad_norm = 0.5928 cg_residual = 0.0001 step_size = 0.4060 reward = 0.0000 fps = 2 mse_loss = 2.0450 
2022-07-08 10:21:16.599553 - gail/main.py:201 - [Discriminator] iter = 55000 loss = -1.5895 grad_norm = 18.9950 grad_penalty = 1.1853 regularization = 0.0000 true_logits = 0.9502 fake_logits = -1.8246 true_prob = 0.7162 fake_prob = 0.1836 
2022-07-08 10:25:52.065825 - gail/main.py:142 - [Evaluate] iter = 55000 episode={ returns = -24.9582 lengths = 1000 } discounted_episode={ returns = -16.3155 lengths = 1000 } 
2022-07-08 10:26:16.088924 - gail/main.py:174 - [TRPO] iter = 56000 dist_mean = -0.0658 dist_std = 1.0503 vf_loss = 4.7005 grad_norm = 0.3944 nat_grad_norm = 0.7810 cg_residual = 0.0008 step_size = 0.3530 reward = -0.0000 fps = 3 mse_loss = 2.2479 
2022-07-08 10:26:40.076589 - gail/main.py:174 - [TRPO] iter = 57000 dist_mean = 0.0451 dist_std = 1.0528 vf_loss = 2.7114 grad_norm = 0.3908 nat_grad_norm = 0.7676 cg_residual = 0.0002 step_size = 0.3197 reward = 0.0000 fps = 3 mse_loss = 2.2251 
2022-07-08 10:27:03.988945 - gail/main.py:174 - [TRPO] iter = 58000 dist_mean = 0.0267 dist_std = 1.0566 vf_loss = 1.2706 grad_norm = 0.3707 nat_grad_norm = 0.6691 cg_residual = 0.0000 step_size = 0.3675 reward = 0.0000 fps = 2 mse_loss = 2.2996 
2022-07-08 10:27:28.354607 - gail/main.py:174 - [TRPO] iter = 59000 dist_mean = 0.0341 dist_std = 1.0567 vf_loss = 2.3063 grad_norm = 0.3293 nat_grad_norm = 0.6792 cg_residual = 0.0001 step_size = 0.3888 reward = -0.0000 fps = 2 mse_loss = 2.3944 
2022-07-08 10:27:51.758423 - gail/main.py:174 - [TRPO] iter = 60000 dist_mean = -0.0315 dist_std = 1.0542 vf_loss = 2.2479 grad_norm = 0.4484 nat_grad_norm = 0.7907 cg_residual = 0.0003 step_size = 0.3269 reward = -0.0000 fps = 2 mse_loss = 2.4027 
2022-07-08 10:27:52.604912 - gail/main.py:201 - [Discriminator] iter = 60000 loss = -1.8656 grad_norm = 12.5605 grad_penalty = 0.7667 regularization = 0.0000 true_logits = 1.0443 fake_logits = -1.5881 true_prob = 0.7344 fake_prob = 0.1980 
