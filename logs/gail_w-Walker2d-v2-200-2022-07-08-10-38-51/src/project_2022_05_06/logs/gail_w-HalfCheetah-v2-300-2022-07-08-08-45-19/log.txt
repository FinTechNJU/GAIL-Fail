2022-07-08 08:45:19.450855 - utils/flags.py:257 - log_dir = logs/gail_w-HalfCheetah-v2-300-2022-07-08-08-45-19
2022-07-08 08:46:24.572282 - gail/utils/replay_buffer.py:86 - Load dataset from /content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/HalfCheetah-v2
2022-07-08 08:46:33.223396 - gail/main.py:80 - Expert Reward 6594.485577
2022-07-08 08:46:33.616362 - gail/main.py:84 - Original dataset size 3000
2022-07-08 08:46:33.659233 - gail/main.py:86 - Subsampled dataset size 3000
2022-07-08 08:46:33.664872 - gail/main.py:87 - np random: 864 random : 786
2022-07-08 08:46:33.669293 - gail/main.py:91 - Sampled obs: 0.6422, acs: 0.2570
2022-07-08 08:46:34.700041 - lunzi/nn/flat_param.py:17 - Enabling flattening... ['GaussianMLPPolicy_1/log_std:0', 'GaussianMLPPolicy_1/Variable:0', 'GaussianMLPPolicy_1/Variable_1:0', 'GaussianMLPPolicy_1/Variable_2:0', 'GaussianMLPPolicy_1/Variable_3:0', 'GaussianMLPPolicy_1/Variable_4:0', 'GaussianMLPPolicy_1/Variable_5:0']
2022-07-08 08:46:41.841199 - gail/discriminator/discriminator.py:30 - Use predefined normalization.
2022-07-08 08:46:41.847326 - gail/discriminator/discriminator.py:33 - Normalizer loc:[[-1.0330443e-01 -9.3026832e-03  2.9791275e-01  8.1040427e-02
   1.7489190e-01  4.0859219e-01 -3.0232381e-02 -9.5055901e-02
   7.1118622e+00  5.5424552e-03 -1.8441556e-01  3.1117105e-01
  -2.7393317e-01  2.6658252e-01  4.0665963e-01 -2.8198040e-01
   2.5822452e-01]] 
 scale:[[ 0.03102476  0.07177044  0.37663096  0.5276757   0.46577376  0.12057849
   0.35446092  0.3551628   1.290817    0.4731574   1.4939526  12.314076
  14.637607   14.150712    3.6817648  11.124868    9.962843  ]]
2022-07-08 08:46:45.099808 - gail/discriminator/discriminator.py:67 - Discriminator uses Wasserstein distance.
2022-07-08 08:46:45.101811 - gail/discriminator/discriminator.py:68 - [<tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable:0' shape=(23, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_1:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_2:0' shape=(100, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_3:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_4:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_5:0' shape=(1,) dtype=float32_ref>]
2022-07-08 08:46:45.102904 - gail/discriminator/discriminator.py:69 - Use gradient penalty regularization (coef = 10.000000)
2022-07-08 08:46:45.885752 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions_mean]
2022-07-08 08:47:53.889629 - gail/main.py:142 - [Evaluate] iter = 0 episode={ returns = 0.0497 lengths = 1000 } discounted_episode={ returns = -0.2886 lengths = 1000 } 
2022-07-08 08:47:53.892035 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions]
2022-07-08 08:48:01.874157 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [fake_states fake_actions] => [scaled_neural_reward]
2022-07-08 08:48:02.192638 - lunzi/nn/module.py:71 - [MLPVFunction] is making TensorFlow callables, key = [states] => [values]
2022-07-08 08:48:02.695179 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [] => [sync_old]
2022-07-08 08:48:02.946478 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss flat_grad dist_std mean_kl dist_mean]
2022-07-08 08:48:04.329907 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states tangents actions] => [hessian_vec_prod]
2022-07-08 08:48:06.990830 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [] => [get_flat]
2022-07-08 08:48:07.322476 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [feed_flat] => [set_flat]
2022-07-08 08:48:07.591943 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss mean_kl]
2022-07-08 08:48:08.068737 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [train_vf vf_loss]
2022-07-08 08:48:08.899181 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [vf_loss]
2022-07-08 08:48:09.175872 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states actions_] => [mse_loss]
2022-07-08 08:48:09.436977 - gail/main.py:174 - [TRPO] iter = 1000 dist_mean = -0.0000 dist_std = 1.0000 vf_loss = 0.9310 grad_norm = 0.3044 nat_grad_norm = 0.5189 cg_residual = 0.0000 step_size = 0.4012 reward = -0.0000 fps = 12 mse_loss = 0.9230 
2022-07-08 08:48:15.014914 - gail/main.py:174 - [TRPO] iter = 2000 dist_mean = 0.0076 dist_std = 0.9946 vf_loss = 0.8663 grad_norm = 0.3123 nat_grad_norm = 0.5171 cg_residual = 0.0000 step_size = 0.4079 reward = -0.0000 fps = 11 mse_loss = 0.9096 
2022-07-08 08:48:20.893256 - gail/main.py:174 - [TRPO] iter = 3000 dist_mean = 0.0113 dist_std = 0.9992 vf_loss = 1.8100 grad_norm = 0.2948 nat_grad_norm = 0.5528 cg_residual = 0.0000 step_size = 0.3915 reward = 0.0000 fps = 10 mse_loss = 0.9029 
2022-07-08 08:48:26.844317 - gail/main.py:174 - [TRPO] iter = 4000 dist_mean = 0.0131 dist_std = 1.0011 vf_loss = 2.5428 grad_norm = 0.3180 nat_grad_norm = 0.5054 cg_residual = 0.0000 step_size = 0.4217 reward = 0.0000 fps = 9 mse_loss = 0.9105 
2022-07-08 08:48:32.655542 - gail/main.py:174 - [TRPO] iter = 5000 dist_mean = 0.0048 dist_std = 1.0023 vf_loss = 1.0869 grad_norm = 0.2934 nat_grad_norm = 0.6174 cg_residual = 0.0001 step_size = 0.3895 reward = 0.0000 fps = 9 mse_loss = 0.8791 
2022-07-08 08:48:32.657969 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [true_states true_actions fake_states fake_actions true_masks] => [train loss true_logits fake_logits true_prob fake_prob grad_norm grad_penalty regularization]
2022-07-08 08:48:35.079870 - gail/main.py:201 - [Discriminator] iter = 5000 loss = 1.9551 grad_norm = 80.9477 grad_penalty = 1.7928 regularization = 0.0000 true_logits = -0.0399 fake_logits = 0.1225 true_prob = 0.4902 fake_prob = 0.5273 
2022-07-08 08:49:46.962903 - gail/main.py:142 - [Evaluate] iter = 5000 episode={ returns = -1.7388 lengths = 1000 } discounted_episode={ returns = -0.7453 lengths = 1000 } 
2022-07-08 08:49:52.891449 - gail/main.py:174 - [TRPO] iter = 6000 dist_mean = 0.0025 dist_std = 1.0057 vf_loss = 0.5450 grad_norm = 0.3096 nat_grad_norm = 0.5398 cg_residual = 0.0000 step_size = 0.3837 reward = 0.0000 fps = 12 mse_loss = 0.9021 
2022-07-08 08:49:58.547585 - gail/main.py:174 - [TRPO] iter = 7000 dist_mean = -0.0038 dist_std = 1.0050 vf_loss = 0.1864 grad_norm = 0.2801 nat_grad_norm = 0.4563 cg_residual = 0.0000 step_size = 0.4640 reward = -0.0000 fps = 11 mse_loss = 0.8705 
2022-07-08 08:50:07.939355 - gail/main.py:174 - [TRPO] iter = 8000 dist_mean = 0.0139 dist_std = 1.0151 vf_loss = 0.3025 grad_norm = 0.3201 nat_grad_norm = 0.5450 cg_residual = 0.0000 step_size = 0.3828 reward = -0.0000 fps = 10 mse_loss = 0.8830 
2022-07-08 08:50:15.948557 - gail/main.py:174 - [TRPO] iter = 9000 dist_mean = 0.0264 dist_std = 1.0125 vf_loss = 0.2016 grad_norm = 0.3185 nat_grad_norm = 0.5782 cg_residual = 0.0000 step_size = 0.4086 reward = 0.0000 fps = 9 mse_loss = 0.9440 
2022-07-08 08:50:22.245057 - gail/main.py:174 - [TRPO] iter = 10000 dist_mean = 0.0239 dist_std = 1.0178 vf_loss = 0.5258 grad_norm = 0.2946 nat_grad_norm = 0.5352 cg_residual = 0.0000 step_size = 0.4058 reward = -0.0000 fps = 9 mse_loss = 0.9555 
2022-07-08 08:50:22.539342 - gail/main.py:201 - [Discriminator] iter = 10000 loss = 0.0920 grad_norm = 47.7800 grad_penalty = 1.9564 regularization = 0.0000 true_logits = 0.0672 fake_logits = -1.7972 true_prob = 0.5167 fake_prob = 0.1803 
2022-07-08 08:51:31.889793 - gail/main.py:142 - [Evaluate] iter = 10000 episode={ returns = -14.3423 lengths = 1000 } discounted_episode={ returns = -8.9294 lengths = 1000 } 
2022-07-08 08:51:37.484798 - gail/main.py:174 - [TRPO] iter = 11000 dist_mean = 0.0249 dist_std = 1.0209 vf_loss = 1.9844 grad_norm = 0.3148 nat_grad_norm = 0.5348 cg_residual = 0.0000 step_size = 0.3959 reward = -0.0000 fps = 13 mse_loss = 0.9599 
2022-07-08 08:51:43.385004 - gail/main.py:174 - [TRPO] iter = 12000 dist_mean = 0.0304 dist_std = 1.0232 vf_loss = 2.2726 grad_norm = 0.3083 nat_grad_norm = 0.6730 cg_residual = 0.0000 step_size = 0.3563 reward = 0.0000 fps = 12 mse_loss = 1.0812 
2022-07-08 08:51:49.315728 - gail/main.py:174 - [TRPO] iter = 13000 dist_mean = 0.0014 dist_std = 1.0236 vf_loss = 5.7882 grad_norm = 0.2803 nat_grad_norm = 0.4871 cg_residual = 0.0000 step_size = 0.4550 reward = 0.0000 fps = 11 mse_loss = 0.9914 
2022-07-08 08:51:54.991053 - gail/main.py:174 - [TRPO] iter = 14000 dist_mean = -0.0133 dist_std = 1.0200 vf_loss = 6.8675 grad_norm = 0.3228 nat_grad_norm = 0.5562 cg_residual = 0.0000 step_size = 0.4018 reward = 0.0000 fps = 10 mse_loss = 1.0269 
2022-07-08 08:52:00.658980 - gail/main.py:174 - [TRPO] iter = 15000 dist_mean = 0.0387 dist_std = 1.0142 vf_loss = 4.7836 grad_norm = 0.3106 nat_grad_norm = 0.5628 cg_residual = 0.0000 step_size = 0.3987 reward = 0.0000 fps = 10 mse_loss = 0.9920 
2022-07-08 08:52:00.889318 - gail/main.py:201 - [Discriminator] iter = 15000 loss = -1.2221 grad_norm = 24.5887 grad_penalty = 1.6917 regularization = 0.0000 true_logits = 0.1376 fake_logits = -2.7762 true_prob = 0.5340 fake_prob = 0.1200 
2022-07-08 08:53:06.391211 - gail/main.py:142 - [Evaluate] iter = 15000 episode={ returns = -16.1296 lengths = 1000 } discounted_episode={ returns = -10.2912 lengths = 1000 } 
2022-07-08 08:53:12.045050 - gail/main.py:174 - [TRPO] iter = 16000 dist_mean = 0.0583 dist_std = 1.0140 vf_loss = 2.2366 grad_norm = 0.3028 nat_grad_norm = 0.6005 cg_residual = 0.0000 step_size = 0.3810 reward = 0.0000 fps = 14 mse_loss = 0.9942 
2022-07-08 08:53:17.669710 - gail/main.py:174 - [TRPO] iter = 17000 dist_mean = 0.0640 dist_std = 1.0195 vf_loss = 0.9651 grad_norm = 0.3105 nat_grad_norm = 0.7245 cg_residual = 0.0001 step_size = 0.3580 reward = 0.0000 fps = 13 mse_loss = 1.0556 
2022-07-08 08:53:23.569499 - gail/main.py:174 - [TRPO] iter = 18000 dist_mean = 0.0288 dist_std = 1.0215 vf_loss = 12.0084 grad_norm = 0.3581 nat_grad_norm = 0.5606 cg_residual = 0.0001 step_size = 0.3846 reward = 0.0000 fps = 12 mse_loss = 1.0047 
2022-07-08 08:53:29.914298 - gail/main.py:174 - [TRPO] iter = 19000 dist_mean = -0.0137 dist_std = 1.0154 vf_loss = 1.5065 grad_norm = 0.3489 nat_grad_norm = 0.5931 cg_residual = 0.0003 step_size = 0.3623 reward = -0.0000 fps = 11 mse_loss = 1.0726 
2022-07-08 08:53:36.230732 - gail/main.py:174 - [TRPO] iter = 20000 dist_mean = 0.0375 dist_std = 1.0201 vf_loss = 1.9757 grad_norm = 0.3471 nat_grad_norm = 0.5715 cg_residual = 0.0000 step_size = 0.3821 reward = -0.0000 fps = 10 mse_loss = 0.9998 
2022-07-08 08:53:36.477548 - gail/main.py:201 - [Discriminator] iter = 20000 loss = 1.1496 grad_norm = 27.3559 grad_penalty = 2.1111 regularization = 0.0000 true_logits = 0.2078 fake_logits = -0.7537 true_prob = 0.5508 fake_prob = 0.3235 
2022-07-08 08:54:43.279688 - gail/main.py:142 - [Evaluate] iter = 20000 episode={ returns = -32.8444 lengths = 1000 } discounted_episode={ returns = -20.5234 lengths = 1000 } 
2022-07-08 08:54:49.297982 - gail/main.py:174 - [TRPO] iter = 21000 dist_mean = 0.0249 dist_std = 1.0216 vf_loss = 2.8469 grad_norm = 0.3170 nat_grad_norm = 0.5373 cg_residual = 0.0000 step_size = 0.4206 reward = -0.0000 fps = 13 mse_loss = 1.0349 
2022-07-08 08:54:55.898299 - gail/main.py:174 - [TRPO] iter = 22000 dist_mean = 0.0568 dist_std = 1.0162 vf_loss = 2.1972 grad_norm = 0.3541 nat_grad_norm = 0.6090 cg_residual = 0.0000 step_size = 0.3527 reward = 0.0000 fps = 12 mse_loss = 1.1193 
2022-07-08 08:55:02.790469 - gail/main.py:174 - [TRPO] iter = 23000 dist_mean = 0.0331 dist_std = 1.0218 vf_loss = 3.5304 grad_norm = 0.3232 nat_grad_norm = 0.5602 cg_residual = 0.0000 step_size = 0.3985 reward = -0.0000 fps = 11 mse_loss = 1.1045 
2022-07-08 08:55:09.499985 - gail/main.py:174 - [TRPO] iter = 24000 dist_mean = 0.0264 dist_std = 1.0242 vf_loss = 3.0527 grad_norm = 0.2965 nat_grad_norm = 0.4703 cg_residual = 0.0000 step_size = 0.4851 reward = 0.0000 fps = 10 mse_loss = 1.1268 
2022-07-08 08:55:15.665021 - gail/main.py:174 - [TRPO] iter = 25000 dist_mean = 0.0403 dist_std = 1.0321 vf_loss = 2.2743 grad_norm = 0.3114 nat_grad_norm = 0.5511 cg_residual = 0.0000 step_size = 0.4185 reward = -0.0000 fps = 10 mse_loss = 1.1435 
2022-07-08 08:55:15.930978 - gail/main.py:201 - [Discriminator] iter = 25000 loss = 0.2046 grad_norm = 20.0469 grad_penalty = 1.4515 regularization = 0.0000 true_logits = 0.2889 fake_logits = -0.9580 true_prob = 0.5697 fake_prob = 0.2811 
2022-07-08 08:56:21.771239 - gail/main.py:142 - [Evaluate] iter = 25000 episode={ returns = -29.1826 lengths = 1000 } discounted_episode={ returns = -18.5164 lengths = 1000 } 
2022-07-08 08:56:27.671782 - gail/main.py:174 - [TRPO] iter = 26000 dist_mean = 0.0517 dist_std = 1.0287 vf_loss = 4.5853 grad_norm = 0.3048 nat_grad_norm = 0.4883 cg_residual = 0.0000 step_size = 0.4531 reward = -0.0000 fps = 13 mse_loss = 1.1608 
2022-07-08 08:56:33.888056 - gail/main.py:174 - [TRPO] iter = 27000 dist_mean = 0.0533 dist_std = 1.0212 vf_loss = 2.8777 grad_norm = 0.3353 nat_grad_norm = 0.5534 cg_residual = 0.0000 step_size = 0.4101 reward = -0.0000 fps = 12 mse_loss = 1.0613 
2022-07-08 08:56:39.999442 - gail/main.py:174 - [TRPO] iter = 28000 dist_mean = 0.0523 dist_std = 1.0167 vf_loss = 0.5546 grad_norm = 0.3213 nat_grad_norm = 0.5261 cg_residual = 0.0000 step_size = 0.4329 reward = -0.0000 fps = 11 mse_loss = 1.0754 
2022-07-08 08:56:45.646258 - gail/main.py:174 - [TRPO] iter = 29000 dist_mean = 0.1236 dist_std = 1.0155 vf_loss = 1.2730 grad_norm = 0.3684 nat_grad_norm = 0.5696 cg_residual = 0.0001 step_size = 0.3779 reward = -0.0000 fps = 11 mse_loss = 1.0890 
2022-07-08 08:56:51.578014 - gail/main.py:174 - [TRPO] iter = 30000 dist_mean = 0.0554 dist_std = 1.0168 vf_loss = 1.0473 grad_norm = 0.3237 nat_grad_norm = 0.5172 cg_residual = 0.0000 step_size = 0.4342 reward = -0.0000 fps = 10 mse_loss = 1.0433 
2022-07-08 08:56:51.789036 - gail/main.py:201 - [Discriminator] iter = 30000 loss = -0.3026 grad_norm = 17.2650 grad_penalty = 1.2467 regularization = 0.0000 true_logits = 0.3698 fake_logits = -1.1796 true_prob = 0.5882 fake_prob = 0.2401 
2022-07-08 08:57:58.372993 - gail/main.py:142 - [Evaluate] iter = 30000 episode={ returns = -31.2151 lengths = 1000 } discounted_episode={ returns = -19.5088 lengths = 1000 } 
2022-07-08 08:58:04.132457 - gail/main.py:174 - [TRPO] iter = 31000 dist_mean = 0.0698 dist_std = 1.0104 vf_loss = 1.4108 grad_norm = 0.3304 nat_grad_norm = 0.5987 cg_residual = 0.0001 step_size = 0.4108 reward = 0.0000 fps = 13 mse_loss = 1.0862 
2022-07-08 08:58:09.712072 - gail/main.py:174 - [TRPO] iter = 32000 dist_mean = -0.0901 dist_std = 1.0178 vf_loss = 1.2166 grad_norm = 0.4585 nat_grad_norm = 0.8512 cg_residual = 0.0024 step_size = 0.3035 reward = 0.0000 fps = 12 mse_loss = 1.0479 
2022-07-08 08:58:15.465721 - gail/main.py:174 - [TRPO] iter = 33000 dist_mean = 0.1103 dist_std = 1.0217 vf_loss = 0.9871 grad_norm = 0.3649 nat_grad_norm = 0.6060 cg_residual = 0.0001 step_size = 0.3859 reward = 0.0000 fps = 11 mse_loss = 1.1601 
2022-07-08 08:58:20.995374 - gail/main.py:174 - [TRPO] iter = 34000 dist_mean = 0.1045 dist_std = 1.0166 vf_loss = 0.8172 grad_norm = 0.4214 nat_grad_norm = 0.5571 cg_residual = 0.0001 step_size = 0.3900 reward = 0.0000 fps = 11 mse_loss = 1.1570 
2022-07-08 08:58:26.695800 - gail/main.py:174 - [TRPO] iter = 35000 dist_mean = 0.1042 dist_std = 1.0133 vf_loss = 0.9728 grad_norm = 0.3330 nat_grad_norm = 0.5311 cg_residual = 0.0001 step_size = 0.4299 reward = -0.0000 fps = 10 mse_loss = 1.1221 
2022-07-08 08:58:26.964647 - gail/main.py:201 - [Discriminator] iter = 35000 loss = -0.5935 grad_norm = 20.3795 grad_penalty = 1.2482 regularization = 0.0000 true_logits = 0.4288 fake_logits = -1.4129 true_prob = 0.6011 fake_prob = 0.2009 
2022-07-08 08:59:33.906058 - gail/main.py:142 - [Evaluate] iter = 35000 episode={ returns = -33.4928 lengths = 1000 } discounted_episode={ returns = -21.0073 lengths = 1000 } 
2022-07-08 08:59:39.905018 - gail/main.py:174 - [TRPO] iter = 36000 dist_mean = 0.1275 dist_std = 1.0151 vf_loss = 2.7468 grad_norm = 0.3716 nat_grad_norm = 0.6467 cg_residual = 0.0001 step_size = 0.3593 reward = -0.0000 fps = 13 mse_loss = 1.1033 
2022-07-08 08:59:45.852312 - gail/main.py:174 - [TRPO] iter = 37000 dist_mean = 0.1173 dist_std = 1.0129 vf_loss = 0.6896 grad_norm = 0.3904 nat_grad_norm = 0.5693 cg_residual = 0.0001 step_size = 0.3958 reward = 0.0000 fps = 12 mse_loss = 1.1564 
2022-07-08 08:59:51.590799 - gail/main.py:174 - [TRPO] iter = 38000 dist_mean = 0.1204 dist_std = 1.0174 vf_loss = 0.4355 grad_norm = 0.5082 nat_grad_norm = 0.5706 cg_residual = 0.0001 step_size = 0.3470 reward = 0.0000 fps = 11 mse_loss = 1.1007 
2022-07-08 08:59:57.378359 - gail/main.py:174 - [TRPO] iter = 39000 dist_mean = 0.1397 dist_std = 1.0204 vf_loss = 0.4001 grad_norm = 0.4612 nat_grad_norm = 0.5855 cg_residual = 0.0001 step_size = 0.3586 reward = 0.0000 fps = 11 mse_loss = 0.9920 
2022-07-08 09:00:03.856485 - gail/main.py:174 - [TRPO] iter = 40000 dist_mean = 0.1456 dist_std = 1.0227 vf_loss = 0.5515 grad_norm = 0.4527 nat_grad_norm = 0.5435 cg_residual = 0.0001 step_size = 0.3910 reward = -0.0000 fps = 10 mse_loss = 1.0281 
2022-07-08 09:00:04.271508 - gail/main.py:201 - [Discriminator] iter = 40000 loss = -1.1275 grad_norm = 15.2255 grad_penalty = 0.9479 regularization = 0.0000 true_logits = 0.5212 fake_logits = -1.5542 true_prob = 0.6210 fake_prob = 0.1813 
2022-07-08 09:01:18.542013 - gail/main.py:142 - [Evaluate] iter = 40000 episode={ returns = -34.1951 lengths = 1000 } discounted_episode={ returns = -21.7724 lengths = 1000 } 
2022-07-08 09:01:23.950200 - gail/main.py:174 - [TRPO] iter = 41000 dist_mean = 0.1551 dist_std = 1.0181 vf_loss = 0.3698 grad_norm = 0.5068 nat_grad_norm = 0.5745 cg_residual = 0.0001 step_size = 0.3454 reward = 0.0000 fps = 12 mse_loss = 0.9402 
2022-07-08 09:01:29.830176 - gail/main.py:174 - [TRPO] iter = 42000 dist_mean = 0.1667 dist_std = 1.0137 vf_loss = 0.5348 grad_norm = 0.3802 nat_grad_norm = 0.5790 cg_residual = 0.0001 step_size = 0.3889 reward = 0.0000 fps = 11 mse_loss = 0.9016 
2022-07-08 09:01:35.337960 - gail/main.py:174 - [TRPO] iter = 43000 dist_mean = 0.3855 dist_std = 1.0122 vf_loss = 0.3970 grad_norm = 0.3657 nat_grad_norm = 0.7944 cg_residual = 0.0036 step_size = 0.3102 reward = 0.0000 fps = 10 mse_loss = 1.0559 
2022-07-08 09:01:40.777297 - gail/main.py:174 - [TRPO] iter = 44000 dist_mean = 0.2137 dist_std = 1.0122 vf_loss = 0.5646 grad_norm = 0.3842 nat_grad_norm = 0.5464 cg_residual = 0.0001 step_size = 0.4097 reward = 0.0000 fps = 10 mse_loss = 0.9620 
2022-07-08 09:01:46.468550 - gail/main.py:174 - [TRPO] iter = 45000 dist_mean = 0.2204 dist_std = 1.0104 vf_loss = 0.4237 grad_norm = 0.4770 nat_grad_norm = 0.5485 cg_residual = 0.0001 step_size = 0.3939 reward = -0.0000 fps = 9 mse_loss = 1.0136 
2022-07-08 09:01:46.693918 - gail/main.py:201 - [Discriminator] iter = 45000 loss = -1.5381 grad_norm = 13.5048 grad_penalty = 0.8845 regularization = 0.0000 true_logits = 0.6105 fake_logits = -1.8121 true_prob = 0.6391 fake_prob = 0.1479 
2022-07-08 09:02:54.093205 - gail/main.py:142 - [Evaluate] iter = 45000 episode={ returns = -33.3467 lengths = 1000 } discounted_episode={ returns = -20.8312 lengths = 1000 } 
2022-07-08 09:03:00.181117 - gail/main.py:174 - [TRPO] iter = 46000 dist_mean = 0.2291 dist_std = 1.0137 vf_loss = 0.4417 grad_norm = 0.4096 nat_grad_norm = 0.5569 cg_residual = 0.0001 step_size = 0.3971 reward = -0.0000 fps = 13 mse_loss = 1.0014 
2022-07-08 09:03:06.028149 - gail/main.py:174 - [TRPO] iter = 47000 dist_mean = 0.0323 dist_std = 1.0132 vf_loss = 0.3443 grad_norm = 0.5520 nat_grad_norm = 0.9630 cg_residual = 0.0038 step_size = 0.2728 reward = -0.0000 fps = 12 mse_loss = 1.1538 
2022-07-08 09:03:11.885013 - gail/main.py:174 - [TRPO] iter = 48000 dist_mean = 0.2324 dist_std = 1.0193 vf_loss = 0.5848 grad_norm = 0.3663 nat_grad_norm = 0.5649 cg_residual = 0.0002 step_size = 0.4119 reward = 0.0000 fps = 11 mse_loss = 1.0796 
2022-07-08 09:03:17.626542 - gail/main.py:174 - [TRPO] iter = 49000 dist_mean = 0.2317 dist_std = 1.0216 vf_loss = 0.5568 grad_norm = 0.4132 nat_grad_norm = 0.5796 cg_residual = 0.0003 step_size = 0.4018 reward = 0.0000 fps = 10 mse_loss = 1.0624 
2022-07-08 09:03:23.327458 - gail/main.py:174 - [TRPO] iter = 50000 dist_mean = 0.2381 dist_std = 1.0209 vf_loss = 0.4377 grad_norm = 0.4411 nat_grad_norm = 0.5880 cg_residual = 0.0002 step_size = 0.3853 reward = -0.0000 fps = 10 mse_loss = 1.0246 
2022-07-08 09:03:23.587292 - gail/main.py:201 - [Discriminator] iter = 50000 loss = -1.5074 grad_norm = 19.3714 grad_penalty = 0.9845 regularization = 0.0000 true_logits = 0.6667 fake_logits = -1.8252 true_prob = 0.6500 fake_prob = 0.1444 
2022-07-08 09:04:43.914454 - gail/main.py:142 - [Evaluate] iter = 50000 episode={ returns = -48.7324 lengths = 1000 } discounted_episode={ returns = -29.8998 lengths = 1000 } 
2022-07-08 09:04:54.810274 - gail/main.py:174 - [TRPO] iter = 51000 dist_mean = 0.2645 dist_std = 1.0218 vf_loss = 0.5650 grad_norm = 0.4244 nat_grad_norm = 0.5592 cg_residual = 0.0001 step_size = 0.3849 reward = -0.0000 fps = 10 mse_loss = 1.0329 
2022-07-08 09:05:05.864337 - gail/main.py:174 - [TRPO] iter = 52000 dist_mean = 0.2572 dist_std = 1.0198 vf_loss = 0.5545 grad_norm = 0.4299 nat_grad_norm = 0.5538 cg_residual = 0.0002 step_size = 0.4036 reward = -0.0000 fps = 9 mse_loss = 1.0814 
2022-07-08 09:05:20.169750 - gail/main.py:174 - [TRPO] iter = 53000 dist_mean = 0.2735 dist_std = 1.0219 vf_loss = 0.2840 grad_norm = 0.5379 nat_grad_norm = 0.6236 cg_residual = 0.0003 step_size = 0.3336 reward = -0.0000 fps = 8 mse_loss = 1.0514 
2022-07-08 09:05:33.806509 - gail/main.py:174 - [TRPO] iter = 54000 dist_mean = 0.2739 dist_std = 1.0253 vf_loss = 0.3985 grad_norm = 0.4405 nat_grad_norm = 0.6155 cg_residual = 0.0003 step_size = 0.3702 reward = -0.0000 fps = 7 mse_loss = 1.0410 
2022-07-08 09:05:47.731290 - gail/main.py:174 - [TRPO] iter = 55000 dist_mean = 0.2715 dist_std = 1.0276 vf_loss = 0.4056 grad_norm = 0.4407 nat_grad_norm = 0.5833 cg_residual = 0.0002 step_size = 0.3833 reward = 0.0000 fps = 6 mse_loss = 0.9731 
2022-07-08 09:05:48.202274 - gail/main.py:201 - [Discriminator] iter = 55000 loss = -1.8907 grad_norm = 19.5786 grad_penalty = 0.9540 regularization = 0.0000 true_logits = 0.7383 fake_logits = -2.1064 true_prob = 0.6645 fake_prob = 0.1135 
2022-07-08 09:08:27.039157 - gail/main.py:142 - [Evaluate] iter = 55000 episode={ returns = -61.2603 lengths = 1000 } discounted_episode={ returns = -35.3850 lengths = 1000 } 
2022-07-08 09:08:39.761751 - gail/main.py:174 - [TRPO] iter = 56000 dist_mean = 0.2864 dist_std = 1.0305 vf_loss = 0.3135 grad_norm = 0.4207 nat_grad_norm = 0.5822 cg_residual = 0.0003 step_size = 0.3905 reward = -0.0000 fps = 5 mse_loss = 1.0435 
2022-07-08 09:08:52.777343 - gail/main.py:174 - [TRPO] iter = 57000 dist_mean = 0.2899 dist_std = 1.0324 vf_loss = 0.5071 grad_norm = 0.4107 nat_grad_norm = 0.5510 cg_residual = 0.0002 step_size = 0.4007 reward = 0.0000 fps = 5 mse_loss = 1.0106 
2022-07-08 09:09:09.237781 - gail/main.py:174 - [TRPO] iter = 58000 dist_mean = 0.2898 dist_std = 1.0351 vf_loss = 0.3118 grad_norm = 0.4117 nat_grad_norm = 0.5517 cg_residual = 0.0003 step_size = 0.4117 reward = 0.0000 fps = 4 mse_loss = 1.0581 
2022-07-08 09:09:29.345286 - gail/main.py:174 - [TRPO] iter = 59000 dist_mean = 0.2946 dist_std = 1.0351 vf_loss = 0.3060 grad_norm = 0.5070 nat_grad_norm = 0.6147 cg_residual = 0.0003 step_size = 0.3563 reward = 0.0000 fps = 4 mse_loss = 0.9545 
2022-07-08 09:09:47.089135 - gail/main.py:174 - [TRPO] iter = 60000 dist_mean = 0.2892 dist_std = 1.0404 vf_loss = 0.3036 grad_norm = 0.4401 nat_grad_norm = 0.5820 cg_residual = 0.0004 step_size = 0.3924 reward = -0.0000 fps = 4 mse_loss = 0.9172 
2022-07-08 09:09:47.682032 - gail/main.py:201 - [Discriminator] iter = 60000 loss = -2.3826 grad_norm = 16.9095 grad_penalty = 0.7978 regularization = 0.0000 true_logits = 0.8491 fake_logits = -2.3312 true_prob = 0.6837 fake_prob = 0.0939 
2022-07-08 09:14:22.994900 - gail/main.py:142 - [Evaluate] iter = 60000 episode={ returns = -96.4857 lengths = 1000 } discounted_episode={ returns = -56.5462 lengths = 1000 } 
2022-07-08 09:14:47.658326 - gail/main.py:174 - [TRPO] iter = 61000 dist_mean = 0.2860 dist_std = 1.0362 vf_loss = 0.3403 grad_norm = 0.4576 nat_grad_norm = 0.5800 cg_residual = 0.0005 step_size = 0.3665 reward = 0.0000 fps = 3 mse_loss = 0.8737 
2022-07-08 09:15:12.210861 - gail/main.py:174 - [TRPO] iter = 62000 dist_mean = 0.2916 dist_std = 1.0378 vf_loss = 0.3683 grad_norm = 0.4514 nat_grad_norm = 0.5992 cg_residual = 0.0006 step_size = 0.3631 reward = 0.0000 fps = 3 mse_loss = 0.8492 
2022-07-08 09:15:35.601783 - gail/main.py:174 - [TRPO] iter = 63000 dist_mean = 0.2947 dist_std = 1.0405 vf_loss = 0.3069 grad_norm = 0.4099 nat_grad_norm = 0.6199 cg_residual = 0.0003 step_size = 0.3674 reward = 0.0000 fps = 2 mse_loss = 0.8492 
2022-07-08 09:15:58.679520 - gail/main.py:174 - [TRPO] iter = 64000 dist_mean = 0.2955 dist_std = 1.0391 vf_loss = 0.3841 grad_norm = 0.3911 nat_grad_norm = 0.5810 cg_residual = 0.0005 step_size = 0.4031 reward = -0.0000 fps = 2 mse_loss = 0.8472 
2022-07-08 09:16:20.845266 - gail/main.py:174 - [TRPO] iter = 65000 dist_mean = 0.2871 dist_std = 1.0419 vf_loss = 0.4481 grad_norm = 0.4698 nat_grad_norm = 0.6055 cg_residual = 0.0005 step_size = 0.3562 reward = -0.0000 fps = 2 mse_loss = 0.8890 
2022-07-08 09:16:21.614083 - gail/main.py:201 - [Discriminator] iter = 65000 loss = -2.9395 grad_norm = 18.8935 grad_penalty = 0.6377 regularization = 0.0000 true_logits = 0.9108 fake_logits = -2.6665 true_prob = 0.6950 fake_prob = 0.0704 
2022-07-08 09:20:57.013683 - gail/main.py:142 - [Evaluate] iter = 65000 episode={ returns = -146.5866 lengths = 1000 } discounted_episode={ returns = -87.4492 lengths = 1000 } 
2022-07-08 09:21:19.123803 - gail/main.py:174 - [TRPO] iter = 66000 dist_mean = 0.2785 dist_std = 1.0424 vf_loss = 0.3360 grad_norm = 0.4013 nat_grad_norm = 0.6420 cg_residual = 0.0004 step_size = 0.3722 reward = 0.0000 fps = 3 mse_loss = 0.8371 
2022-07-08 09:21:41.491673 - gail/main.py:174 - [TRPO] iter = 67000 dist_mean = 0.2890 dist_std = 1.0425 vf_loss = 0.5313 grad_norm = 0.3795 nat_grad_norm = 0.5910 cg_residual = 0.0006 step_size = 0.4023 reward = 0.0000 fps = 3 mse_loss = 0.8040 
2022-07-08 09:22:10.520288 - gail/main.py:174 - [TRPO] iter = 68000 dist_mean = 0.2877 dist_std = 1.0445 vf_loss = 0.6444 grad_norm = 0.4016 nat_grad_norm = 0.5814 cg_residual = 0.0007 step_size = 0.4024 reward = 0.0000 fps = 2 mse_loss = 0.7682 
2022-07-08 09:22:32.579495 - gail/main.py:174 - [TRPO] iter = 69000 dist_mean = 0.2871 dist_std = 1.0443 vf_loss = 0.4080 grad_norm = 0.3745 nat_grad_norm = 0.6159 cg_residual = 0.0006 step_size = 0.4184 reward = 0.0000 fps = 2 mse_loss = 0.7680 
2022-07-08 09:22:56.186698 - gail/main.py:174 - [TRPO] iter = 70000 dist_mean = 0.2729 dist_std = 1.0455 vf_loss = 0.3034 grad_norm = 0.4298 nat_grad_norm = 0.6456 cg_residual = 0.0008 step_size = 0.3646 reward = 0.0000 fps = 2 mse_loss = 0.7786 
2022-07-08 09:22:57.048098 - gail/main.py:201 - [Discriminator] iter = 70000 loss = -3.4366 grad_norm = 14.5353 grad_penalty = 0.6012 regularization = 0.0000 true_logits = 0.9979 fake_logits = -3.0399 true_prob = 0.7109 fake_prob = 0.0509 
2022-07-08 09:27:10.549171 - gail/main.py:142 - [Evaluate] iter = 70000 episode={ returns = 449.6975 lengths = 1000 } discounted_episode={ returns = 283.5373 lengths = 1000 } 
2022-07-08 09:27:31.662705 - gail/main.py:174 - [TRPO] iter = 71000 dist_mean = 0.2861 dist_std = 1.0457 vf_loss = 0.6181 grad_norm = 0.4024 nat_grad_norm = 0.6403 cg_residual = 0.0007 step_size = 0.3791 reward = -0.0000 fps = 3 mse_loss = 0.8217 
2022-07-08 09:27:52.522535 - gail/main.py:174 - [TRPO] iter = 72000 dist_mean = 0.2886 dist_std = 1.0520 vf_loss = 0.6328 grad_norm = 0.4038 nat_grad_norm = 0.7659 cg_residual = 0.0010 step_size = 0.3559 reward = -0.0000 fps = 3 mse_loss = 0.8025 
2022-07-08 09:28:13.688390 - gail/main.py:174 - [TRPO] iter = 73000 dist_mean = 0.2562 dist_std = 1.0475 vf_loss = 0.6387 grad_norm = 0.3602 nat_grad_norm = 0.6356 cg_residual = 0.0010 step_size = 0.4292 reward = 0.0000 fps = 3 mse_loss = 0.8558 
2022-07-08 09:28:35.009787 - gail/main.py:174 - [TRPO] iter = 74000 dist_mean = 0.2957 dist_std = 1.0457 vf_loss = 0.4838 grad_norm = 0.4155 nat_grad_norm = 0.6341 cg_residual = 0.0010 step_size = 0.3827 reward = 0.0000 fps = 2 mse_loss = 0.7592 
2022-07-08 09:28:56.819996 - gail/main.py:174 - [TRPO] iter = 75000 dist_mean = 0.3083 dist_std = 1.0447 vf_loss = 0.6926 grad_norm = 0.4185 nat_grad_norm = 0.6453 cg_residual = 0.0011 step_size = 0.3711 reward = 0.0000 fps = 2 mse_loss = 0.7731 
2022-07-08 09:28:57.669876 - gail/main.py:201 - [Discriminator] iter = 75000 loss = -3.8703 grad_norm = 17.0170 grad_penalty = 0.6296 regularization = 0.0000 true_logits = 1.1255 fake_logits = -3.3744 true_prob = 0.7316 fake_prob = 0.0394 
2022-07-08 09:33:04.250069 - gail/main.py:142 - [Evaluate] iter = 75000 episode={ returns = 847.1400 lengths = 1000 } discounted_episode={ returns = 520.7121 lengths = 1000 } 
2022-07-08 09:33:25.357511 - gail/main.py:174 - [TRPO] iter = 76000 dist_mean = 0.3751 dist_std = 1.0463 vf_loss = 0.2711 grad_norm = 0.3919 nat_grad_norm = 0.7078 cg_residual = 0.0087 step_size = 0.3427 reward = 0.0000 fps = 3 mse_loss = 0.8260 
2022-07-08 09:33:47.371931 - gail/main.py:174 - [TRPO] iter = 77000 dist_mean = 0.2749 dist_std = 1.0513 vf_loss = 0.7314 grad_norm = 0.3916 nat_grad_norm = 0.7354 cg_residual = 0.0007 step_size = 0.3576 reward = -0.0000 fps = 3 mse_loss = 0.8324 
2022-07-08 09:34:07.780686 - gail/main.py:174 - [TRPO] iter = 78000 dist_mean = 0.2535 dist_std = 1.0557 vf_loss = 0.7092 grad_norm = 0.4498 nat_grad_norm = 0.6718 cg_residual = 0.0013 step_size = 0.3560 reward = 0.0000 fps = 3 mse_loss = 0.8032 
2022-07-08 09:34:28.588613 - gail/main.py:174 - [TRPO] iter = 79000 dist_mean = 0.2587 dist_std = 1.0536 vf_loss = 0.7093 grad_norm = 0.4554 nat_grad_norm = 0.6625 cg_residual = 0.0010 step_size = 0.3597 reward = 0.0000 fps = 3 mse_loss = 0.8461 
2022-07-08 09:34:49.956755 - gail/main.py:174 - [TRPO] iter = 80000 dist_mean = 0.2675 dist_std = 1.0507 vf_loss = 0.6877 grad_norm = 0.3835 nat_grad_norm = 0.7173 cg_residual = 0.0013 step_size = 0.3792 reward = -0.0000 fps = 2 mse_loss = 0.8395 
2022-07-08 09:34:50.774022 - gail/main.py:201 - [Discriminator] iter = 80000 loss = -4.5741 grad_norm = 15.8623 grad_penalty = 0.5564 regularization = 0.0000 true_logits = 1.2271 fake_logits = -3.9035 true_prob = 0.7463 fake_prob = 0.0245 
2022-07-08 09:38:42.889193 - gail/main.py:142 - [Evaluate] iter = 80000 episode={ returns = 1061.3467 lengths = 1000 } discounted_episode={ returns = 693.1174 lengths = 1000 } 
2022-07-08 09:39:03.137853 - gail/main.py:174 - [TRPO] iter = 81000 dist_mean = 0.2666 dist_std = 1.0553 vf_loss = 0.5827 grad_norm = 0.3989 nat_grad_norm = 0.6312 cg_residual = 0.0007 step_size = 0.4051 reward = -0.0000 fps = 3 mse_loss = 0.7964 
2022-07-08 09:39:23.095930 - gail/main.py:174 - [TRPO] iter = 82000 dist_mean = 0.2738 dist_std = 1.0550 vf_loss = 0.5836 grad_norm = 0.4301 nat_grad_norm = 0.6808 cg_residual = 0.0015 step_size = 0.3699 reward = -0.0000 fps = 3 mse_loss = 0.8000 
2022-07-08 09:39:44.579753 - gail/main.py:174 - [TRPO] iter = 83000 dist_mean = 0.2658 dist_std = 1.0516 vf_loss = 0.6016 grad_norm = 0.4776 nat_grad_norm = 0.7066 cg_residual = 0.0011 step_size = 0.3591 reward = 0.0000 fps = 3 mse_loss = 0.7544 
2022-07-08 09:40:05.700449 - gail/main.py:174 - [TRPO] iter = 84000 dist_mean = 0.2845 dist_std = 1.0544 vf_loss = 1.6338 grad_norm = 0.3863 nat_grad_norm = 0.7431 cg_residual = 0.0017 step_size = 0.3593 reward = 0.0000 fps = 3 mse_loss = 0.7528 
2022-07-08 09:40:27.119303 - gail/main.py:174 - [TRPO] iter = 85000 dist_mean = 0.2835 dist_std = 1.0581 vf_loss = 0.4617 grad_norm = 0.4116 nat_grad_norm = 0.6842 cg_residual = 0.0055 step_size = 0.3512 reward = 0.0000 fps = 2 mse_loss = 0.7486 
2022-07-08 09:40:27.885803 - gail/main.py:201 - [Discriminator] iter = 85000 loss = -5.9686 grad_norm = 25.9468 grad_penalty = 1.0632 regularization = 0.0000 true_logits = 1.1955 fake_logits = -5.8364 true_prob = 0.7406 fake_prob = 0.0060 
2022-07-08 09:44:18.614520 - gail/main.py:142 - [Evaluate] iter = 85000 episode={ returns = 1356.4249 lengths = 1000 } discounted_episode={ returns = 812.1685 lengths = 1000 } 
2022-07-08 09:44:38.492100 - gail/main.py:174 - [TRPO] iter = 86000 dist_mean = 0.2443 dist_std = 1.0543 vf_loss = 0.8158 grad_norm = 0.4586 nat_grad_norm = 0.6918 cg_residual = 0.0017 step_size = 0.3680 reward = -0.0000 fps = 3 mse_loss = 0.6906 
2022-07-08 09:44:58.498858 - gail/main.py:174 - [TRPO] iter = 87000 dist_mean = 0.2560 dist_std = 1.0505 vf_loss = 0.7555 grad_norm = 0.4559 nat_grad_norm = 0.6748 cg_residual = 0.0025 step_size = 0.3798 reward = -0.0000 fps = 3 mse_loss = 0.7047 
2022-07-08 09:45:17.904353 - gail/main.py:174 - [TRPO] iter = 88000 dist_mean = 0.2388 dist_std = 1.0442 vf_loss = 0.7964 grad_norm = 0.4544 nat_grad_norm = 0.7200 cg_residual = 0.0030 step_size = 0.3578 reward = -0.0000 fps = 3 mse_loss = 0.7514 
2022-07-08 09:45:37.856613 - gail/main.py:174 - [TRPO] iter = 89000 dist_mean = 0.2380 dist_std = 1.0404 vf_loss = 0.7984 grad_norm = 0.4286 nat_grad_norm = 0.7497 cg_residual = 0.0020 step_size = 0.3513 reward = 0.0000 fps = 3 mse_loss = 0.7304 
2022-07-08 09:45:57.294126 - gail/main.py:174 - [TRPO] iter = 90000 dist_mean = 0.2642 dist_std = 1.0399 vf_loss = 0.8144 grad_norm = 0.4263 nat_grad_norm = 0.7169 cg_residual = 0.0031 step_size = 0.3743 reward = 0.0000 fps = 3 mse_loss = 0.7388 
2022-07-08 09:45:58.087564 - gail/main.py:201 - [Discriminator] iter = 90000 loss = -5.1245 grad_norm = 22.9540 grad_penalty = 0.7618 regularization = 0.0000 true_logits = 1.2990 fake_logits = -4.5873 true_prob = 0.7566 fake_prob = 0.0142 
2022-07-08 09:49:41.353742 - gail/main.py:142 - [Evaluate] iter = 90000 episode={ returns = 1495.4076 lengths = 1000 } discounted_episode={ returns = 943.1762 lengths = 1000 } 
2022-07-08 09:50:03.861193 - gail/main.py:174 - [TRPO] iter = 91000 dist_mean = 0.2540 dist_std = 1.0352 vf_loss = 0.6319 grad_norm = 0.4276 nat_grad_norm = 0.7357 cg_residual = 0.0039 step_size = 0.3614 reward = -0.0000 fps = 4 mse_loss = 0.7196 
2022-07-08 09:50:25.337969 - gail/main.py:174 - [TRPO] iter = 92000 dist_mean = 0.2229 dist_std = 1.0339 vf_loss = 0.7521 grad_norm = 0.4522 nat_grad_norm = 0.7430 cg_residual = 0.0033 step_size = 0.3517 reward = 0.0000 fps = 3 mse_loss = 0.7343 
2022-07-08 09:50:45.145446 - gail/main.py:174 - [TRPO] iter = 93000 dist_mean = 0.2421 dist_std = 1.0320 vf_loss = 0.6219 grad_norm = 0.3977 nat_grad_norm = 0.6959 cg_residual = 0.0028 step_size = 0.3805 reward = 0.0000 fps = 3 mse_loss = 0.7577 
2022-07-08 09:51:04.428504 - gail/main.py:174 - [TRPO] iter = 94000 dist_mean = 0.2458 dist_std = 1.0350 vf_loss = 1.0486 grad_norm = 0.4318 nat_grad_norm = 0.6951 cg_residual = 0.0030 step_size = 0.3731 reward = 0.0000 fps = 3 mse_loss = 0.6547 
2022-07-08 09:51:23.384878 - gail/main.py:174 - [TRPO] iter = 95000 dist_mean = 0.2153 dist_std = 1.0300 vf_loss = 0.8488 grad_norm = 0.3938 nat_grad_norm = 0.6900 cg_residual = 0.0048 step_size = 0.3906 reward = 0.0000 fps = 3 mse_loss = 0.7703 
2022-07-08 09:51:24.107653 - gail/main.py:201 - [Discriminator] iter = 95000 loss = -5.6909 grad_norm = 19.2359 grad_penalty = 0.6485 regularization = 0.0000 true_logits = 1.3514 fake_logits = -4.9879 true_prob = 0.7609 fake_prob = 0.0101 
2022-07-08 09:55:21.289168 - gail/main.py:142 - [Evaluate] iter = 95000 episode={ returns = 1756.5987 lengths = 1000 } discounted_episode={ returns = 1114.5013 lengths = 1000 } 
2022-07-08 09:55:40.042517 - gail/main.py:174 - [TRPO] iter = 96000 dist_mean = 0.2427 dist_std = 1.0286 vf_loss = 0.8949 grad_norm = 0.4588 nat_grad_norm = 0.7082 cg_residual = 0.0041 step_size = 0.3405 reward = -0.0000 fps = 3 mse_loss = 0.7180 
2022-07-08 09:55:59.268303 - gail/main.py:174 - [TRPO] iter = 97000 dist_mean = 0.2392 dist_std = 1.0274 vf_loss = 0.8943 grad_norm = 0.4669 nat_grad_norm = 0.6994 cg_residual = 0.0038 step_size = 0.3553 reward = -0.0000 fps = 3 mse_loss = 0.7503 
2022-07-08 09:56:18.139579 - gail/main.py:174 - [TRPO] iter = 98000 dist_mean = 0.3450 dist_std = 1.0275 vf_loss = 0.9667 grad_norm = 0.4039 nat_grad_norm = 0.7305 cg_residual = 0.0056 step_size = 0.3635 reward = 0.0000 fps = 3 mse_loss = 0.7885 
2022-07-08 09:56:36.815843 - gail/main.py:174 - [TRPO] iter = 99000 dist_mean = 0.2157 dist_std = 1.0244 vf_loss = 0.8731 grad_norm = 0.4577 nat_grad_norm = 0.7686 cg_residual = 0.0051 step_size = 0.3515 reward = 0.0000 fps = 3 mse_loss = 0.7725 
2022-07-08 09:56:55.869635 - gail/main.py:174 - [TRPO] iter = 100000 dist_mean = 0.2507 dist_std = 1.0209 vf_loss = 1.0455 grad_norm = 0.4875 nat_grad_norm = 0.7399 cg_residual = 0.0045 step_size = 0.3443 reward = -0.0000 fps = 3 mse_loss = 0.7941 
2022-07-08 09:56:56.610141 - gail/main.py:201 - [Discriminator] iter = 100000 loss = -5.9876 grad_norm = 15.7712 grad_penalty = 0.7935 regularization = 0.0000 true_logits = 1.3426 fake_logits = -5.4385 true_prob = 0.7586 fake_prob = 0.0072 
2022-07-08 10:00:40.046117 - gail/main.py:142 - [Evaluate] iter = 100000 episode={ returns = 1803.7722 lengths = 1000 } discounted_episode={ returns = 1187.3167 lengths = 1000 } 
2022-07-08 10:01:01.919342 - gail/main.py:174 - [TRPO] iter = 101000 dist_mean = 0.2340 dist_std = 1.0139 vf_loss = 0.9787 grad_norm = 0.4212 nat_grad_norm = 0.6929 cg_residual = 0.0039 step_size = 0.3883 reward = 0.0000 fps = 4 mse_loss = 0.7465 
2022-07-08 10:01:23.169210 - gail/main.py:174 - [TRPO] iter = 102000 dist_mean = 0.4034 dist_std = 1.0117 vf_loss = 1.0652 grad_norm = 0.3942 nat_grad_norm = 0.7195 cg_residual = 0.0111 step_size = 0.3381 reward = 0.0000 fps = 3 mse_loss = 0.7210 
2022-07-08 10:01:46.046316 - gail/main.py:174 - [TRPO] iter = 103000 dist_mean = 0.1473 dist_std = 1.0111 vf_loss = 0.6021 grad_norm = 0.4740 nat_grad_norm = 0.7348 cg_residual = 0.0090 step_size = 0.3415 reward = -0.0000 fps = 3 mse_loss = 0.6904 
2022-07-08 10:02:14.936461 - gail/main.py:174 - [TRPO] iter = 104000 dist_mean = 0.2272 dist_std = 1.0061 vf_loss = 2.2912 grad_norm = 0.4803 nat_grad_norm = 0.7296 cg_residual = 0.0045 step_size = 0.3643 reward = -0.0000 fps = 3 mse_loss = 0.6509 
2022-07-08 10:02:37.084418 - gail/main.py:174 - [TRPO] iter = 105000 dist_mean = 0.2286 dist_std = 1.0078 vf_loss = 1.3949 grad_norm = 0.5435 nat_grad_norm = 0.7342 cg_residual = 0.0071 step_size = 0.3416 reward = 0.0000 fps = 2 mse_loss = 0.7032 
2022-07-08 10:02:37.878412 - gail/main.py:201 - [Discriminator] iter = 105000 loss = -6.2273 grad_norm = 21.5968 grad_penalty = 0.9374 regularization = 0.0000 true_logits = 1.3921 fake_logits = -5.7726 true_prob = 0.7616 fake_prob = 0.0056 
2022-07-08 10:06:57.746460 - gail/main.py:142 - [Evaluate] iter = 105000 episode={ returns = 2109.6934 lengths = 1000 } discounted_episode={ returns = 1325.2312 lengths = 1000 } 
2022-07-08 10:07:21.332462 - gail/main.py:174 - [TRPO] iter = 106000 dist_mean = 0.2207 dist_std = 1.0055 vf_loss = 0.9069 grad_norm = 0.5747 nat_grad_norm = 0.7648 cg_residual = 0.0078 step_size = 0.3209 reward = 0.0000 fps = 3 mse_loss = 0.6155 
2022-07-08 10:07:43.524198 - gail/main.py:174 - [TRPO] iter = 107000 dist_mean = 0.3347 dist_std = 1.0071 vf_loss = 1.1161 grad_norm = 0.5213 nat_grad_norm = 0.7461 cg_residual = 0.0134 step_size = 0.3221 reward = -0.0000 fps = 3 mse_loss = 0.6931 
2022-07-08 10:08:04.901252 - gail/main.py:174 - [TRPO] iter = 108000 dist_mean = 0.2332 dist_std = 1.0093 vf_loss = 1.0916 grad_norm = 0.4984 nat_grad_norm = 0.8064 cg_residual = 0.0076 step_size = 0.3284 reward = -0.0000 fps = 3 mse_loss = 0.6563 
2022-07-08 10:08:28.162502 - gail/main.py:174 - [TRPO] iter = 109000 dist_mean = 0.2224 dist_std = 1.0111 vf_loss = 0.8924 grad_norm = 0.5397 nat_grad_norm = 0.7846 cg_residual = 0.0076 step_size = 0.3246 reward = -0.0000 fps = 2 mse_loss = 0.6320 
2022-07-08 10:08:50.597632 - gail/main.py:174 - [TRPO] iter = 110000 dist_mean = 0.2262 dist_std = 1.0139 vf_loss = 0.8517 grad_norm = 0.4720 nat_grad_norm = 0.7206 cg_residual = 0.0070 step_size = 0.3615 reward = -0.0000 fps = 2 mse_loss = 0.6134 
2022-07-08 10:08:51.461494 - gail/main.py:201 - [Discriminator] iter = 110000 loss = -6.3359 grad_norm = 18.3177 grad_penalty = 0.9261 regularization = 0.0000 true_logits = 1.4087 fake_logits = -5.8532 true_prob = 0.7626 fake_prob = 0.0054 
2022-07-08 10:13:16.743572 - gail/main.py:142 - [Evaluate] iter = 110000 episode={ returns = 1853.9386 lengths = 1000 } discounted_episode={ returns = 1140.9556 lengths = 1000 } 
2022-07-08 10:13:39.158016 - gail/main.py:174 - [TRPO] iter = 111000 dist_mean = 0.2231 dist_std = 1.0105 vf_loss = 1.3632 grad_norm = 0.5916 nat_grad_norm = 0.7588 cg_residual = 0.0118 step_size = 0.3314 reward = 0.0000 fps = 3 mse_loss = 0.5884 
2022-07-08 10:14:01.874267 - gail/main.py:174 - [TRPO] iter = 112000 dist_mean = 0.1557 dist_std = 1.0102 vf_loss = 0.8727 grad_norm = 0.4740 nat_grad_norm = 0.8185 cg_residual = 0.0096 step_size = 0.3321 reward = 0.0000 fps = 3 mse_loss = 0.5615 
2022-07-08 10:14:25.193342 - gail/main.py:174 - [TRPO] iter = 113000 dist_mean = 0.2206 dist_std = 1.0126 vf_loss = 2.0824 grad_norm = 0.5052 nat_grad_norm = 0.7738 cg_residual = 0.0109 step_size = 0.3429 reward = -0.0000 fps = 2 mse_loss = 0.6545 
2022-07-08 10:14:47.985523 - gail/main.py:174 - [TRPO] iter = 114000 dist_mean = 0.2289 dist_std = 1.0108 vf_loss = 1.4333 grad_norm = 0.6487 nat_grad_norm = 0.7284 cg_residual = 0.0167 step_size = 0.3239 reward = -0.0000 fps = 2 mse_loss = 0.7008 
2022-07-08 10:15:10.958624 - gail/main.py:174 - [TRPO] iter = 115000 dist_mean = 0.2218 dist_std = 1.0045 vf_loss = 1.3457 grad_norm = 0.5612 nat_grad_norm = 0.7732 cg_residual = 0.0160 step_size = 0.3124 reward = 0.0000 fps = 2 mse_loss = 0.7035 
2022-07-08 10:15:11.727405 - gail/main.py:201 - [Discriminator] iter = 115000 loss = -6.2631 grad_norm = 22.5588 grad_penalty = 1.0392 regularization = 0.0000 true_logits = 1.3108 fake_logits = -5.9914 true_prob = 0.7504 fake_prob = 0.0051 
2022-07-08 10:19:26.091163 - gail/main.py:142 - [Evaluate] iter = 115000 episode={ returns = 1489.1431 lengths = 1000 } discounted_episode={ returns = 862.7943 lengths = 1000 } 
2022-07-08 10:19:49.129956 - gail/main.py:174 - [TRPO] iter = 116000 dist_mean = 0.2142 dist_std = 0.9993 vf_loss = 1.3898 grad_norm = 0.5125 nat_grad_norm = 0.7739 cg_residual = 0.0177 step_size = 0.3370 reward = 0.0000 fps = 3 mse_loss = 0.6725 
2022-07-08 10:20:10.690673 - gail/main.py:174 - [TRPO] iter = 117000 dist_mean = 0.1773 dist_std = 0.9974 vf_loss = 1.2056 grad_norm = 0.5729 nat_grad_norm = 0.7189 cg_residual = 0.0137 step_size = 0.3490 reward = -0.0000 fps = 3 mse_loss = 0.6919 
2022-07-08 10:20:33.056598 - gail/main.py:174 - [TRPO] iter = 118000 dist_mean = 0.2134 dist_std = 0.9985 vf_loss = 1.1826 grad_norm = 0.5802 nat_grad_norm = 0.6925 cg_residual = 0.0197 step_size = 0.3526 reward = -0.0000 fps = 3 mse_loss = 0.6610 
2022-07-08 10:20:55.680637 - gail/main.py:174 - [TRPO] iter = 119000 dist_mean = 0.2292 dist_std = 0.9993 vf_loss = 1.2506 grad_norm = 0.6389 nat_grad_norm = 0.7655 cg_residual = 0.0189 step_size = 0.3265 reward = -0.0000 fps = 2 mse_loss = 0.7194 
2022-07-08 10:21:18.471638 - gail/main.py:174 - [TRPO] iter = 120000 dist_mean = 0.2183 dist_std = 0.9974 vf_loss = 0.9009 grad_norm = 0.5245 nat_grad_norm = 0.8399 cg_residual = 0.0163 step_size = 0.3069 reward = 0.0000 fps = 2 mse_loss = 0.7457 
2022-07-08 10:21:19.340119 - gail/main.py:201 - [Discriminator] iter = 120000 loss = -6.5827 grad_norm = 17.3885 grad_penalty = 1.0220 regularization = 0.0000 true_logits = 1.4292 fake_logits = -6.1755 true_prob = 0.7620 fake_prob = 0.0044 
2022-07-08 10:25:37.787186 - gail/main.py:142 - [Evaluate] iter = 120000 episode={ returns = 1223.2119 lengths = 1000 } discounted_episode={ returns = 764.5195 lengths = 1000 } 
2022-07-08 10:25:59.557458 - gail/main.py:174 - [TRPO] iter = 121000 dist_mean = 0.2233 dist_std = 0.9893 vf_loss = 0.8777 grad_norm = 0.5781 nat_grad_norm = 0.7526 cg_residual = 0.0235 step_size = 0.3097 reward = -0.0000 fps = 3 mse_loss = 0.7727 
2022-07-08 10:26:23.745632 - gail/main.py:174 - [TRPO] iter = 122000 dist_mean = 0.2260 dist_std = 0.9895 vf_loss = 1.2671 grad_norm = 0.6515 nat_grad_norm = 0.7007 cg_residual = 0.0242 step_size = 0.3240 reward = 0.0000 fps = 3 mse_loss = 0.7450 
2022-07-08 10:26:47.797213 - gail/main.py:174 - [TRPO] iter = 123000 dist_mean = 0.2182 dist_std = 0.9867 vf_loss = 1.3608 grad_norm = 0.6394 nat_grad_norm = 0.7750 cg_residual = 0.0186 step_size = 0.3216 reward = 0.0000 fps = 3 mse_loss = 0.7659 
2022-07-08 10:27:10.850577 - gail/main.py:174 - [TRPO] iter = 124000 dist_mean = -0.0605 dist_std = 0.9860 vf_loss = 0.4440 grad_norm = 0.4269 nat_grad_norm = 0.7656 cg_residual = 0.0451 step_size = 0.3202 reward = 0.0000 fps = 2 mse_loss = 0.7832 
2022-07-08 10:27:34.163695 - gail/main.py:174 - [TRPO] iter = 125000 dist_mean = 0.2287 dist_std = 0.9847 vf_loss = 1.6006 grad_norm = 0.5978 nat_grad_norm = 0.7918 cg_residual = 0.0198 step_size = 0.3069 reward = -0.0000 fps = 2 mse_loss = 0.7026 
2022-07-08 10:27:35.010073 - gail/main.py:201 - [Discriminator] iter = 125000 loss = -6.5080 grad_norm = 12.5886 grad_penalty = 0.9496 regularization = 0.0000 true_logits = 1.3187 fake_logits = -6.1390 true_prob = 0.7454 fake_prob = 0.0056 
2022-07-08 10:32:07.963968 - gail/main.py:142 - [Evaluate] iter = 125000 episode={ returns = 1801.1472 lengths = 1000 } discounted_episode={ returns = 1069.9111 lengths = 1000 } 
2022-07-08 10:32:14.517402 - gail/main.py:174 - [TRPO] iter = 126000 dist_mean = 0.2460 dist_std = 0.9828 vf_loss = 1.1576 grad_norm = 0.5973 nat_grad_norm = 0.7470 cg_residual = 0.0280 step_size = 0.3176 reward = 0.0000 fps = 3 mse_loss = 0.7053 
2022-07-08 10:32:21.104340 - gail/main.py:174 - [TRPO] iter = 127000 dist_mean = 0.2307 dist_std = 0.9784 vf_loss = 1.8124 grad_norm = 0.7004 nat_grad_norm = 0.7052 cg_residual = 0.0291 step_size = 0.3217 reward = 0.0000 fps = 3 mse_loss = 0.6909 
2022-07-08 10:32:29.140593 - gail/main.py:174 - [TRPO] iter = 128000 dist_mean = 0.2339 dist_std = 0.9729 vf_loss = 1.9283 grad_norm = 0.8244 nat_grad_norm = 0.7472 cg_residual = 0.0308 step_size = 0.2989 reward = 0.0000 fps = 3 mse_loss = 0.7361 
2022-07-08 10:32:36.916061 - gail/main.py:174 - [TRPO] iter = 129000 dist_mean = 0.2169 dist_std = 0.9726 vf_loss = 1.8213 grad_norm = 0.6146 nat_grad_norm = 0.6889 cg_residual = 0.0345 step_size = 0.3320 reward = 0.0000 fps = 3 mse_loss = 0.6979 
2022-07-08 10:32:44.966474 - gail/main.py:174 - [TRPO] iter = 130000 dist_mean = 0.2163 dist_std = 0.9714 vf_loss = 1.6118 grad_norm = 0.5339 nat_grad_norm = 0.6898 cg_residual = 0.0245 step_size = 0.3370 reward = 0.0000 fps = 3 mse_loss = 0.6587 
2022-07-08 10:32:45.261563 - gail/main.py:201 - [Discriminator] iter = 130000 loss = -6.1344 grad_norm = 14.2601 grad_penalty = 0.9475 regularization = 0.0000 true_logits = 1.2059 fake_logits = -5.8760 true_prob = 0.7369 fake_prob = 0.0066 
