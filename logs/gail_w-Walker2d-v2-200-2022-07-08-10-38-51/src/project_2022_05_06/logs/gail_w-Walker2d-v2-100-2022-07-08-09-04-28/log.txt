2022-07-08 09:04:28.584710 - utils/flags.py:257 - log_dir = logs/gail_w-Walker2d-v2-100-2022-07-08-09-04-28
2022-07-08 09:04:43.528512 - gail/utils/replay_buffer.py:86 - Load dataset from /content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/Walker2d-v2
2022-07-08 09:04:59.677614 - gail/main.py:80 - Expert Reward 5150.674112
2022-07-08 09:05:00.456048 - gail/main.py:84 - Original dataset size 3000
2022-07-08 09:05:00.532136 - gail/main.py:86 - Subsampled dataset size 3000
2022-07-08 09:05:00.550486 - gail/main.py:87 - np random: 864 random : 786
2022-07-08 09:05:00.570099 - gail/main.py:91 - Sampled obs: 0.0531, acs: 0.2269
2022-07-08 09:05:02.870078 - lunzi/nn/flat_param.py:17 - Enabling flattening... ['GaussianMLPPolicy_1/log_std:0', 'GaussianMLPPolicy_1/Variable:0', 'GaussianMLPPolicy_1/Variable_1:0', 'GaussianMLPPolicy_1/Variable_2:0', 'GaussianMLPPolicy_1/Variable_3:0', 'GaussianMLPPolicy_1/Variable_4:0', 'GaussianMLPPolicy_1/Variable_5:0']
2022-07-08 09:05:28.674074 - gail/discriminator/discriminator.py:30 - Use predefined normalization.
2022-07-08 09:05:28.810566 - gail/discriminator/discriminator.py:33 - Normalizer loc:[[ 1.2194959e+00  2.4445076e-01 -7.8132987e-02 -2.6673764e-01
   1.8222688e-01 -9.5077172e-02 -3.3649772e-01  5.3370733e-02
   4.1614923e+00  4.1431887e-03  3.8142569e-02 -2.6013174e-03
  -1.0202496e-02  5.6982285e-01  2.9836079e-02 -1.5763690e-01
   1.7689442e-02]] 
 scale:[[0.06687175 0.23681822 0.23042987 0.33821535 0.664349   0.20301929
  0.42807332 0.7138035  0.986894   0.65049744 2.0363257  2.3816926
  3.7250905  6.026913   2.0511289  4.406521   6.1475325 ]]
2022-07-08 09:05:40.427915 - gail/discriminator/discriminator.py:67 - Discriminator uses Wasserstein distance.
2022-07-08 09:05:40.440122 - gail/discriminator/discriminator.py:68 - [<tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable:0' shape=(23, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_1:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_2:0' shape=(100, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_3:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_4:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_5:0' shape=(1,) dtype=float32_ref>]
2022-07-08 09:05:40.441028 - gail/discriminator/discriminator.py:69 - Use gradient penalty regularization (coef = 10.000000)
2022-07-08 09:05:43.012104 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions_mean]
2022-07-08 09:06:26.258194 - gail/main.py:142 - [Evaluate] iter = 0 episode={ returns = 154.4648 lengths = 185 } discounted_episode={ returns = 152.4866 lengths = 170 } 
2022-07-08 09:06:26.262334 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions]
2022-07-08 09:06:51.995408 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [fake_states fake_actions] => [scaled_neural_reward]
2022-07-08 09:06:52.752653 - lunzi/nn/module.py:71 - [MLPVFunction] is making TensorFlow callables, key = [states] => [values]
2022-07-08 09:06:54.105830 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [] => [sync_old]
2022-07-08 09:06:54.719101 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss flat_grad dist_std mean_kl dist_mean]
2022-07-08 09:06:58.760339 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states tangents actions] => [hessian_vec_prod]
2022-07-08 09:07:05.766657 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [] => [get_flat]
2022-07-08 09:07:06.453637 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [feed_flat] => [set_flat]
2022-07-08 09:07:07.163254 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss mean_kl]
2022-07-08 09:07:08.442617 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [train_vf vf_loss]
2022-07-08 09:07:10.408990 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [vf_loss]
2022-07-08 09:07:11.113422 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states actions_] => [mse_loss]
2022-07-08 09:07:11.816551 - gail/main.py:174 - [TRPO] iter = 1000 dist_mean = -0.0000 dist_std = 1.0000 vf_loss = 0.6732 grad_norm = 0.3302 nat_grad_norm = 0.4330 cg_residual = 0.0000 step_size = 0.4424 reward = -0.0000 fps = 11 mse_loss = 0.3947 
2022-07-08 09:07:28.696767 - gail/main.py:174 - [TRPO] iter = 2000 dist_mean = -0.0054 dist_std = 0.9958 vf_loss = 0.6101 grad_norm = 0.4075 nat_grad_norm = 0.4600 cg_residual = 0.0000 step_size = 0.3874 reward = 0.0000 fps = 9 mse_loss = 0.4196 
2022-07-08 09:07:46.461835 - gail/main.py:174 - [TRPO] iter = 3000 dist_mean = -0.0084 dist_std = 0.9990 vf_loss = 0.8399 grad_norm = 0.4533 nat_grad_norm = 0.5118 cg_residual = 0.0000 step_size = 0.3425 reward = -0.0000 fps = 8 mse_loss = 0.4391 
2022-07-08 09:08:04.778898 - gail/main.py:174 - [TRPO] iter = 4000 dist_mean = -0.0088 dist_std = 0.9946 vf_loss = 0.6656 grad_norm = 0.4738 nat_grad_norm = 0.4880 cg_residual = 0.0001 step_size = 0.3526 reward = 0.0000 fps = 7 mse_loss = 0.4331 
2022-07-08 09:08:24.328506 - gail/main.py:174 - [TRPO] iter = 5000 dist_mean = -0.0022 dist_std = 0.9907 vf_loss = 0.7723 grad_norm = 0.5320 nat_grad_norm = 0.4963 cg_residual = 0.0001 step_size = 0.3217 reward = -0.0000 fps = 6 mse_loss = 0.4623 
2022-07-08 09:08:24.330591 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [true_states true_actions fake_states fake_actions true_masks] => [train loss true_logits fake_logits true_prob fake_prob grad_norm grad_penalty regularization]
2022-07-08 09:08:29.780469 - gail/main.py:201 - [Discriminator] iter = 5000 loss = 1.8659 grad_norm = 13.8695 grad_penalty = 1.5255 regularization = 0.0000 true_logits = -0.0193 fake_logits = 0.3212 true_prob = 0.4952 fake_prob = 0.5761 
2022-07-08 09:08:34.700762 - gail/main.py:142 - [Evaluate] iter = 5000 episode={ returns = -7.1408 lengths = 19 } discounted_episode={ returns = -7.0578 lengths = 19 } 
2022-07-08 09:08:52.785426 - gail/main.py:174 - [TRPO] iter = 6000 dist_mean = -0.0121 dist_std = 0.9852 vf_loss = 0.7109 grad_norm = 0.5759 nat_grad_norm = 0.5205 cg_residual = 0.0001 step_size = 0.3021 reward = 0.0000 fps = 43 mse_loss = 0.4968 
2022-07-08 09:09:16.299816 - gail/main.py:174 - [TRPO] iter = 7000 dist_mean = 0.0041 dist_std = 0.9777 vf_loss = 0.3959 grad_norm = 0.5019 nat_grad_norm = 0.4932 cg_residual = 0.0002 step_size = 0.3491 reward = -0.0000 fps = 21 mse_loss = 0.5500 
2022-07-08 09:09:41.980501 - gail/main.py:174 - [TRPO] iter = 8000 dist_mean = 0.0182 dist_std = 0.9747 vf_loss = 0.3301 grad_norm = 0.5295 nat_grad_norm = 0.5267 cg_residual = 0.0003 step_size = 0.3311 reward = 0.0000 fps = 13 mse_loss = 0.5573 
2022-07-08 09:10:05.383452 - gail/main.py:174 - [TRPO] iter = 9000 dist_mean = 0.0292 dist_std = 0.9685 vf_loss = 0.2379 grad_norm = 0.5613 nat_grad_norm = 0.4399 cg_residual = 0.0002 step_size = 0.3557 reward = 0.0000 fps = 10 mse_loss = 0.5984 
2022-07-08 09:10:35.178997 - gail/main.py:174 - [TRPO] iter = 10000 dist_mean = 0.0372 dist_std = 0.9627 vf_loss = 0.2773 grad_norm = 0.5062 nat_grad_norm = 0.4898 cg_residual = 0.0004 step_size = 0.3592 reward = -0.0000 fps = 7 mse_loss = 0.6384 
2022-07-08 09:10:35.955504 - gail/main.py:201 - [Discriminator] iter = 10000 loss = 0.8904 grad_norm = 10.2298 grad_penalty = 0.8741 regularization = 0.0000 true_logits = -0.0085 fake_logits = 0.0078 true_prob = 0.4981 fake_prob = 0.5024 
2022-07-08 09:10:41.579274 - gail/main.py:142 - [Evaluate] iter = 10000 episode={ returns = -11.6015 lengths = 15 } discounted_episode={ returns = -11.5358 lengths = 15 } 
2022-07-08 09:11:16.181710 - gail/main.py:174 - [TRPO] iter = 11000 dist_mean = 0.0402 dist_std = 0.9561 vf_loss = 0.2551 grad_norm = 0.5274 nat_grad_norm = 0.4791 cg_residual = 0.0004 step_size = 0.3638 reward = 0.0000 fps = 24 mse_loss = 0.5759 
2022-07-08 09:11:44.911525 - gail/main.py:174 - [TRPO] iter = 12000 dist_mean = 0.0639 dist_std = 0.9534 vf_loss = 0.2240 grad_norm = 0.6430 nat_grad_norm = 0.5001 cg_residual = 0.0007 step_size = 0.3312 reward = 0.0000 fps = 14 mse_loss = 0.6167 
2022-07-08 09:12:14.330145 - gail/main.py:174 - [TRPO] iter = 13000 dist_mean = 0.0898 dist_std = 0.9486 vf_loss = 0.1727 grad_norm = 0.5995 nat_grad_norm = 0.5065 cg_residual = 0.0007 step_size = 0.3228 reward = -0.0000 fps = 10 mse_loss = 0.6181 
2022-07-08 09:12:47.290410 - gail/main.py:174 - [TRPO] iter = 14000 dist_mean = 0.1051 dist_std = 0.9354 vf_loss = 0.1022 grad_norm = 0.5674 nat_grad_norm = 0.5401 cg_residual = 0.0008 step_size = 0.3413 reward = 0.0000 fps = 7 mse_loss = 0.6735 
2022-07-08 09:13:18.358584 - gail/main.py:174 - [TRPO] iter = 15000 dist_mean = 0.1208 dist_std = 0.9252 vf_loss = 0.1410 grad_norm = 0.6225 nat_grad_norm = 0.5543 cg_residual = 0.0008 step_size = 0.3204 reward = -0.0000 fps = 6 mse_loss = 0.6972 
2022-07-08 09:13:19.139463 - gail/main.py:201 - [Discriminator] iter = 15000 loss = -0.0435 grad_norm = 9.8571 grad_penalty = 0.5803 regularization = 0.0000 true_logits = 0.0030 fake_logits = -0.6208 true_prob = 0.5012 fake_prob = 0.3543 
2022-07-08 09:13:25.755800 - gail/main.py:142 - [Evaluate] iter = 15000 episode={ returns = -11.1722 lengths = 15 } discounted_episode={ returns = -10.9988 lengths = 15 } 
2022-07-08 09:13:56.954556 - gail/main.py:174 - [TRPO] iter = 16000 dist_mean = 0.1453 dist_std = 0.9156 vf_loss = 0.0961 grad_norm = 0.5960 nat_grad_norm = 0.4166 cg_residual = 0.0009 step_size = 0.3738 reward = -0.0000 fps = 26 mse_loss = 0.6643 
2022-07-08 09:14:26.145197 - gail/main.py:174 - [TRPO] iter = 17000 dist_mean = 0.1552 dist_std = 0.9177 vf_loss = 0.1886 grad_norm = 0.6683 nat_grad_norm = 0.4832 cg_residual = 0.0007 step_size = 0.3162 reward = 0.0000 fps = 14 mse_loss = 0.6757 
2022-07-08 09:14:58.762349 - gail/main.py:174 - [TRPO] iter = 18000 dist_mean = 0.1747 dist_std = 0.9134 vf_loss = 0.1913 grad_norm = 0.7476 nat_grad_norm = 0.5109 cg_residual = 0.0010 step_size = 0.2720 reward = -0.0000 fps = 10 mse_loss = 0.6239 
2022-07-08 09:15:29.327234 - gail/main.py:174 - [TRPO] iter = 19000 dist_mean = 0.2023 dist_std = 0.9109 vf_loss = 0.2039 grad_norm = 0.7696 nat_grad_norm = 0.4972 cg_residual = 0.0008 step_size = 0.2813 reward = -0.0000 fps = 7 mse_loss = 0.5965 
2022-07-08 09:15:59.984579 - gail/main.py:174 - [TRPO] iter = 20000 dist_mean = 0.2250 dist_std = 0.9122 vf_loss = 0.2915 grad_norm = 0.7985 nat_grad_norm = 0.4829 cg_residual = 0.0005 step_size = 0.2812 reward = 0.0000 fps = 6 mse_loss = 0.5884 
2022-07-08 09:16:00.878327 - gail/main.py:201 - [Discriminator] iter = 20000 loss = -0.8522 grad_norm = 8.3713 grad_penalty = 0.4744 regularization = 0.0000 true_logits = 0.0393 fake_logits = -1.2872 true_prob = 0.5105 fake_prob = 0.2318 
2022-07-08 09:16:10.634653 - gail/main.py:142 - [Evaluate] iter = 20000 episode={ returns = -2.0548 lengths = 24 } discounted_episode={ returns = -2.0434 lengths = 24 } 
2022-07-08 09:16:39.919349 - gail/main.py:174 - [TRPO] iter = 21000 dist_mean = 0.2396 dist_std = 0.9070 vf_loss = 0.3936 grad_norm = 0.6827 nat_grad_norm = 0.5532 cg_residual = 0.0011 step_size = 0.3052 reward = 0.0000 fps = 25 mse_loss = 0.5812 
2022-07-08 09:17:10.735188 - gail/main.py:174 - [TRPO] iter = 22000 dist_mean = 0.2644 dist_std = 0.9060 vf_loss = 0.8316 grad_norm = 0.7997 nat_grad_norm = 0.5036 cg_residual = 0.0006 step_size = 0.2974 reward = 0.0000 fps = 14 mse_loss = 0.5863 
2022-07-08 09:17:40.763158 - gail/main.py:174 - [TRPO] iter = 23000 dist_mean = 0.2853 dist_std = 0.9025 vf_loss = 0.8804 grad_norm = 0.6642 nat_grad_norm = 0.4925 cg_residual = 0.0006 step_size = 0.3395 reward = -0.0000 fps = 10 mse_loss = 0.6140 
2022-07-08 09:18:10.103053 - gail/main.py:174 - [TRPO] iter = 24000 dist_mean = 0.3323 dist_std = 0.8962 vf_loss = 1.1027 grad_norm = 0.5262 nat_grad_norm = 0.5396 cg_residual = 0.0015 step_size = 0.3989 reward = -0.0000 fps = 7 mse_loss = 0.6200 
2022-07-08 09:18:39.237796 - gail/main.py:174 - [TRPO] iter = 25000 dist_mean = 0.3372 dist_std = 0.9002 vf_loss = 1.2830 grad_norm = 0.5969 nat_grad_norm = 0.4806 cg_residual = 0.0016 step_size = 0.4140 reward = 0.0000 fps = 6 mse_loss = 0.6009 
2022-07-08 09:18:40.084921 - gail/main.py:201 - [Discriminator] iter = 25000 loss = -0.9189 grad_norm = 6.9237 grad_penalty = 0.4582 regularization = 0.0000 true_logits = -0.0008 fake_logits = -1.3779 true_prob = 0.5037 fake_prob = 0.2251 
2022-07-08 09:20:41.665477 - gail/main.py:142 - [Evaluate] iter = 25000 episode={ returns = 356.3630 lengths = 278 } discounted_episode={ returns = 309.0623 lengths = 312 } 
2022-07-08 09:21:10.148207 - gail/main.py:174 - [TRPO] iter = 26000 dist_mean = 0.3904 dist_std = 0.9010 vf_loss = 1.2270 grad_norm = 0.6293 nat_grad_norm = 0.5430 cg_residual = 0.0011 step_size = 0.3875 reward = -0.0000 fps = 6 mse_loss = 0.5930 
2022-07-08 09:21:38.158855 - gail/main.py:174 - [TRPO] iter = 27000 dist_mean = 0.4234 dist_std = 0.8976 vf_loss = 2.2162 grad_norm = 0.6075 nat_grad_norm = 0.5051 cg_residual = 0.0014 step_size = 0.4172 reward = 0.0000 fps = 5 mse_loss = 0.5861 
2022-07-08 09:22:13.132666 - gail/main.py:174 - [TRPO] iter = 28000 dist_mean = 0.4455 dist_std = 0.8995 vf_loss = 1.2191 grad_norm = 0.4596 nat_grad_norm = 0.4507 cg_residual = 0.0036 step_size = 0.4594 reward = 0.0000 fps = 4 mse_loss = 0.5880 
2022-07-08 09:22:42.024690 - gail/main.py:174 - [TRPO] iter = 29000 dist_mean = 0.4327 dist_std = 0.8974 vf_loss = 1.4023 grad_norm = 0.5944 nat_grad_norm = 0.4829 cg_residual = 0.0019 step_size = 0.4078 reward = -0.0000 fps = 4 mse_loss = 0.6391 
2022-07-08 09:23:10.956220 - gail/main.py:174 - [TRPO] iter = 30000 dist_mean = 0.5061 dist_std = 0.8915 vf_loss = 1.5225 grad_norm = 0.6181 nat_grad_norm = 0.4893 cg_residual = 0.0011 step_size = 0.4050 reward = -0.0000 fps = 3 mse_loss = 0.6539 
2022-07-08 09:23:11.859631 - gail/main.py:201 - [Discriminator] iter = 30000 loss = -0.7311 grad_norm = 5.6234 grad_penalty = 0.4035 regularization = 0.0000 true_logits = 0.0113 fake_logits = -1.1233 true_prob = 0.5076 fake_prob = 0.2685 
2022-07-08 09:24:39.404063 - gail/main.py:142 - [Evaluate] iter = 30000 episode={ returns = 345.6545 lengths = 221 } discounted_episode={ returns = 299.4411 lengths = 216 } 
2022-07-08 09:25:06.993690 - gail/main.py:174 - [TRPO] iter = 31000 dist_mean = 0.4956 dist_std = 0.8945 vf_loss = 1.4681 grad_norm = 0.4431 nat_grad_norm = 0.5517 cg_residual = 0.0048 step_size = 0.3886 reward = 0.0000 fps = 8 mse_loss = 0.6564 
2022-07-08 09:25:34.938448 - gail/main.py:174 - [TRPO] iter = 32000 dist_mean = 0.5008 dist_std = 0.8920 vf_loss = 0.7091 grad_norm = 0.4674 nat_grad_norm = 0.5449 cg_residual = 0.0050 step_size = 0.3870 reward = 0.0000 fps = 6 mse_loss = 0.6231 
2022-07-08 09:26:04.477730 - gail/main.py:174 - [TRPO] iter = 33000 dist_mean = 0.4631 dist_std = 0.8877 vf_loss = 1.3014 grad_norm = 0.3681 nat_grad_norm = 0.4218 cg_residual = 0.0029 step_size = 0.5123 reward = 0.0000 fps = 5 mse_loss = 0.6776 
2022-07-08 09:26:33.975748 - gail/main.py:174 - [TRPO] iter = 34000 dist_mean = 0.5168 dist_std = 0.8865 vf_loss = 1.1920 grad_norm = 0.4168 nat_grad_norm = 0.4866 cg_residual = 0.0017 step_size = 0.4344 reward = -0.0000 fps = 4 mse_loss = 0.6361 
2022-07-08 09:27:02.227670 - gail/main.py:174 - [TRPO] iter = 35000 dist_mean = 0.5479 dist_std = 0.8825 vf_loss = 1.3578 grad_norm = 0.5746 nat_grad_norm = 0.3938 cg_residual = 0.0015 step_size = 0.4698 reward = 0.0000 fps = 4 mse_loss = 0.6673 
2022-07-08 09:27:03.030800 - gail/main.py:201 - [Discriminator] iter = 35000 loss = -1.1371 grad_norm = 5.3012 grad_penalty = 0.3370 regularization = 0.0000 true_logits = 0.0255 fake_logits = -1.4485 true_prob = 0.5119 fake_prob = 0.2057 
2022-07-08 09:28:35.547660 - gail/main.py:142 - [Evaluate] iter = 35000 episode={ returns = 197.5802 lengths = 256 } discounted_episode={ returns = 153.7397 lengths = 227 } 
2022-07-08 09:29:03.731394 - gail/main.py:174 - [TRPO] iter = 36000 dist_mean = 0.5523 dist_std = 0.8809 vf_loss = 1.1925 grad_norm = 0.3310 nat_grad_norm = 0.4971 cg_residual = 0.0024 step_size = 0.4593 reward = 0.0000 fps = 8 mse_loss = 0.6698 
2022-07-08 09:29:32.528169 - gail/main.py:174 - [TRPO] iter = 37000 dist_mean = 0.5407 dist_std = 0.8797 vf_loss = 0.9599 grad_norm = 0.3667 nat_grad_norm = 0.5241 cg_residual = 0.0051 step_size = 0.4056 reward = -0.0000 fps = 6 mse_loss = 0.7549 
2022-07-08 09:29:59.944902 - gail/main.py:174 - [TRPO] iter = 38000 dist_mean = 0.5243 dist_std = 0.8764 vf_loss = 1.1924 grad_norm = 0.3451 nat_grad_norm = 0.4073 cg_residual = 0.0010 step_size = 0.5005 reward = -0.0000 fps = 5 mse_loss = 0.7114 
2022-07-08 09:30:27.769380 - gail/main.py:174 - [TRPO] iter = 39000 dist_mean = 0.5360 dist_std = 0.8758 vf_loss = 0.5598 grad_norm = 0.4667 nat_grad_norm = 0.5638 cg_residual = 0.0031 step_size = 0.3874 reward = 0.0000 fps = 4 mse_loss = 0.7654 
2022-07-08 09:30:54.962356 - gail/main.py:174 - [TRPO] iter = 40000 dist_mean = 0.5408 dist_std = 0.8725 vf_loss = 0.5603 grad_norm = 0.4932 nat_grad_norm = 0.5147 cg_residual = 0.0056 step_size = 0.4248 reward = 0.0000 fps = 4 mse_loss = 0.7359 
2022-07-08 09:30:55.818283 - gail/main.py:201 - [Discriminator] iter = 40000 loss = -1.4575 grad_norm = 5.5958 grad_penalty = 0.3332 regularization = 0.0000 true_logits = 0.0472 fake_logits = -1.7434 true_prob = 0.5156 fake_prob = 0.1658 
2022-07-08 09:32:49.030031 - gail/main.py:142 - [Evaluate] iter = 40000 episode={ returns = 376.5583 lengths = 263 } discounted_episode={ returns = 327.4839 lengths = 298 } 
2022-07-08 09:33:16.198303 - gail/main.py:174 - [TRPO] iter = 41000 dist_mean = 0.5441 dist_std = 0.8696 vf_loss = 0.4442 grad_norm = 0.5550 nat_grad_norm = 0.5015 cg_residual = 0.0024 step_size = 0.4177 reward = -0.0000 fps = 7 mse_loss = 0.7796 
2022-07-08 09:33:43.594715 - gail/main.py:174 - [TRPO] iter = 42000 dist_mean = 0.5322 dist_std = 0.8742 vf_loss = 0.7694 grad_norm = 0.3754 nat_grad_norm = 0.4664 cg_residual = 0.0037 step_size = 0.4565 reward = -0.0000 fps = 5 mse_loss = 0.7565 
2022-07-08 09:34:10.254785 - gail/main.py:174 - [TRPO] iter = 43000 dist_mean = 0.5533 dist_std = 0.8749 vf_loss = 0.9244 grad_norm = 0.4676 nat_grad_norm = 0.4584 cg_residual = 0.0031 step_size = 0.4171 reward = -0.0000 fps = 5 mse_loss = 0.7778 
2022-07-08 09:34:37.733483 - gail/main.py:174 - [TRPO] iter = 44000 dist_mean = 0.5395 dist_std = 0.8744 vf_loss = 0.5331 grad_norm = 0.3696 nat_grad_norm = 0.4801 cg_residual = 0.0045 step_size = 0.4248 reward = 0.0000 fps = 4 mse_loss = 0.7346 
2022-07-08 09:35:04.714498 - gail/main.py:174 - [TRPO] iter = 45000 dist_mean = 0.5494 dist_std = 0.8778 vf_loss = 0.5165 grad_norm = 0.4287 nat_grad_norm = 0.5094 cg_residual = 0.0050 step_size = 0.3780 reward = -0.0000 fps = 4 mse_loss = 0.7646 
2022-07-08 09:35:05.461627 - gail/main.py:201 - [Discriminator] iter = 45000 loss = -2.1551 grad_norm = 6.0369 grad_penalty = 0.3110 regularization = 0.0000 true_logits = 0.0646 fake_logits = -2.4015 true_prob = 0.5271 fake_prob = 0.0994 
2022-07-08 09:36:39.702112 - gail/main.py:142 - [Evaluate] iter = 45000 episode={ returns = 184.5824 lengths = 243 } discounted_episode={ returns = 215.0765 lengths = 251 } 
2022-07-08 09:37:07.006733 - gail/main.py:174 - [TRPO] iter = 46000 dist_mean = 0.5543 dist_std = 0.8788 vf_loss = 0.8865 grad_norm = 0.4372 nat_grad_norm = 0.4532 cg_residual = 0.0040 step_size = 0.4335 reward = 0.0000 fps = 8 mse_loss = 0.8600 
2022-07-08 09:37:34.151286 - gail/main.py:174 - [TRPO] iter = 47000 dist_mean = 0.5588 dist_std = 0.8773 vf_loss = 1.0516 grad_norm = 0.4372 nat_grad_norm = 0.4757 cg_residual = 0.0046 step_size = 0.4414 reward = 0.0000 fps = 6 mse_loss = 0.7574 
2022-07-08 09:38:00.624437 - gail/main.py:174 - [TRPO] iter = 48000 dist_mean = 0.5352 dist_std = 0.8738 vf_loss = 1.0619 grad_norm = 0.3008 nat_grad_norm = 0.4572 cg_residual = 0.0020 step_size = 0.4727 reward = -0.0000 fps = 5 mse_loss = 0.7273 
2022-07-08 09:38:27.395097 - gail/main.py:174 - [TRPO] iter = 49000 dist_mean = 0.5291 dist_std = 0.8761 vf_loss = 1.4219 grad_norm = 0.3256 nat_grad_norm = 0.5154 cg_residual = 0.0019 step_size = 0.4516 reward = -0.0000 fps = 4 mse_loss = 0.8115 
2022-07-08 09:38:54.670465 - gail/main.py:174 - [TRPO] iter = 50000 dist_mean = 0.5277 dist_std = 0.8734 vf_loss = 0.8625 grad_norm = 0.3659 nat_grad_norm = 0.5064 cg_residual = 0.0023 step_size = 0.4731 reward = 0.0000 fps = 4 mse_loss = 0.7569 
2022-07-08 09:38:55.475540 - gail/main.py:201 - [Discriminator] iter = 50000 loss = -2.7009 grad_norm = 5.7393 grad_penalty = 0.3253 regularization = 0.0000 true_logits = 0.0924 fake_logits = -2.9339 true_prob = 0.5361 fake_prob = 0.0660 
2022-07-08 09:40:12.346084 - gail/main.py:142 - [Evaluate] iter = 50000 episode={ returns = 354.1514 lengths = 202 } discounted_episode={ returns = 319.9417 lengths = 212 } 
2022-07-08 09:40:39.575140 - gail/main.py:174 - [TRPO] iter = 51000 dist_mean = 0.4977 dist_std = 0.8745 vf_loss = 1.0924 grad_norm = 0.3565 nat_grad_norm = 0.5134 cg_residual = 0.0019 step_size = 0.4348 reward = -0.0000 fps = 9 mse_loss = 0.8384 
2022-07-08 09:41:05.601906 - gail/main.py:174 - [TRPO] iter = 52000 dist_mean = 0.4706 dist_std = 0.8727 vf_loss = 0.4795 grad_norm = 0.4167 nat_grad_norm = 0.6957 cg_residual = 0.0037 step_size = 0.3558 reward = -0.0000 fps = 7 mse_loss = 0.7694 
2022-07-08 09:41:32.102059 - gail/main.py:174 - [TRPO] iter = 53000 dist_mean = 0.4937 dist_std = 0.8795 vf_loss = 1.1385 grad_norm = 0.4277 nat_grad_norm = 0.5688 cg_residual = 0.0016 step_size = 0.4026 reward = 0.0000 fps = 6 mse_loss = 0.8234 
2022-07-08 09:41:58.165063 - gail/main.py:174 - [TRPO] iter = 54000 dist_mean = 0.5019 dist_std = 0.8752 vf_loss = 0.8163 grad_norm = 0.4152 nat_grad_norm = 0.5503 cg_residual = 0.0023 step_size = 0.3944 reward = 0.0000 fps = 5 mse_loss = 0.8839 
2022-07-08 09:42:30.975289 - gail/main.py:174 - [TRPO] iter = 55000 dist_mean = 0.4770 dist_std = 0.8719 vf_loss = 0.5707 grad_norm = 0.4645 nat_grad_norm = 0.6086 cg_residual = 0.0048 step_size = 0.3459 reward = -0.0000 fps = 4 mse_loss = 0.8885 
2022-07-08 09:42:31.706333 - gail/main.py:201 - [Discriminator] iter = 55000 loss = -3.0481 grad_norm = 5.7770 grad_penalty = 0.4309 regularization = 0.0000 true_logits = 0.1140 fake_logits = -3.3650 true_prob = 0.5391 fake_prob = 0.0506 
2022-07-08 09:43:26.915866 - gail/main.py:142 - [Evaluate] iter = 55000 episode={ returns = 327.6200 lengths = 156 } discounted_episode={ returns = 305.8889 lengths = 163 } 
2022-07-08 09:43:52.814196 - gail/main.py:174 - [TRPO] iter = 56000 dist_mean = 0.5051 dist_std = 0.8705 vf_loss = 0.5667 grad_norm = 0.3618 nat_grad_norm = 0.4875 cg_residual = 0.0026 step_size = 0.4625 reward = -0.0000 fps = 12 mse_loss = 0.9290 
2022-07-08 09:44:18.332651 - gail/main.py:174 - [TRPO] iter = 57000 dist_mean = 0.4943 dist_std = 0.8735 vf_loss = 0.8267 grad_norm = 0.5136 nat_grad_norm = 0.5452 cg_residual = 0.0039 step_size = 0.3602 reward = -0.0000 fps = 9 mse_loss = 0.8969 
2022-07-08 09:44:44.213137 - gail/main.py:174 - [TRPO] iter = 58000 dist_mean = 0.4931 dist_std = 0.8744 vf_loss = 0.9858 grad_norm = 0.4953 nat_grad_norm = 0.5514 cg_residual = 0.0023 step_size = 0.3860 reward = 0.0000 fps = 7 mse_loss = 0.9033 
2022-07-08 09:45:10.084562 - gail/main.py:174 - [TRPO] iter = 59000 dist_mean = 0.4513 dist_std = 0.8729 vf_loss = 1.0255 grad_norm = 0.4754 nat_grad_norm = 0.6582 cg_residual = 0.0038 step_size = 0.3607 reward = 0.0000 fps = 6 mse_loss = 1.0043 
2022-07-08 09:45:35.532436 - gail/main.py:174 - [TRPO] iter = 60000 dist_mean = 0.4769 dist_std = 0.8684 vf_loss = 0.5689 grad_norm = 0.5661 nat_grad_norm = 0.7743 cg_residual = 0.0078 step_size = 0.3256 reward = 0.0000 fps = 5 mse_loss = 0.9001 
2022-07-08 09:45:36.321862 - gail/main.py:201 - [Discriminator] iter = 60000 loss = -3.3062 grad_norm = 6.2913 grad_penalty = 0.4275 regularization = 0.0000 true_logits = 0.1760 fake_logits = -3.5577 true_prob = 0.5531 fake_prob = 0.0434 
2022-07-08 09:46:26.561622 - gail/main.py:142 - [Evaluate] iter = 60000 episode={ returns = 322.4545 lengths = 148 } discounted_episode={ returns = 288.8502 lengths = 144 } 
2022-07-08 09:46:52.131512 - gail/main.py:174 - [TRPO] iter = 61000 dist_mean = 0.4839 dist_std = 0.8712 vf_loss = 0.7346 grad_norm = 0.4935 nat_grad_norm = 0.5692 cg_residual = 0.0029 step_size = 0.3792 reward = -0.0000 fps = 13 mse_loss = 0.9061 
2022-07-08 09:47:17.604626 - gail/main.py:174 - [TRPO] iter = 62000 dist_mean = 0.4693 dist_std = 0.8667 vf_loss = 0.8488 grad_norm = 0.4512 nat_grad_norm = 0.4867 cg_residual = 0.0024 step_size = 0.4168 reward = 0.0000 fps = 9 mse_loss = 0.9081 
2022-07-08 09:47:43.426384 - gail/main.py:174 - [TRPO] iter = 63000 dist_mean = 0.4280 dist_std = 0.8690 vf_loss = 0.6738 grad_norm = 0.3356 nat_grad_norm = 0.5515 cg_residual = 0.0022 step_size = 0.4465 reward = 0.0000 fps = 7 mse_loss = 0.9893 
2022-07-08 09:48:09.816999 - gail/main.py:174 - [TRPO] iter = 64000 dist_mean = 0.4558 dist_std = 0.8679 vf_loss = 0.4511 grad_norm = 0.4813 nat_grad_norm = 0.6695 cg_residual = 0.0058 step_size = 0.3386 reward = 0.0000 fps = 6 mse_loss = 1.0405 
2022-07-08 09:48:34.935039 - gail/main.py:174 - [TRPO] iter = 65000 dist_mean = 0.4557 dist_std = 0.8594 vf_loss = 0.6763 grad_norm = 0.4467 nat_grad_norm = 0.5838 cg_residual = 0.0043 step_size = 0.3904 reward = -0.0000 fps = 5 mse_loss = 0.8817 
2022-07-08 09:48:35.662045 - gail/main.py:201 - [Discriminator] iter = 65000 loss = -3.5022 grad_norm = 5.1984 grad_penalty = 0.3992 regularization = 0.0000 true_logits = 0.0851 fake_logits = -3.8163 true_prob = 0.5393 fake_prob = 0.0381 
2022-07-08 09:49:25.056255 - gail/main.py:142 - [Evaluate] iter = 65000 episode={ returns = 308.1736 lengths = 144 } discounted_episode={ returns = 283.0464 lengths = 144 } 
2022-07-08 09:49:54.417443 - gail/main.py:174 - [TRPO] iter = 66000 dist_mean = 0.4535 dist_std = 0.8562 vf_loss = 0.8283 grad_norm = 0.5492 nat_grad_norm = 0.7310 cg_residual = 0.0037 step_size = 0.3361 reward = 0.0000 fps = 12 mse_loss = 0.8423 
2022-07-08 09:50:22.655428 - gail/main.py:174 - [TRPO] iter = 67000 dist_mean = 0.4541 dist_std = 0.8560 vf_loss = 1.0429 grad_norm = 0.4522 nat_grad_norm = 0.6028 cg_residual = 0.0024 step_size = 0.3852 reward = -0.0000 fps = 9 mse_loss = 0.8897 
2022-07-08 09:50:48.316369 - gail/main.py:174 - [TRPO] iter = 68000 dist_mean = 0.4498 dist_std = 0.8564 vf_loss = 1.1628 grad_norm = 0.4221 nat_grad_norm = 0.6344 cg_residual = 0.0031 step_size = 0.3906 reward = -0.0000 fps = 7 mse_loss = 0.8583 
2022-07-08 09:51:13.635738 - gail/main.py:174 - [TRPO] iter = 69000 dist_mean = 0.4092 dist_std = 0.8567 vf_loss = 0.5591 grad_norm = 0.5262 nat_grad_norm = 0.6801 cg_residual = 0.0033 step_size = 0.3582 reward = 0.0000 fps = 6 mse_loss = 0.8277 
2022-07-08 09:51:39.384400 - gail/main.py:174 - [TRPO] iter = 70000 dist_mean = 0.4281 dist_std = 0.8570 vf_loss = 0.8077 grad_norm = 0.4585 nat_grad_norm = 0.6082 cg_residual = 0.0045 step_size = 0.3888 reward = -0.0000 fps = 5 mse_loss = 0.7592 
2022-07-08 09:51:40.102964 - gail/main.py:201 - [Discriminator] iter = 70000 loss = -3.6418 grad_norm = 4.9246 grad_penalty = 0.4266 regularization = 0.0000 true_logits = 0.0416 fake_logits = -4.0269 true_prob = 0.5335 fake_prob = 0.0300 
2022-07-08 09:52:38.825054 - gail/main.py:142 - [Evaluate] iter = 70000 episode={ returns = 319.4675 lengths = 148 } discounted_episode={ returns = 293.4766 lengths = 148 } 
2022-07-08 09:53:05.338709 - gail/main.py:174 - [TRPO] iter = 71000 dist_mean = 0.3980 dist_std = 0.8544 vf_loss = 0.6524 grad_norm = 0.5850 nat_grad_norm = 0.7579 cg_residual = 0.0057 step_size = 0.3468 reward = -0.0000 fps = 11 mse_loss = 0.8429 
2022-07-08 09:53:32.537661 - gail/main.py:174 - [TRPO] iter = 72000 dist_mean = 0.4065 dist_std = 0.8533 vf_loss = 0.5975 grad_norm = 0.5125 nat_grad_norm = 0.7410 cg_residual = 0.0041 step_size = 0.3303 reward = -0.0000 fps = 8 mse_loss = 0.8035 
2022-07-08 09:53:59.880981 - gail/main.py:174 - [TRPO] iter = 73000 dist_mean = 0.3765 dist_std = 0.8555 vf_loss = 0.7057 grad_norm = 0.5045 nat_grad_norm = 0.6654 cg_residual = 0.0050 step_size = 0.3428 reward = -0.0000 fps = 7 mse_loss = 0.9594 
2022-07-08 09:54:26.632815 - gail/main.py:174 - [TRPO] iter = 74000 dist_mean = 0.3615 dist_std = 0.8518 vf_loss = 0.3156 grad_norm = 0.5927 nat_grad_norm = 0.6723 cg_residual = 0.0098 step_size = 0.3365 reward = -0.0000 fps = 6 mse_loss = 0.8652 
2022-07-08 09:54:53.327810 - gail/main.py:174 - [TRPO] iter = 75000 dist_mean = 0.3907 dist_std = 0.8526 vf_loss = 0.3904 grad_norm = 0.5053 nat_grad_norm = 0.5565 cg_residual = 0.0041 step_size = 0.4016 reward = -0.0000 fps = 5 mse_loss = 0.9356 
2022-07-08 09:54:54.186289 - gail/main.py:201 - [Discriminator] iter = 75000 loss = -3.4951 grad_norm = 4.6899 grad_penalty = 0.3954 regularization = 0.0000 true_logits = -0.0075 fake_logits = -3.8979 true_prob = 0.5218 fake_prob = 0.0320 
2022-07-08 09:55:44.850641 - gail/main.py:142 - [Evaluate] iter = 75000 episode={ returns = 325.1748 lengths = 153 } discounted_episode={ returns = 297.2537 lengths = 152 } 
2022-07-08 09:56:09.553721 - gail/main.py:174 - [TRPO] iter = 76000 dist_mean = 0.4051 dist_std = 0.8508 vf_loss = 0.4432 grad_norm = 0.4406 nat_grad_norm = 0.4660 cg_residual = 0.0028 step_size = 0.4443 reward = -0.0000 fps = 13 mse_loss = 0.9915 
2022-07-08 09:56:34.362323 - gail/main.py:174 - [TRPO] iter = 77000 dist_mean = 0.3826 dist_std = 0.8545 vf_loss = 0.4325 grad_norm = 0.5145 nat_grad_norm = 0.7634 cg_residual = 0.0064 step_size = 0.3284 reward = 0.0000 fps = 9 mse_loss = 0.8590 
2022-07-08 09:56:59.127037 - gail/main.py:174 - [TRPO] iter = 78000 dist_mean = 0.3717 dist_std = 0.8503 vf_loss = 0.4382 grad_norm = 0.5066 nat_grad_norm = 0.6218 cg_residual = 0.0035 step_size = 0.3892 reward = -0.0000 fps = 8 mse_loss = 0.9791 
2022-07-08 09:57:23.614623 - gail/main.py:174 - [TRPO] iter = 79000 dist_mean = 0.3786 dist_std = 0.8531 vf_loss = 0.6530 grad_norm = 0.4654 nat_grad_norm = 0.6224 cg_residual = 0.0048 step_size = 0.3785 reward = 0.0000 fps = 6 mse_loss = 0.9663 
2022-07-08 09:57:47.514412 - gail/main.py:174 - [TRPO] iter = 80000 dist_mean = 0.3822 dist_std = 0.8471 vf_loss = 0.4486 grad_norm = 0.6557 nat_grad_norm = 0.8528 cg_residual = 0.0121 step_size = 0.2907 reward = -0.0000 fps = 5 mse_loss = 1.0602 
2022-07-08 09:57:48.257765 - gail/main.py:201 - [Discriminator] iter = 80000 loss = -3.9395 grad_norm = 5.3941 grad_penalty = 0.3639 regularization = 0.0000 true_logits = -0.0706 fake_logits = -4.3739 true_prob = 0.5083 fake_prob = 0.0247 
2022-07-08 09:58:34.437065 - gail/main.py:142 - [Evaluate] iter = 80000 episode={ returns = 288.9293 lengths = 136 } discounted_episode={ returns = 265.7132 lengths = 134 } 
2022-07-08 09:59:00.439164 - gail/main.py:174 - [TRPO] iter = 81000 dist_mean = 0.4040 dist_std = 0.8444 vf_loss = 0.5422 grad_norm = 0.5127 nat_grad_norm = 0.7836 cg_residual = 0.0057 step_size = 0.3294 reward = 0.0000 fps = 13 mse_loss = 1.0387 
2022-07-08 09:59:26.062913 - gail/main.py:174 - [TRPO] iter = 82000 dist_mean = 0.3984 dist_std = 0.8414 vf_loss = 0.6360 grad_norm = 0.4431 nat_grad_norm = 0.6006 cg_residual = 0.0041 step_size = 0.4255 reward = -0.0000 fps = 10 mse_loss = 0.9577 
2022-07-08 09:59:53.845675 - gail/main.py:174 - [TRPO] iter = 83000 dist_mean = 0.3915 dist_std = 0.8398 vf_loss = 0.5421 grad_norm = 0.5083 nat_grad_norm = 0.6687 cg_residual = 0.0057 step_size = 0.3369 reward = -0.0000 fps = 7 mse_loss = 1.0125 
2022-07-08 10:00:20.865650 - gail/main.py:174 - [TRPO] iter = 84000 dist_mean = 0.3741 dist_std = 0.8422 vf_loss = 0.5194 grad_norm = 0.5439 nat_grad_norm = 0.5787 cg_residual = 0.0058 step_size = 0.3651 reward = -0.0000 fps = 6 mse_loss = 1.1409 
2022-07-08 10:00:47.698919 - gail/main.py:174 - [TRPO] iter = 85000 dist_mean = 0.3759 dist_std = 0.8390 vf_loss = 0.3797 grad_norm = 0.5963 nat_grad_norm = 0.6819 cg_residual = 0.0061 step_size = 0.3402 reward = 0.0000 fps = 5 mse_loss = 1.1723 
2022-07-08 10:00:48.502066 - gail/main.py:201 - [Discriminator] iter = 85000 loss = -3.6300 grad_norm = 4.7184 grad_penalty = 0.4326 regularization = 0.0000 true_logits = -0.2193 fake_logits = -4.2820 true_prob = 0.4790 fake_prob = 0.0240 
2022-07-08 10:01:35.609070 - gail/main.py:142 - [Evaluate] iter = 85000 episode={ returns = 282.1510 lengths = 129 } discounted_episode={ returns = 259.1484 lengths = 127 } 
2022-07-08 10:02:11.901202 - gail/main.py:174 - [TRPO] iter = 86000 dist_mean = 0.3949 dist_std = 0.8340 vf_loss = 0.2802 grad_norm = 0.5561 nat_grad_norm = 0.5640 cg_residual = 0.0062 step_size = 0.3728 reward = -0.0000 fps = 11 mse_loss = 1.0623 
2022-07-08 10:02:41.297426 - gail/main.py:174 - [TRPO] iter = 87000 dist_mean = 0.3343 dist_std = 0.8307 vf_loss = 0.3116 grad_norm = 0.5571 nat_grad_norm = 0.6498 cg_residual = 0.0082 step_size = 0.3691 reward = -0.0000 fps = 8 mse_loss = 1.0627 
2022-07-08 10:03:11.215303 - gail/main.py:174 - [TRPO] iter = 88000 dist_mean = 0.3663 dist_std = 0.8253 vf_loss = 0.2937 grad_norm = 0.5507 nat_grad_norm = 0.6803 cg_residual = 0.0146 step_size = 0.3512 reward = 0.0000 fps = 7 mse_loss = 1.0556 
2022-07-08 10:03:40.734647 - gail/main.py:174 - [TRPO] iter = 89000 dist_mean = 0.4074 dist_std = 0.8207 vf_loss = 0.4623 grad_norm = 0.4382 nat_grad_norm = 0.5160 cg_residual = 0.0094 step_size = 0.4316 reward = -0.0000 fps = 5 mse_loss = 1.0936 
2022-07-08 10:04:11.103819 - gail/main.py:174 - [TRPO] iter = 90000 dist_mean = 0.3710 dist_std = 0.8195 vf_loss = 0.2880 grad_norm = 0.5116 nat_grad_norm = 0.5607 cg_residual = 0.0071 step_size = 0.3806 reward = 0.0000 fps = 4 mse_loss = 1.1052 
2022-07-08 10:04:11.946449 - gail/main.py:201 - [Discriminator] iter = 90000 loss = -3.5486 grad_norm = 4.4431 grad_penalty = 0.3867 regularization = 0.0000 true_logits = -0.2341 fake_logits = -4.1694 true_prob = 0.4824 fake_prob = 0.0239 
2022-07-08 10:05:00.746852 - gail/main.py:142 - [Evaluate] iter = 90000 episode={ returns = 263.9015 lengths = 121 } discounted_episode={ returns = 249.6447 lengths = 121 } 
2022-07-08 10:05:30.149964 - gail/main.py:174 - [TRPO] iter = 91000 dist_mean = 0.4155 dist_std = 0.8172 vf_loss = 0.3260 grad_norm = 0.5654 nat_grad_norm = 0.6222 cg_residual = 0.0116 step_size = 0.3415 reward = 0.0000 fps = 12 mse_loss = 1.0892 
2022-07-08 10:06:00.112371 - gail/main.py:174 - [TRPO] iter = 92000 dist_mean = 0.3644 dist_std = 0.8120 vf_loss = 0.2981 grad_norm = 0.5024 nat_grad_norm = 0.6385 cg_residual = 0.0082 step_size = 0.3474 reward = 0.0000 fps = 9 mse_loss = 0.9732 
2022-07-08 10:06:30.168737 - gail/main.py:174 - [TRPO] iter = 93000 dist_mean = 0.4403 dist_std = 0.8088 vf_loss = 0.4913 grad_norm = 0.4402 nat_grad_norm = 0.5767 cg_residual = 0.0101 step_size = 0.4208 reward = 0.0000 fps = 7 mse_loss = 1.1400 
2022-07-08 10:07:01.280608 - gail/main.py:174 - [TRPO] iter = 94000 dist_mean = 0.3998 dist_std = 0.8051 vf_loss = 0.4249 grad_norm = 0.4701 nat_grad_norm = 0.5793 cg_residual = 0.0138 step_size = 0.3672 reward = -0.0000 fps = 5 mse_loss = 1.0146 
2022-07-08 10:07:30.172607 - gail/main.py:174 - [TRPO] iter = 95000 dist_mean = 0.3909 dist_std = 0.8033 vf_loss = 0.5157 grad_norm = 0.5560 nat_grad_norm = 0.7249 cg_residual = 0.0156 step_size = 0.3157 reward = 0.0000 fps = 5 mse_loss = 1.1111 
2022-07-08 10:07:31.034576 - gail/main.py:201 - [Discriminator] iter = 95000 loss = -3.6596 grad_norm = 3.7448 grad_penalty = 0.3556 regularization = 0.0000 true_logits = -0.1942 fake_logits = -4.2093 true_prob = 0.4851 fake_prob = 0.0250 
2022-07-08 10:08:17.382085 - gail/main.py:142 - [Evaluate] iter = 95000 episode={ returns = 279.9512 lengths = 122 } discounted_episode={ returns = 255.4356 lengths = 121 } 
2022-07-08 10:08:46.507370 - gail/main.py:174 - [TRPO] iter = 96000 dist_mean = 0.3598 dist_std = 0.8030 vf_loss = 0.3170 grad_norm = 0.4362 nat_grad_norm = 0.6227 cg_residual = 0.0099 step_size = 0.4115 reward = 0.0000 fps = 13 mse_loss = 0.9875 
2022-07-08 10:09:16.500737 - gail/main.py:174 - [TRPO] iter = 97000 dist_mean = 0.3800 dist_std = 0.8056 vf_loss = 0.3719 grad_norm = 0.5219 nat_grad_norm = 0.5984 cg_residual = 0.0152 step_size = 0.3512 reward = 0.0000 fps = 9 mse_loss = 1.0685 
2022-07-08 10:09:46.439087 - gail/main.py:174 - [TRPO] iter = 98000 dist_mean = 0.3888 dist_std = 0.7991 vf_loss = 0.5099 grad_norm = 0.6076 nat_grad_norm = 0.6038 cg_residual = 0.0137 step_size = 0.3856 reward = -0.0000 fps = 7 mse_loss = 0.9646 
2022-07-08 10:10:14.867625 - gail/main.py:174 - [TRPO] iter = 99000 dist_mean = 0.3842 dist_std = 0.7988 vf_loss = 0.6028 grad_norm = 0.4479 nat_grad_norm = 0.6067 cg_residual = 0.0108 step_size = 0.3910 reward = -0.0000 fps = 6 mse_loss = 0.9592 
2022-07-08 10:10:43.044585 - gail/main.py:174 - [TRPO] iter = 100000 dist_mean = 0.3828 dist_std = 0.7954 vf_loss = 0.4585 grad_norm = 0.4801 nat_grad_norm = 0.5617 cg_residual = 0.0101 step_size = 0.3999 reward = -0.0000 fps = 5 mse_loss = 0.8656 
2022-07-08 10:10:43.800279 - gail/main.py:201 - [Discriminator] iter = 100000 loss = -3.7043 grad_norm = 2.9938 grad_penalty = 0.3665 regularization = 0.0000 true_logits = -0.1722 fake_logits = -4.2429 true_prob = 0.4867 fake_prob = 0.0227 
2022-07-08 10:11:32.621062 - gail/main.py:142 - [Evaluate] iter = 100000 episode={ returns = 277.5413 lengths = 124 } discounted_episode={ returns = 256.1505 lengths = 124 } 
2022-07-08 10:12:10.193288 - gail/main.py:174 - [TRPO] iter = 101000 dist_mean = 0.3932 dist_std = 0.7972 vf_loss = 0.4475 grad_norm = 0.5065 nat_grad_norm = 0.6567 cg_residual = 0.0115 step_size = 0.3592 reward = 0.0000 fps = 11 mse_loss = 0.8962 
2022-07-08 10:12:40.670695 - gail/main.py:174 - [TRPO] iter = 102000 dist_mean = 0.4007 dist_std = 0.7954 vf_loss = 0.5827 grad_norm = 0.5572 nat_grad_norm = 0.6209 cg_residual = 0.0172 step_size = 0.3399 reward = -0.0000 fps = 8 mse_loss = 0.8459 
2022-07-08 10:13:09.930734 - gail/main.py:174 - [TRPO] iter = 103000 dist_mean = 0.3647 dist_std = 0.7948 vf_loss = 0.3772 grad_norm = 0.5004 nat_grad_norm = 0.5527 cg_residual = 0.0162 step_size = 0.3766 reward = -0.0000 fps = 6 mse_loss = 0.9704 
2022-07-08 10:13:37.691064 - gail/main.py:174 - [TRPO] iter = 104000 dist_mean = 0.3681 dist_std = 0.7933 vf_loss = 0.2565 grad_norm = 0.5325 nat_grad_norm = 0.5919 cg_residual = 0.0213 step_size = 0.3561 reward = -0.0000 fps = 5 mse_loss = 0.9322 
2022-07-08 10:14:05.754144 - gail/main.py:174 - [TRPO] iter = 105000 dist_mean = 0.3607 dist_std = 0.7912 vf_loss = 0.3027 grad_norm = 0.6102 nat_grad_norm = 0.6307 cg_residual = 0.0108 step_size = 0.3614 reward = -0.0000 fps = 4 mse_loss = 1.0290 
2022-07-08 10:14:06.540360 - gail/main.py:201 - [Discriminator] iter = 105000 loss = -3.8504 grad_norm = 3.9019 grad_penalty = 0.3683 regularization = 0.0000 true_logits = -0.1343 fake_logits = -4.3530 true_prob = 0.4997 fake_prob = 0.0197 
2022-07-08 10:14:54.551371 - gail/main.py:142 - [Evaluate] iter = 105000 episode={ returns = 273.7188 lengths = 123 } discounted_episode={ returns = 256.5482 lengths = 123 } 
2022-07-08 10:15:24.222284 - gail/main.py:174 - [TRPO] iter = 106000 dist_mean = 0.3645 dist_std = 0.7894 vf_loss = 0.3978 grad_norm = 0.5359 nat_grad_norm = 0.5611 cg_residual = 0.0129 step_size = 0.4047 reward = 0.0000 fps = 12 mse_loss = 1.0240 
2022-07-08 10:15:53.974132 - gail/main.py:174 - [TRPO] iter = 107000 dist_mean = 0.3998 dist_std = 0.7887 vf_loss = 0.4013 grad_norm = 0.5380 nat_grad_norm = 0.5906 cg_residual = 0.0126 step_size = 0.3835 reward = 0.0000 fps = 9 mse_loss = 0.9889 
2022-07-08 10:16:22.766419 - gail/main.py:174 - [TRPO] iter = 108000 dist_mean = 0.3699 dist_std = 0.7891 vf_loss = 0.2961 grad_norm = 0.6253 nat_grad_norm = 0.6726 cg_residual = 0.0190 step_size = 0.3350 reward = 0.0000 fps = 7 mse_loss = 0.9441 
2022-07-08 10:16:51.625526 - gail/main.py:174 - [TRPO] iter = 109000 dist_mean = 0.3471 dist_std = 0.7866 vf_loss = 0.2992 grad_norm = 0.4743 nat_grad_norm = 0.6514 cg_residual = 0.0190 step_size = 0.3674 reward = -0.0000 fps = 6 mse_loss = 0.9298 
2022-07-08 10:17:19.768660 - gail/main.py:174 - [TRPO] iter = 110000 dist_mean = 0.3533 dist_std = 0.7853 vf_loss = 0.2101 grad_norm = 0.4220 nat_grad_norm = 0.5384 cg_residual = 0.0154 step_size = 0.4296 reward = -0.0000 fps = 5 mse_loss = 0.9576 
2022-07-08 10:17:20.538996 - gail/main.py:201 - [Discriminator] iter = 110000 loss = -3.7392 grad_norm = 2.9893 grad_penalty = 0.3612 regularization = 0.0000 true_logits = -0.0683 fake_logits = -4.1687 true_prob = 0.5043 fake_prob = 0.0234 
2022-07-08 10:18:06.406577 - gail/main.py:142 - [Evaluate] iter = 110000 episode={ returns = 302.2715 lengths = 123 } discounted_episode={ returns = 281.6681 lengths = 123 } 
2022-07-08 10:18:34.226597 - gail/main.py:174 - [TRPO] iter = 111000 dist_mean = 0.3610 dist_std = 0.7880 vf_loss = 0.4796 grad_norm = 0.6177 nat_grad_norm = 0.4732 cg_residual = 0.0135 step_size = 0.4090 reward = 0.0000 fps = 13 mse_loss = 0.9339 
2022-07-08 10:19:00.890358 - gail/main.py:174 - [TRPO] iter = 112000 dist_mean = 0.3671 dist_std = 0.7848 vf_loss = 0.6359 grad_norm = 0.5827 nat_grad_norm = 0.5773 cg_residual = 0.0128 step_size = 0.3951 reward = 0.0000 fps = 9 mse_loss = 0.9412 
2022-07-08 10:19:29.493808 - gail/main.py:174 - [TRPO] iter = 113000 dist_mean = 0.3337 dist_std = 0.7867 vf_loss = 0.2995 grad_norm = 0.6671 nat_grad_norm = 0.5601 cg_residual = 0.0172 step_size = 0.3422 reward = -0.0000 fps = 7 mse_loss = 0.9780 
2022-07-08 10:19:58.139171 - gail/main.py:174 - [TRPO] iter = 114000 dist_mean = 0.3524 dist_std = 0.7867 vf_loss = 0.2808 grad_norm = 0.6072 nat_grad_norm = 0.5344 cg_residual = 0.0176 step_size = 0.3793 reward = -0.0000 fps = 6 mse_loss = 1.1750 
2022-07-08 10:20:27.630066 - gail/main.py:174 - [TRPO] iter = 115000 dist_mean = 0.3044 dist_std = 0.7837 vf_loss = 0.2490 grad_norm = 0.4879 nat_grad_norm = 0.4987 cg_residual = 0.0149 step_size = 0.4254 reward = -0.0000 fps = 5 mse_loss = 1.0038 
2022-07-08 10:20:28.700869 - gail/main.py:201 - [Discriminator] iter = 115000 loss = -3.7081 grad_norm = 3.0084 grad_penalty = 0.3379 regularization = 0.0000 true_logits = -0.0430 fake_logits = -4.0889 true_prob = 0.5123 fake_prob = 0.0261 
2022-07-08 10:21:16.065065 - gail/main.py:142 - [Evaluate] iter = 115000 episode={ returns = 312.8352 lengths = 123 } discounted_episode={ returns = 289.0721 lengths = 123 } 
2022-07-08 10:21:50.199191 - gail/main.py:174 - [TRPO] iter = 116000 dist_mean = 0.4144 dist_std = 0.7827 vf_loss = 0.6487 grad_norm = 0.5815 nat_grad_norm = 0.7204 cg_residual = 0.0232 step_size = 0.3333 reward = -0.0000 fps = 12 mse_loss = 1.1099 
2022-07-08 10:22:17.672157 - gail/main.py:174 - [TRPO] iter = 117000 dist_mean = 0.3356 dist_std = 0.7829 vf_loss = 0.2853 grad_norm = 0.6045 nat_grad_norm = 0.5287 cg_residual = 0.0329 step_size = 0.3518 reward = -0.0000 fps = 9 mse_loss = 1.0327 
2022-07-08 10:22:46.119038 - gail/main.py:174 - [TRPO] iter = 118000 dist_mean = 0.3177 dist_std = 0.7843 vf_loss = 0.3298 grad_norm = 0.4958 nat_grad_norm = 0.5126 cg_residual = 0.0140 step_size = 0.3952 reward = -0.0000 fps = 7 mse_loss = 1.0043 
2022-07-08 10:23:14.663467 - gail/main.py:174 - [TRPO] iter = 119000 dist_mean = 0.3512 dist_std = 0.7844 vf_loss = 0.3870 grad_norm = 0.6867 nat_grad_norm = 0.7193 cg_residual = 0.0277 step_size = 0.3192 reward = -0.0000 fps = 6 mse_loss = 0.9754 
2022-07-08 10:23:43.317978 - gail/main.py:174 - [TRPO] iter = 120000 dist_mean = 0.3725 dist_std = 0.7837 vf_loss = 0.4328 grad_norm = 0.5185 nat_grad_norm = 0.6190 cg_residual = 0.0202 step_size = 0.3681 reward = -0.0000 fps = 5 mse_loss = 1.0229 
2022-07-08 10:23:44.077439 - gail/main.py:201 - [Discriminator] iter = 120000 loss = -3.8652 grad_norm = 3.0519 grad_penalty = 0.3635 regularization = 0.0000 true_logits = -0.0433 fake_logits = -4.2719 true_prob = 0.5139 fake_prob = 0.0206 
2022-07-08 10:24:32.712085 - gail/main.py:142 - [Evaluate] iter = 120000 episode={ returns = 301.6089 lengths = 123 } discounted_episode={ returns = 278.8715 lengths = 123 } 
2022-07-08 10:25:02.056365 - gail/main.py:174 - [TRPO] iter = 121000 dist_mean = 0.3292 dist_std = 0.7830 vf_loss = 0.3711 grad_norm = 0.6015 nat_grad_norm = 0.6222 cg_residual = 0.0279 step_size = 0.3502 reward = -0.0000 fps = 12 mse_loss = 0.9485 
2022-07-08 10:25:30.514340 - gail/main.py:174 - [TRPO] iter = 122000 dist_mean = 0.3442 dist_std = 0.7782 vf_loss = 0.4004 grad_norm = 0.7874 nat_grad_norm = 0.5388 cg_residual = 0.0209 step_size = 0.3547 reward = -0.0000 fps = 9 mse_loss = 1.0933 
2022-07-08 10:25:58.966714 - gail/main.py:174 - [TRPO] iter = 123000 dist_mean = 0.3619 dist_std = 0.7733 vf_loss = 0.4348 grad_norm = 0.5814 nat_grad_norm = 0.6659 cg_residual = 0.0360 step_size = 0.3359 reward = -0.0000 fps = 7 mse_loss = 0.9582 
2022-07-08 10:26:31.130548 - gail/main.py:174 - [TRPO] iter = 124000 dist_mean = 0.3449 dist_std = 0.7731 vf_loss = 0.4424 grad_norm = 0.5418 nat_grad_norm = 0.6609 cg_residual = 0.0533 step_size = 0.3298 reward = 0.0000 fps = 5 mse_loss = 1.0246 
2022-07-08 10:27:00.685970 - gail/main.py:174 - [TRPO] iter = 125000 dist_mean = 0.3250 dist_std = 0.7704 vf_loss = 0.3376 grad_norm = 0.5471 nat_grad_norm = 0.5692 cg_residual = 0.0258 step_size = 0.3694 reward = -0.0000 fps = 5 mse_loss = 1.0524 
2022-07-08 10:27:01.553818 - gail/main.py:201 - [Discriminator] iter = 125000 loss = -3.7185 grad_norm = 4.0510 grad_penalty = 0.3438 regularization = 0.0000 true_logits = -0.0096 fake_logits = -4.0718 true_prob = 0.5155 fake_prob = 0.0291 
2022-07-08 10:27:53.309696 - gail/main.py:142 - [Evaluate] iter = 125000 episode={ returns = 347.5227 lengths = 130 } discounted_episode={ returns = 324.1180 lengths = 130 } 
2022-07-08 10:28:22.660489 - gail/main.py:174 - [TRPO] iter = 126000 dist_mean = 0.3317 dist_std = 0.7666 vf_loss = 0.2446 grad_norm = 0.6361 nat_grad_norm = 0.5433 cg_residual = 0.0254 step_size = 0.3553 reward = 0.0000 fps = 12 mse_loss = 1.0808 
2022-07-08 10:28:52.366748 - gail/main.py:174 - [TRPO] iter = 127000 dist_mean = 0.3236 dist_std = 0.7658 vf_loss = 0.2390 grad_norm = 0.8049 nat_grad_norm = 0.5506 cg_residual = 0.0421 step_size = 0.3195 reward = 0.0000 fps = 9 mse_loss = 1.0646 
2022-07-08 10:29:21.859631 - gail/main.py:174 - [TRPO] iter = 128000 dist_mean = 0.3403 dist_std = 0.7662 vf_loss = 0.2394 grad_norm = 0.4475 nat_grad_norm = 0.4895 cg_residual = 0.0213 step_size = 0.4085 reward = 0.0000 fps = 7 mse_loss = 1.0435 
2022-07-08 10:29:52.804389 - gail/main.py:174 - [TRPO] iter = 129000 dist_mean = 0.3475 dist_std = 0.7666 vf_loss = 0.3309 grad_norm = 0.6984 nat_grad_norm = 0.5310 cg_residual = 0.0294 step_size = 0.3668 reward = 0.0000 fps = 5 mse_loss = 0.9500 
2022-07-08 10:30:22.443945 - gail/main.py:174 - [TRPO] iter = 130000 dist_mean = 0.3164 dist_std = 0.7625 vf_loss = 0.3161 grad_norm = 0.6234 nat_grad_norm = 0.4996 cg_residual = 0.0180 step_size = 0.3856 reward = -0.0000 fps = 4 mse_loss = 0.9613 
2022-07-08 10:30:23.311434 - gail/main.py:201 - [Discriminator] iter = 130000 loss = -3.5965 grad_norm = 3.6001 grad_penalty = 0.3353 regularization = 0.0000 true_logits = -0.0264 fake_logits = -3.9582 true_prob = 0.5164 fake_prob = 0.0302 
2022-07-08 10:31:15.187058 - gail/main.py:142 - [Evaluate] iter = 130000 episode={ returns = 310.9466 lengths = 126 } discounted_episode={ returns = 288.8319 lengths = 126 } 
2022-07-08 10:31:51.981749 - gail/main.py:174 - [TRPO] iter = 131000 dist_mean = 0.3458 dist_std = 0.7623 vf_loss = 0.3860 grad_norm = 0.7721 nat_grad_norm = 0.5110 cg_residual = 0.0218 step_size = 0.3510 reward = -0.0000 fps = 11 mse_loss = 1.0013 
