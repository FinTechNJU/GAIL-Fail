2022-07-08 09:05:02.551811 - utils/flags.py:257 - log_dir = logs/gail_w-Hopper-v2-300-2022-07-08-09-05-01
2022-07-08 09:05:20.581729 - gail/utils/replay_buffer.py:86 - Load dataset from /content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/Hopper-v2
2022-07-08 09:05:40.096185 - gail/main.py:80 - Expert Reward 3582.436530
2022-07-08 09:05:40.994061 - gail/main.py:84 - Original dataset size 3000
2022-07-08 09:05:41.096697 - gail/main.py:86 - Subsampled dataset size 3000
2022-07-08 09:05:41.110534 - gail/main.py:87 - np random: 864 random : 786
2022-07-08 09:05:41.116054 - gail/main.py:91 - Sampled obs: 0.4652, acs: 0.0749
2022-07-08 09:05:43.802130 - lunzi/nn/flat_param.py:17 - Enabling flattening... ['GaussianMLPPolicy_1/log_std:0', 'GaussianMLPPolicy_1/Variable:0', 'GaussianMLPPolicy_1/Variable_1:0', 'GaussianMLPPolicy_1/Variable_2:0', 'GaussianMLPPolicy_1/Variable_3:0', 'GaussianMLPPolicy_1/Variable_4:0', 'GaussianMLPPolicy_1/Variable_5:0']
2022-07-08 09:06:04.926527 - gail/discriminator/discriminator.py:30 - Use predefined normalization.
2022-07-08 09:06:04.955330 - gail/discriminator/discriminator.py:33 - Normalizer loc:[[ 1.3966653  -0.06165453 -0.19472611 -0.4584401   0.18350822  2.5732448
   0.00400542 -0.00549955 -0.04755233 -0.02386179  0.00759995]] 
 scale:[[0.16755195 0.05883223 0.15990146 0.34682125 0.5992658  0.6461788
  1.5187451  0.8811966  2.0685835  3.6282625  5.862049  ]]
2022-07-08 09:06:15.604044 - gail/discriminator/discriminator.py:67 - Discriminator uses Wasserstein distance.
2022-07-08 09:06:15.609092 - gail/discriminator/discriminator.py:68 - [<tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable:0' shape=(14, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_1:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_2:0' shape=(100, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_3:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_4:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_5:0' shape=(1,) dtype=float32_ref>]
2022-07-08 09:06:15.613773 - gail/discriminator/discriminator.py:69 - Use gradient penalty regularization (coef = 10.000000)
2022-07-08 09:06:18.122866 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions_mean]
2022-07-08 09:07:03.605011 - gail/main.py:142 - [Evaluate] iter = 0 episode={ returns = 176.4389 lengths = 193 } discounted_episode={ returns = 145.2316 lengths = 186 } 
2022-07-08 09:07:03.615587 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions]
2022-07-08 09:07:25.928864 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [fake_states fake_actions] => [scaled_neural_reward]
2022-07-08 09:07:26.574709 - lunzi/nn/module.py:71 - [MLPVFunction] is making TensorFlow callables, key = [states] => [values]
2022-07-08 09:07:27.754942 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [] => [sync_old]
2022-07-08 09:07:28.427047 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss flat_grad dist_std mean_kl dist_mean]
2022-07-08 09:07:32.212972 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states tangents actions] => [hessian_vec_prod]
2022-07-08 09:07:38.714105 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [] => [get_flat]
2022-07-08 09:07:39.396608 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [feed_flat] => [set_flat]
2022-07-08 09:07:40.079861 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss mean_kl]
2022-07-08 09:07:41.192252 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [train_vf vf_loss]
2022-07-08 09:07:43.095288 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [vf_loss]
2022-07-08 09:07:43.868068 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states actions_] => [mse_loss]
2022-07-08 09:07:44.622743 - gail/main.py:174 - [TRPO] iter = 1000 dist_mean = 0.0000 dist_std = 1.0000 vf_loss = 0.0893 grad_norm = 0.2735 nat_grad_norm = 0.3080 cg_residual = 0.0000 step_size = 0.5282 reward = -0.0000 fps = 11 mse_loss = 0.3020 
2022-07-08 09:08:00.908792 - gail/main.py:174 - [TRPO] iter = 2000 dist_mean = 0.0518 dist_std = 0.9998 vf_loss = 0.1021 grad_norm = 0.1874 nat_grad_norm = 0.2988 cg_residual = 0.0000 step_size = 0.7212 reward = -0.0000 fps = 9 mse_loss = 0.2801 
2022-07-08 09:08:17.992326 - gail/main.py:174 - [TRPO] iter = 3000 dist_mean = 0.0605 dist_std = 1.0094 vf_loss = 0.0958 grad_norm = 0.2970 nat_grad_norm = 0.3500 cg_residual = 0.0000 step_size = 0.4870 reward = 0.0000 fps = 8 mse_loss = 0.3062 
2022-07-08 09:08:35.041213 - gail/main.py:174 - [TRPO] iter = 4000 dist_mean = 0.0843 dist_std = 1.0021 vf_loss = 0.0983 grad_norm = 0.2958 nat_grad_norm = 0.4074 cg_residual = 0.0000 step_size = 0.4906 reward = 0.0000 fps = 7 mse_loss = 0.3226 
2022-07-08 09:08:51.637236 - gail/main.py:174 - [TRPO] iter = 5000 dist_mean = 0.1118 dist_std = 0.9981 vf_loss = 0.1753 grad_norm = 0.3245 nat_grad_norm = 0.3193 cg_residual = 0.0000 step_size = 0.5108 reward = -0.0000 fps = 6 mse_loss = 0.3361 
2022-07-08 09:08:51.640072 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [true_states true_actions fake_states fake_actions true_masks] => [train loss true_logits fake_logits true_prob fake_prob grad_norm grad_penalty regularization]
2022-07-08 09:08:57.877562 - gail/main.py:201 - [Discriminator] iter = 5000 loss = 1.6857 grad_norm = 13.9240 grad_penalty = 1.7181 regularization = 0.0000 true_logits = -0.0537 fake_logits = -0.0862 true_prob = 0.4866 fake_prob = 0.4785 
2022-07-08 09:09:04.419421 - gail/main.py:142 - [Evaluate] iter = 5000 episode={ returns = 24.3797 lengths = 23 } discounted_episode={ returns = 24.1963 lengths = 23 } 
2022-07-08 09:09:28.730275 - gail/main.py:174 - [TRPO] iter = 6000 dist_mean = 0.1356 dist_std = 0.9838 vf_loss = 0.1885 grad_norm = 0.3739 nat_grad_norm = 0.3805 cg_residual = 0.0000 step_size = 0.4288 reward = -0.0000 fps = 32 mse_loss = 0.3522 
2022-07-08 09:09:50.571415 - gail/main.py:174 - [TRPO] iter = 7000 dist_mean = 0.1719 dist_std = 0.9871 vf_loss = 0.3947 grad_norm = 0.4376 nat_grad_norm = 0.3273 cg_residual = 0.0000 step_size = 0.4055 reward = -0.0000 fps = 18 mse_loss = 0.4048 
2022-07-08 09:10:13.410074 - gail/main.py:174 - [TRPO] iter = 8000 dist_mean = 0.2410 dist_std = 0.9820 vf_loss = 0.5669 grad_norm = 0.4189 nat_grad_norm = 0.4182 cg_residual = 0.0000 step_size = 0.4188 reward = 0.0000 fps = 13 mse_loss = 0.3722 
2022-07-08 09:10:40.007484 - gail/main.py:174 - [TRPO] iter = 9000 dist_mean = 0.2971 dist_std = 0.9815 vf_loss = 0.4890 grad_norm = 0.5694 nat_grad_norm = 0.3796 cg_residual = 0.0000 step_size = 0.3700 reward = 0.0000 fps = 9 mse_loss = 0.3895 
2022-07-08 09:11:09.922346 - gail/main.py:174 - [TRPO] iter = 10000 dist_mean = 0.3015 dist_std = 0.9738 vf_loss = 0.5037 grad_norm = 0.4065 nat_grad_norm = 0.4599 cg_residual = 0.0001 step_size = 0.4226 reward = -0.0000 fps = 7 mse_loss = 0.4640 
2022-07-08 09:11:11.171800 - gail/main.py:201 - [Discriminator] iter = 10000 loss = 0.6828 grad_norm = 9.0137 grad_penalty = 0.8898 regularization = 0.0000 true_logits = -0.0209 fake_logits = -0.2278 true_prob = 0.4948 fake_prob = 0.4435 
2022-07-08 09:11:34.048124 - gail/main.py:142 - [Evaluate] iter = 10000 episode={ returns = 88.7811 lengths = 60 } discounted_episode={ returns = 85.9995 lengths = 60 } 
2022-07-08 09:12:01.486429 - gail/main.py:174 - [TRPO] iter = 11000 dist_mean = 0.3543 dist_std = 0.9616 vf_loss = 0.5588 grad_norm = 0.4426 nat_grad_norm = 0.3434 cg_residual = 0.0000 step_size = 0.4756 reward = -0.0000 fps = 19 mse_loss = 0.4854 
2022-07-08 09:12:30.231812 - gail/main.py:174 - [TRPO] iter = 12000 dist_mean = 0.3449 dist_std = 0.9428 vf_loss = 0.6071 grad_norm = 0.3303 nat_grad_norm = 0.3769 cg_residual = 0.0002 step_size = 0.4952 reward = 0.0000 fps = 12 mse_loss = 0.4574 
2022-07-08 09:12:59.721547 - gail/main.py:174 - [TRPO] iter = 13000 dist_mean = 0.3460 dist_std = 0.9248 vf_loss = 0.4828 grad_norm = 0.4647 nat_grad_norm = 0.3377 cg_residual = 0.0003 step_size = 0.4662 reward = 0.0000 fps = 9 mse_loss = 0.5401 
2022-07-08 09:13:28.192669 - gail/main.py:174 - [TRPO] iter = 14000 dist_mean = 0.3678 dist_std = 0.9177 vf_loss = 0.3280 grad_norm = 0.4834 nat_grad_norm = 0.3665 cg_residual = 0.0005 step_size = 0.4692 reward = -0.0000 fps = 7 mse_loss = 0.6540 
2022-07-08 09:13:57.402135 - gail/main.py:174 - [TRPO] iter = 15000 dist_mean = 0.4183 dist_std = 0.9068 vf_loss = 0.2204 grad_norm = 0.5655 nat_grad_norm = 0.3795 cg_residual = 0.0003 step_size = 0.4271 reward = 0.0000 fps = 6 mse_loss = 0.7990 
2022-07-08 09:13:58.192210 - gail/main.py:201 - [Discriminator] iter = 15000 loss = 0.3881 grad_norm = 8.2585 grad_penalty = 0.7118 regularization = 0.0000 true_logits = -0.0021 fake_logits = -0.3258 true_prob = 0.4996 fake_prob = 0.4197 
2022-07-08 09:14:19.447852 - gail/main.py:142 - [Evaluate] iter = 15000 episode={ returns = 93.8004 lengths = 61 } discounted_episode={ returns = 91.3223 lengths = 61 } 
2022-07-08 09:14:48.154547 - gail/main.py:174 - [TRPO] iter = 16000 dist_mean = 0.4660 dist_std = 0.8948 vf_loss = 0.1118 grad_norm = 0.3669 nat_grad_norm = 0.3298 cg_residual = 0.0003 step_size = 0.5510 reward = -0.0000 fps = 20 mse_loss = 0.8771 
2022-07-08 09:15:16.845527 - gail/main.py:174 - [TRPO] iter = 17000 dist_mean = 0.4318 dist_std = 0.8815 vf_loss = 0.0665 grad_norm = 0.4872 nat_grad_norm = 0.3167 cg_residual = 0.0002 step_size = 0.5254 reward = -0.0000 fps = 12 mse_loss = 0.8386 
2022-07-08 09:15:43.892227 - gail/main.py:174 - [TRPO] iter = 18000 dist_mean = 0.4945 dist_std = 0.8704 vf_loss = 0.0552 grad_norm = 0.4926 nat_grad_norm = 0.3294 cg_residual = 0.0003 step_size = 0.4539 reward = -0.0000 fps = 9 mse_loss = 0.9216 
2022-07-08 09:16:11.238863 - gail/main.py:174 - [TRPO] iter = 19000 dist_mean = 0.4956 dist_std = 0.8568 vf_loss = 0.0353 grad_norm = 0.5008 nat_grad_norm = 0.2871 cg_residual = 0.0003 step_size = 0.5071 reward = -0.0000 fps = 7 mse_loss = 0.9007 
2022-07-08 09:16:39.117549 - gail/main.py:174 - [TRPO] iter = 20000 dist_mean = 0.5401 dist_std = 0.8394 vf_loss = 0.0499 grad_norm = 0.5223 nat_grad_norm = 0.3039 cg_residual = 0.0005 step_size = 0.4590 reward = 0.0000 fps = 6 mse_loss = 0.9233 
2022-07-08 09:16:39.980786 - gail/main.py:201 - [Discriminator] iter = 20000 loss = 0.0875 grad_norm = 6.1094 grad_penalty = 0.5830 regularization = 0.0000 true_logits = 0.0155 fake_logits = -0.4799 true_prob = 0.5041 fake_prob = 0.3830 
2022-07-08 09:17:00.763192 - gail/main.py:142 - [Evaluate] iter = 20000 episode={ returns = 87.1648 lengths = 55 } discounted_episode={ returns = 85.2060 lengths = 55 } 
2022-07-08 09:17:28.489269 - gail/main.py:174 - [TRPO] iter = 21000 dist_mean = 0.5526 dist_std = 0.8208 vf_loss = 0.0402 grad_norm = 0.3878 nat_grad_norm = 0.3228 cg_residual = 0.0005 step_size = 0.5359 reward = 0.0000 fps = 20 mse_loss = 0.9190 
2022-07-08 09:17:55.716298 - gail/main.py:174 - [TRPO] iter = 22000 dist_mean = 0.5684 dist_std = 0.8097 vf_loss = 0.0241 grad_norm = 0.5189 nat_grad_norm = 0.2876 cg_residual = 0.0007 step_size = 0.5116 reward = -0.0000 fps = 13 mse_loss = 1.0207 
2022-07-08 09:18:22.583299 - gail/main.py:174 - [TRPO] iter = 23000 dist_mean = 0.5918 dist_std = 0.8014 vf_loss = 0.0287 grad_norm = 0.4687 nat_grad_norm = 0.2930 cg_residual = 0.0006 step_size = 0.5499 reward = -0.0000 fps = 9 mse_loss = 1.1172 
2022-07-08 09:18:50.116332 - gail/main.py:174 - [TRPO] iter = 24000 dist_mean = 0.6220 dist_std = 0.7893 vf_loss = 0.0269 grad_norm = 0.7337 nat_grad_norm = 0.3734 cg_residual = 0.0019 step_size = 0.4126 reward = 0.0000 fps = 7 mse_loss = 1.0835 
2022-07-08 09:19:17.093097 - gail/main.py:174 - [TRPO] iter = 25000 dist_mean = 0.5717 dist_std = 0.7863 vf_loss = 0.0373 grad_norm = 0.6201 nat_grad_norm = 0.2633 cg_residual = 0.0004 step_size = 0.5332 reward = 0.0000 fps = 6 mse_loss = 1.0986 
2022-07-08 09:19:17.850871 - gail/main.py:201 - [Discriminator] iter = 25000 loss = -0.1417 grad_norm = 6.0387 grad_penalty = 0.5568 regularization = 0.0000 true_logits = 0.0139 fake_logits = -0.6845 true_prob = 0.5040 fake_prob = 0.3369 
2022-07-08 09:19:39.153067 - gail/main.py:142 - [Evaluate] iter = 25000 episode={ returns = 95.2370 lengths = 58 } discounted_episode={ returns = 92.0204 lengths = 58 } 
2022-07-08 09:20:06.342730 - gail/main.py:174 - [TRPO] iter = 26000 dist_mean = 0.6131 dist_std = 0.7816 vf_loss = 0.0365 grad_norm = 0.6994 nat_grad_norm = 0.2951 cg_residual = 0.0005 step_size = 0.5080 reward = 0.0000 fps = 20 mse_loss = 1.1452 
2022-07-08 09:20:33.867124 - gail/main.py:174 - [TRPO] iter = 27000 dist_mean = 0.6012 dist_std = 0.7709 vf_loss = 0.0465 grad_norm = 0.8729 nat_grad_norm = 0.3613 cg_residual = 0.0012 step_size = 0.3990 reward = 0.0000 fps = 13 mse_loss = 1.1476 
2022-07-08 09:21:00.228268 - gail/main.py:174 - [TRPO] iter = 28000 dist_mean = 0.5869 dist_std = 0.7633 vf_loss = 0.0283 grad_norm = 0.7586 nat_grad_norm = 0.2943 cg_residual = 0.0012 step_size = 0.5294 reward = 0.0000 fps = 9 mse_loss = 1.0216 
2022-07-08 09:21:26.124085 - gail/main.py:174 - [TRPO] iter = 29000 dist_mean = 0.5655 dist_std = 0.7598 vf_loss = 0.0498 grad_norm = 0.7813 nat_grad_norm = 0.3359 cg_residual = 0.0013 step_size = 0.5000 reward = 0.0000 fps = 7 mse_loss = 1.0321 
2022-07-08 09:21:57.958304 - gail/main.py:174 - [TRPO] iter = 30000 dist_mean = 0.5502 dist_std = 0.7489 vf_loss = 0.0319 grad_norm = 0.4918 nat_grad_norm = 0.3317 cg_residual = 0.0021 step_size = 0.5777 reward = 0.0000 fps = 6 mse_loss = 1.1751 
2022-07-08 09:21:59.377244 - gail/main.py:201 - [Discriminator] iter = 30000 loss = -0.5012 grad_norm = 5.6909 grad_penalty = 0.4362 regularization = 0.0000 true_logits = 0.0422 fake_logits = -0.8952 true_prob = 0.5113 fake_prob = 0.2933 
2022-07-08 09:22:21.365467 - gail/main.py:142 - [Evaluate] iter = 30000 episode={ returns = 107.6240 lengths = 62 } discounted_episode={ returns = 104.2008 lengths = 62 } 
2022-07-08 09:22:48.481812 - gail/main.py:174 - [TRPO] iter = 31000 dist_mean = 0.5943 dist_std = 0.7443 vf_loss = 0.0365 grad_norm = 0.8799 nat_grad_norm = 0.3025 cg_residual = 0.0021 step_size = 0.5014 reward = -0.0000 fps = 20 mse_loss = 1.1433 
2022-07-08 09:23:16.701630 - gail/main.py:174 - [TRPO] iter = 32000 dist_mean = 0.5600 dist_std = 0.7389 vf_loss = 0.0435 grad_norm = 0.7430 nat_grad_norm = 0.2922 cg_residual = 0.0016 step_size = 0.5696 reward = 0.0000 fps = 12 mse_loss = 1.1096 
2022-07-08 09:23:44.099492 - gail/main.py:174 - [TRPO] iter = 33000 dist_mean = 0.5521 dist_std = 0.7374 vf_loss = 0.0165 grad_norm = 0.5302 nat_grad_norm = 0.3699 cg_residual = 0.0039 step_size = 0.5218 reward = 0.0000 fps = 9 mse_loss = 1.1412 
2022-07-08 09:24:10.922706 - gail/main.py:174 - [TRPO] iter = 34000 dist_mean = 0.5530 dist_std = 0.7338 vf_loss = 0.1000 grad_norm = 0.6982 nat_grad_norm = 0.6139 cg_residual = 0.0052 step_size = 0.3122 reward = -0.0000 fps = 7 mse_loss = 1.2430 
2022-07-08 09:24:36.389392 - gail/main.py:174 - [TRPO] iter = 35000 dist_mean = 0.5610 dist_std = 0.7302 vf_loss = 0.3655 grad_norm = 0.4167 nat_grad_norm = 0.4167 cg_residual = 0.0068 step_size = 0.5692 reward = -0.0000 fps = 6 mse_loss = 1.2588 
2022-07-08 09:24:37.170680 - gail/main.py:201 - [Discriminator] iter = 35000 loss = -0.7409 grad_norm = 5.1965 grad_penalty = 0.4136 regularization = 0.0000 true_logits = 0.0581 fake_logits = -1.0964 true_prob = 0.5161 fake_prob = 0.2584 
2022-07-08 09:24:58.968588 - gail/main.py:142 - [Evaluate] iter = 35000 episode={ returns = 116.9039 lengths = 66 } discounted_episode={ returns = 112.8270 lengths = 66 } 
2022-07-08 09:25:24.388482 - gail/main.py:174 - [TRPO] iter = 36000 dist_mean = 0.5538 dist_std = 0.7278 vf_loss = 0.2866 grad_norm = 0.8970 nat_grad_norm = 0.4513 cg_residual = 0.0095 step_size = 0.4177 reward = 0.0000 fps = 21 mse_loss = 1.2210 
2022-07-08 09:25:51.034797 - gail/main.py:174 - [TRPO] iter = 37000 dist_mean = 0.4665 dist_std = 0.7255 vf_loss = 0.6317 grad_norm = 0.6327 nat_grad_norm = 0.4792 cg_residual = 0.0077 step_size = 0.4790 reward = -0.0000 fps = 13 mse_loss = 1.2945 
2022-07-08 09:26:18.562568 - gail/main.py:174 - [TRPO] iter = 38000 dist_mean = 0.4092 dist_std = 0.7269 vf_loss = 0.8376 grad_norm = 0.8387 nat_grad_norm = 0.3284 cg_residual = 0.0093 step_size = 0.4964 reward = -0.0000 fps = 9 mse_loss = 1.2596 
2022-07-08 09:26:44.951166 - gail/main.py:174 - [TRPO] iter = 39000 dist_mean = 0.3617 dist_std = 0.7214 vf_loss = 0.3769 grad_norm = 0.5800 nat_grad_norm = 0.3820 cg_residual = 0.0201 step_size = 0.5211 reward = 0.0000 fps = 7 mse_loss = 1.2779 
2022-07-08 09:27:10.764816 - gail/main.py:174 - [TRPO] iter = 40000 dist_mean = 0.3399 dist_std = 0.7221 vf_loss = 0.1649 grad_norm = 0.8089 nat_grad_norm = 0.3147 cg_residual = 0.0079 step_size = 0.4996 reward = 0.0000 fps = 6 mse_loss = 1.3948 
2022-07-08 09:27:11.549368 - gail/main.py:201 - [Discriminator] iter = 40000 loss = -0.4835 grad_norm = 5.8566 grad_penalty = 0.5082 regularization = 0.0000 true_logits = 0.0599 fake_logits = -0.9317 true_prob = 0.5170 fake_prob = 0.3001 
2022-07-08 09:27:45.587779 - gail/main.py:142 - [Evaluate] iter = 40000 episode={ returns = 249.5141 lengths = 101 } discounted_episode={ returns = 234.0070 lengths = 101 } 
2022-07-08 09:28:10.366006 - gail/main.py:174 - [TRPO] iter = 41000 dist_mean = 0.2923 dist_std = 0.7262 vf_loss = 0.4023 grad_norm = 1.4171 nat_grad_norm = 0.8402 cg_residual = 0.1683 step_size = 0.2417 reward = 0.0000 fps = 17 mse_loss = 1.4647 
2022-07-08 09:28:35.893389 - gail/main.py:174 - [TRPO] iter = 42000 dist_mean = 0.3061 dist_std = 0.7236 vf_loss = 0.1458 grad_norm = 0.6272 nat_grad_norm = 0.3380 cg_residual = 0.0050 step_size = 0.5074 reward = -0.0000 fps = 11 mse_loss = 1.4303 
2022-07-08 09:29:02.363474 - gail/main.py:174 - [TRPO] iter = 43000 dist_mean = 0.3266 dist_std = 0.7178 vf_loss = 0.0887 grad_norm = 0.9288 nat_grad_norm = 0.4896 cg_residual = 0.0109 step_size = 0.3895 reward = 0.0000 fps = 9 mse_loss = 1.3716 
2022-07-08 09:29:28.500355 - gail/main.py:174 - [TRPO] iter = 44000 dist_mean = 0.3295 dist_std = 0.7144 vf_loss = 0.3454 grad_norm = 1.0177 nat_grad_norm = 0.3538 cg_residual = 0.0333 step_size = 0.4502 reward = 0.0000 fps = 7 mse_loss = 1.2712 
2022-07-08 09:29:54.489037 - gail/main.py:174 - [TRPO] iter = 45000 dist_mean = 0.2710 dist_std = 0.7122 vf_loss = 0.1543 grad_norm = 0.5951 nat_grad_norm = 0.4132 cg_residual = 0.0664 step_size = 0.4624 reward = -0.0000 fps = 6 mse_loss = 1.1029 
2022-07-08 09:29:55.306182 - gail/main.py:201 - [Discriminator] iter = 45000 loss = -0.8251 grad_norm = 5.5698 grad_penalty = 0.3405 regularization = 0.0000 true_logits = 0.0698 fake_logits = -1.0957 true_prob = 0.5198 fake_prob = 0.2675 
2022-07-08 09:30:30.464910 - gail/main.py:142 - [Evaluate] iter = 45000 episode={ returns = 269.6948 lengths = 104 } discounted_episode={ returns = 253.5484 lengths = 104 } 
2022-07-08 09:30:56.069598 - gail/main.py:174 - [TRPO] iter = 46000 dist_mean = 0.3070 dist_std = 0.7042 vf_loss = 0.1506 grad_norm = 0.3894 nat_grad_norm = 0.3312 cg_residual = 0.0066 step_size = 0.6270 reward = 0.0000 fps = 16 mse_loss = 1.0870 
2022-07-08 09:31:22.012115 - gail/main.py:174 - [TRPO] iter = 47000 dist_mean = 0.2813 dist_std = 0.7014 vf_loss = 0.3584 grad_norm = 0.6880 nat_grad_norm = 0.4733 cg_residual = 0.0272 step_size = 0.4337 reward = 0.0000 fps = 11 mse_loss = 1.0862 
2022-07-08 09:31:48.213331 - gail/main.py:174 - [TRPO] iter = 48000 dist_mean = 0.3196 dist_std = 0.6943 vf_loss = 0.3517 grad_norm = 1.1561 nat_grad_norm = 0.4025 cg_residual = 0.0160 step_size = 0.4302 reward = -0.0000 fps = 8 mse_loss = 1.1177 
2022-07-08 09:32:18.266458 - gail/main.py:174 - [TRPO] iter = 49000 dist_mean = 0.2884 dist_std = 0.6933 vf_loss = 0.3162 grad_norm = 0.7013 nat_grad_norm = 0.4456 cg_residual = 0.0137 step_size = 0.4587 reward = -0.0000 fps = 6 mse_loss = 1.2251 
2022-07-08 09:32:45.484174 - gail/main.py:174 - [TRPO] iter = 50000 dist_mean = 0.2963 dist_std = 0.6892 vf_loss = 0.3213 grad_norm = 1.0030 nat_grad_norm = 0.4625 cg_residual = 0.0397 step_size = 0.3907 reward = 0.0000 fps = 5 mse_loss = 1.1746 
2022-07-08 09:32:46.259958 - gail/main.py:201 - [Discriminator] iter = 50000 loss = -1.0419 grad_norm = 4.5450 grad_penalty = 0.2627 regularization = 0.0000 true_logits = 0.1010 fake_logits = -1.2035 true_prob = 0.5278 fake_prob = 0.2501 
2022-07-08 09:33:28.384915 - gail/main.py:142 - [Evaluate] iter = 50000 episode={ returns = 387.6634 lengths = 132 } discounted_episode={ returns = 357.4410 lengths = 132 } 
2022-07-08 09:33:53.450371 - gail/main.py:174 - [TRPO] iter = 51000 dist_mean = 0.2658 dist_std = 0.6819 vf_loss = 0.2332 grad_norm = 0.7937 nat_grad_norm = 0.3126 cg_residual = 0.0060 step_size = 0.4730 reward = 0.0000 fps = 14 mse_loss = 1.1847 
2022-07-08 09:34:18.228414 - gail/main.py:174 - [TRPO] iter = 52000 dist_mean = 0.3177 dist_std = 0.6791 vf_loss = 0.2309 grad_norm = 0.8187 nat_grad_norm = 0.2923 cg_residual = 0.0094 step_size = 0.5217 reward = -0.0000 fps = 10 mse_loss = 1.1384 
2022-07-08 09:34:42.608845 - gail/main.py:174 - [TRPO] iter = 53000 dist_mean = 0.2621 dist_std = 0.6780 vf_loss = 0.2392 grad_norm = 0.4685 nat_grad_norm = 0.3811 cg_residual = 0.0072 step_size = 0.5821 reward = 0.0000 fps = 8 mse_loss = 1.3298 
2022-07-08 09:35:07.910309 - gail/main.py:174 - [TRPO] iter = 54000 dist_mean = 0.2744 dist_std = 0.6775 vf_loss = 0.3044 grad_norm = 0.8753 nat_grad_norm = 0.3416 cg_residual = 0.0341 step_size = 0.4986 reward = 0.0000 fps = 7 mse_loss = 1.1911 
2022-07-08 09:35:32.483226 - gail/main.py:174 - [TRPO] iter = 55000 dist_mean = 0.2847 dist_std = 0.6777 vf_loss = 0.4324 grad_norm = 0.6144 nat_grad_norm = 0.5253 cg_residual = 0.0173 step_size = 0.4705 reward = -0.0000 fps = 6 mse_loss = 1.2620 
2022-07-08 09:35:33.197934 - gail/main.py:201 - [Discriminator] iter = 55000 loss = -1.0384 grad_norm = 3.6043 grad_penalty = 0.2384 regularization = 0.0000 true_logits = 0.1242 fake_logits = -1.1526 true_prob = 0.5349 fake_prob = 0.2675 
2022-07-08 09:36:16.654839 - gail/main.py:142 - [Evaluate] iter = 55000 episode={ returns = 396.4137 lengths = 135 } discounted_episode={ returns = 359.1257 lengths = 134 } 
2022-07-08 09:36:41.152389 - gail/main.py:174 - [TRPO] iter = 56000 dist_mean = 0.2668 dist_std = 0.6764 vf_loss = 0.3809 grad_norm = 0.6164 nat_grad_norm = 0.4197 cg_residual = 0.0164 step_size = 0.5125 reward = -0.0000 fps = 14 mse_loss = 1.2899 
2022-07-08 09:37:06.424283 - gail/main.py:174 - [TRPO] iter = 57000 dist_mean = 0.2946 dist_std = 0.6732 vf_loss = 0.3610 grad_norm = 0.9290 nat_grad_norm = 0.4262 cg_residual = 0.0167 step_size = 0.4593 reward = -0.0000 fps = 10 mse_loss = 1.5308 
2022-07-08 09:37:30.894604 - gail/main.py:174 - [TRPO] iter = 58000 dist_mean = 0.2372 dist_std = 0.6683 vf_loss = 0.2076 grad_norm = 1.0669 nat_grad_norm = 0.2910 cg_residual = 0.0328 step_size = 0.4353 reward = 0.0000 fps = 8 mse_loss = 1.4054 
2022-07-08 09:37:56.048626 - gail/main.py:174 - [TRPO] iter = 59000 dist_mean = 0.2172 dist_std = 0.6674 vf_loss = 0.2869 grad_norm = 0.5749 nat_grad_norm = 0.4563 cg_residual = 0.0187 step_size = 0.5088 reward = -0.0000 fps = 7 mse_loss = 1.5511 
2022-07-08 09:38:20.023340 - gail/main.py:174 - [TRPO] iter = 60000 dist_mean = 0.2388 dist_std = 0.6667 vf_loss = 0.3182 grad_norm = 0.6016 nat_grad_norm = 0.2856 cg_residual = 0.0099 step_size = 0.5681 reward = -0.0000 fps = 5 mse_loss = 1.4936 
2022-07-08 09:38:20.775900 - gail/main.py:201 - [Discriminator] iter = 60000 loss = -0.9528 grad_norm = 3.4734 grad_penalty = 0.2124 regularization = 0.0000 true_logits = 0.1531 fake_logits = -1.0121 true_prob = 0.5410 fake_prob = 0.3001 
2022-07-08 09:39:04.460586 - gail/main.py:142 - [Evaluate] iter = 60000 episode={ returns = 392.2479 lengths = 135 } discounted_episode={ returns = 382.1903 lengths = 140 } 
2022-07-08 09:39:29.172757 - gail/main.py:174 - [TRPO] iter = 61000 dist_mean = 0.2680 dist_std = 0.6571 vf_loss = 0.1754 grad_norm = 0.6547 nat_grad_norm = 0.3907 cg_residual = 0.0125 step_size = 0.5438 reward = -0.0000 fps = 14 mse_loss = 1.6730 
2022-07-08 09:39:54.786801 - gail/main.py:174 - [TRPO] iter = 62000 dist_mean = 0.2536 dist_std = 0.6505 vf_loss = 0.4094 grad_norm = 0.7286 nat_grad_norm = 0.3580 cg_residual = 0.0190 step_size = 0.4788 reward = -0.0000 fps = 10 mse_loss = 1.5713 
2022-07-08 09:40:20.822827 - gail/main.py:174 - [TRPO] iter = 63000 dist_mean = 0.2429 dist_std = 0.6531 vf_loss = 0.2746 grad_norm = 0.8377 nat_grad_norm = 0.3759 cg_residual = 0.0109 step_size = 0.5153 reward = -0.0000 fps = 8 mse_loss = 1.6926 
2022-07-08 09:40:45.448541 - gail/main.py:174 - [TRPO] iter = 64000 dist_mean = 0.2243 dist_std = 0.6552 vf_loss = 0.4089 grad_norm = 1.0576 nat_grad_norm = 0.2894 cg_residual = 0.0174 step_size = 0.4766 reward = -0.0000 fps = 6 mse_loss = 1.5435 
2022-07-08 09:41:08.697524 - gail/main.py:174 - [TRPO] iter = 65000 dist_mean = 0.1959 dist_std = 0.6523 vf_loss = 0.4015 grad_norm = 0.7647 nat_grad_norm = 0.3571 cg_residual = 0.0105 step_size = 0.4357 reward = -0.0000 fps = 5 mse_loss = 1.3887 
2022-07-08 09:41:09.433803 - gail/main.py:201 - [Discriminator] iter = 65000 loss = -0.7897 grad_norm = 3.5485 grad_penalty = 0.2271 regularization = 0.0000 true_logits = 0.1642 fake_logits = -0.8526 true_prob = 0.5439 fake_prob = 0.3357 
2022-07-08 09:41:58.091062 - gail/main.py:142 - [Evaluate] iter = 65000 episode={ returns = 515.5308 lengths = 158 } discounted_episode={ returns = 457.1497 lengths = 155 } 
2022-07-08 09:42:28.528024 - gail/main.py:174 - [TRPO] iter = 66000 dist_mean = 0.2158 dist_std = 0.6405 vf_loss = 0.9562 grad_norm = 0.6836 nat_grad_norm = 0.2952 cg_residual = 0.0076 step_size = 0.5658 reward = -0.0000 fps = 12 mse_loss = 1.2911 
2022-07-08 09:42:52.356473 - gail/main.py:174 - [TRPO] iter = 67000 dist_mean = 0.2273 dist_std = 0.6362 vf_loss = 0.4548 grad_norm = 0.4732 nat_grad_norm = 0.3273 cg_residual = 0.0079 step_size = 0.5918 reward = -0.0000 fps = 9 mse_loss = 1.5626 
2022-07-08 09:43:15.291630 - gail/main.py:174 - [TRPO] iter = 68000 dist_mean = 0.1866 dist_std = 0.6344 vf_loss = 1.2105 grad_norm = 1.2432 nat_grad_norm = 0.3999 cg_residual = 0.0289 step_size = 0.3746 reward = -0.0000 fps = 7 mse_loss = 1.4545 
2022-07-08 09:43:38.814738 - gail/main.py:174 - [TRPO] iter = 69000 dist_mean = 0.2020 dist_std = 0.6337 vf_loss = 0.8162 grad_norm = 0.3731 nat_grad_norm = 0.3020 cg_residual = 0.0088 step_size = 0.6585 reward = 0.0000 fps = 6 mse_loss = 1.4920 
2022-07-08 09:44:02.279818 - gail/main.py:174 - [TRPO] iter = 70000 dist_mean = 0.1698 dist_std = 0.6235 vf_loss = 0.5951 grad_norm = 0.6481 nat_grad_norm = 0.2858 cg_residual = 0.0118 step_size = 0.6211 reward = -0.0000 fps = 5 mse_loss = 1.4871 
2022-07-08 09:44:02.961043 - gail/main.py:201 - [Discriminator] iter = 70000 loss = -0.8747 grad_norm = 3.7693 grad_penalty = 0.2161 regularization = 0.0000 true_logits = 0.2275 fake_logits = -0.8633 true_prob = 0.5583 fake_prob = 0.3361 
2022-07-08 09:44:52.338249 - gail/main.py:142 - [Evaluate] iter = 70000 episode={ returns = 530.3091 lengths = 161 } discounted_episode={ returns = 480.1704 lengths = 161 } 
2022-07-08 09:45:15.663744 - gail/main.py:174 - [TRPO] iter = 71000 dist_mean = 0.2518 dist_std = 0.6233 vf_loss = 1.7859 grad_norm = 0.7846 nat_grad_norm = 0.3659 cg_residual = 0.0153 step_size = 0.4971 reward = -0.0000 fps = 13 mse_loss = 1.5907 
2022-07-08 09:45:39.393577 - gail/main.py:174 - [TRPO] iter = 72000 dist_mean = 0.2284 dist_std = 0.6180 vf_loss = 1.0741 grad_norm = 0.8811 nat_grad_norm = 0.3857 cg_residual = 0.0165 step_size = 0.4237 reward = -0.0000 fps = 10 mse_loss = 1.6074 
2022-07-08 09:46:02.764830 - gail/main.py:174 - [TRPO] iter = 73000 dist_mean = 0.1904 dist_std = 0.6184 vf_loss = 1.0293 grad_norm = 0.7808 nat_grad_norm = 0.3698 cg_residual = 0.0205 step_size = 0.5196 reward = 0.0000 fps = 8 mse_loss = 1.7902 
2022-07-08 09:46:26.410348 - gail/main.py:174 - [TRPO] iter = 74000 dist_mean = 0.1302 dist_std = 0.6190 vf_loss = 0.6780 grad_norm = 0.5940 nat_grad_norm = 0.3355 cg_residual = 0.0122 step_size = 0.5519 reward = -0.0000 fps = 6 mse_loss = 1.6435 
2022-07-08 09:46:50.675592 - gail/main.py:174 - [TRPO] iter = 75000 dist_mean = 0.1957 dist_std = 0.6159 vf_loss = 0.4423 grad_norm = 0.5200 nat_grad_norm = 0.3614 cg_residual = 0.0141 step_size = 0.5680 reward = -0.0000 fps = 5 mse_loss = 1.8368 
2022-07-08 09:46:51.458381 - gail/main.py:201 - [Discriminator] iter = 75000 loss = -0.9750 grad_norm = 3.4175 grad_penalty = 0.2223 regularization = 0.0000 true_logits = 0.2446 fake_logits = -0.9527 true_prob = 0.5614 fake_prob = 0.3210 
2022-07-08 09:47:45.768056 - gail/main.py:142 - [Evaluate] iter = 75000 episode={ returns = 592.7360 lengths = 176 } discounted_episode={ returns = 526.1679 lengths = 175 } 
2022-07-08 09:48:09.388711 - gail/main.py:174 - [TRPO] iter = 76000 dist_mean = 0.1706 dist_std = 0.6200 vf_loss = 0.7921 grad_norm = 0.6606 nat_grad_norm = 0.4371 cg_residual = 0.0533 step_size = 0.4576 reward = 0.0000 fps = 12 mse_loss = 2.0233 
2022-07-08 09:48:33.022435 - gail/main.py:174 - [TRPO] iter = 77000 dist_mean = 0.1607 dist_std = 0.6184 vf_loss = 0.3531 grad_norm = 0.6384 nat_grad_norm = 0.3114 cg_residual = 0.0096 step_size = 0.5815 reward = 0.0000 fps = 9 mse_loss = 1.8667 
2022-07-08 09:48:56.197089 - gail/main.py:174 - [TRPO] iter = 78000 dist_mean = 0.1755 dist_std = 0.6198 vf_loss = 1.1082 grad_norm = 0.8225 nat_grad_norm = 0.3934 cg_residual = 0.0137 step_size = 0.4686 reward = 0.0000 fps = 8 mse_loss = 2.0921 
2022-07-08 09:49:19.543596 - gail/main.py:174 - [TRPO] iter = 79000 dist_mean = 0.1128 dist_std = 0.6205 vf_loss = 0.3544 grad_norm = 0.5292 nat_grad_norm = 0.2727 cg_residual = 0.0143 step_size = 0.5932 reward = 0.0000 fps = 6 mse_loss = 2.2972 
2022-07-08 09:49:44.777946 - gail/main.py:174 - [TRPO] iter = 80000 dist_mean = 0.2042 dist_std = 0.6195 vf_loss = 0.7380 grad_norm = 0.4512 nat_grad_norm = 0.2323 cg_residual = 0.0106 step_size = 0.7216 reward = 0.0000 fps = 5 mse_loss = 1.7376 
2022-07-08 09:49:45.548839 - gail/main.py:201 - [Discriminator] iter = 80000 loss = -0.9242 grad_norm = 3.5417 grad_penalty = 0.2173 regularization = 0.0000 true_logits = 0.2386 fake_logits = -0.9029 true_prob = 0.5595 fake_prob = 0.3294 
2022-07-08 09:50:43.984972 - gail/main.py:142 - [Evaluate] iter = 80000 episode={ returns = 606.3843 lengths = 180 } discounted_episode={ returns = 545.7226 lengths = 181 } 
2022-07-08 09:51:06.322676 - gail/main.py:174 - [TRPO] iter = 81000 dist_mean = 0.1670 dist_std = 0.6217 vf_loss = 0.5135 grad_norm = 0.4847 nat_grad_norm = 0.2759 cg_residual = 0.0083 step_size = 0.6225 reward = -0.0000 fps = 12 mse_loss = 1.9001 
2022-07-08 09:51:29.292570 - gail/main.py:174 - [TRPO] iter = 82000 dist_mean = 0.1598 dist_std = 0.6160 vf_loss = 0.4844 grad_norm = 0.6480 nat_grad_norm = 0.3220 cg_residual = 0.0101 step_size = 0.6054 reward = 0.0000 fps = 9 mse_loss = 1.7989 
2022-07-08 09:51:52.714157 - gail/main.py:174 - [TRPO] iter = 83000 dist_mean = 0.1368 dist_std = 0.6140 vf_loss = 0.2602 grad_norm = 0.4419 nat_grad_norm = 0.2878 cg_residual = 0.0156 step_size = 0.6553 reward = 0.0000 fps = 7 mse_loss = 2.0075 
2022-07-08 09:52:23.562367 - gail/main.py:174 - [TRPO] iter = 84000 dist_mean = 0.1682 dist_std = 0.6144 vf_loss = 0.2409 grad_norm = 0.5275 nat_grad_norm = 0.2615 cg_residual = 0.0153 step_size = 0.6342 reward = 0.0000 fps = 6 mse_loss = 2.0500 
2022-07-08 09:52:46.178620 - gail/main.py:174 - [TRPO] iter = 85000 dist_mean = 0.3498 dist_std = 0.6138 vf_loss = 0.4670 grad_norm = 0.7836 nat_grad_norm = 0.2718 cg_residual = 0.0116 step_size = 0.5364 reward = 0.0000 fps = 5 mse_loss = 2.1565 
2022-07-08 09:52:46.855972 - gail/main.py:201 - [Discriminator] iter = 85000 loss = -1.0146 grad_norm = 3.3861 grad_penalty = 0.1987 regularization = 0.0000 true_logits = 0.1692 fake_logits = -1.0441 true_prob = 0.5458 fake_prob = 0.3108 
2022-07-08 09:53:47.776529 - gail/main.py:142 - [Evaluate] iter = 85000 episode={ returns = 642.9132 lengths = 193 } discounted_episode={ returns = 571.8689 lengths = 192 } 
2022-07-08 09:54:12.712606 - gail/main.py:174 - [TRPO] iter = 86000 dist_mean = 0.1211 dist_std = 0.6152 vf_loss = 0.4634 grad_norm = 0.8053 nat_grad_norm = 0.2973 cg_residual = 0.0162 step_size = 0.4836 reward = 0.0000 fps = 11 mse_loss = 2.3049 
2022-07-08 09:54:37.063476 - gail/main.py:174 - [TRPO] iter = 87000 dist_mean = 0.1341 dist_std = 0.6144 vf_loss = 0.2202 grad_norm = 0.4913 nat_grad_norm = 0.3395 cg_residual = 0.0196 step_size = 0.6013 reward = 0.0000 fps = 9 mse_loss = 2.3806 
2022-07-08 09:55:01.112008 - gail/main.py:174 - [TRPO] iter = 88000 dist_mean = 0.1771 dist_std = 0.6133 vf_loss = 1.0854 grad_norm = 0.7807 nat_grad_norm = 0.3839 cg_residual = 0.0324 step_size = 0.4378 reward = -0.0000 fps = 7 mse_loss = 2.6745 
2022-07-08 09:55:24.065670 - gail/main.py:174 - [TRPO] iter = 89000 dist_mean = 0.2055 dist_std = 0.6087 vf_loss = 0.1331 grad_norm = 0.6574 nat_grad_norm = 0.3458 cg_residual = 0.0183 step_size = 0.4961 reward = -0.0000 fps = 6 mse_loss = 2.7527 
2022-07-08 09:55:46.197858 - gail/main.py:174 - [TRPO] iter = 90000 dist_mean = 0.2252 dist_std = 0.6057 vf_loss = 0.9175 grad_norm = 1.0470 nat_grad_norm = 0.3823 cg_residual = 0.0242 step_size = 0.3914 reward = -0.0000 fps = 5 mse_loss = 2.8989 
2022-07-08 09:55:46.899864 - gail/main.py:201 - [Discriminator] iter = 90000 loss = -0.9164 grad_norm = 2.7552 grad_penalty = 0.1841 regularization = 0.0000 true_logits = 0.1874 fake_logits = -0.9131 true_prob = 0.5496 fake_prob = 0.3356 
2022-07-08 09:56:46.998763 - gail/main.py:142 - [Evaluate] iter = 90000 episode={ returns = 685.4149 lengths = 211 } discounted_episode={ returns = 605.6049 lengths = 210 } 
2022-07-08 09:57:09.644191 - gail/main.py:174 - [TRPO] iter = 91000 dist_mean = 0.1368 dist_std = 0.5998 vf_loss = 0.2232 grad_norm = 0.5688 nat_grad_norm = 0.3029 cg_residual = 0.0159 step_size = 0.5654 reward = 0.0000 fps = 12 mse_loss = 3.1119 
2022-07-08 09:57:32.853224 - gail/main.py:174 - [TRPO] iter = 92000 dist_mean = 0.1744 dist_std = 0.5969 vf_loss = 0.2805 grad_norm = 0.8325 nat_grad_norm = 0.2923 cg_residual = 0.0226 step_size = 0.4851 reward = 0.0000 fps = 9 mse_loss = 3.3532 
2022-07-08 09:57:55.384153 - gail/main.py:174 - [TRPO] iter = 93000 dist_mean = 0.1825 dist_std = 0.5895 vf_loss = 0.1488 grad_norm = 0.6732 nat_grad_norm = 0.3030 cg_residual = 0.0274 step_size = 0.5031 reward = 0.0000 fps = 7 mse_loss = 3.5367 
2022-07-08 09:58:18.247182 - gail/main.py:174 - [TRPO] iter = 94000 dist_mean = 0.2193 dist_std = 0.5856 vf_loss = 0.1219 grad_norm = 0.4764 nat_grad_norm = 0.3381 cg_residual = 0.0275 step_size = 0.5627 reward = 0.0000 fps = 6 mse_loss = 3.3459 
2022-07-08 09:58:42.175830 - gail/main.py:174 - [TRPO] iter = 95000 dist_mean = 0.1586 dist_std = 0.5800 vf_loss = 0.1246 grad_norm = 0.3911 nat_grad_norm = 0.2864 cg_residual = 0.0162 step_size = 0.6915 reward = 0.0000 fps = 5 mse_loss = 3.3344 
2022-07-08 09:58:42.960188 - gail/main.py:201 - [Discriminator] iter = 95000 loss = -0.7343 grad_norm = 2.6365 grad_penalty = 0.1834 regularization = 0.0000 true_logits = 0.1685 fake_logits = -0.7492 true_prob = 0.5453 fake_prob = 0.3645 
2022-07-08 09:59:52.437135 - gail/main.py:142 - [Evaluate] iter = 95000 episode={ returns = 793.1862 lengths = 236 } discounted_episode={ returns = 608.5683 lengths = 213 } 
2022-07-08 10:00:16.986734 - gail/main.py:174 - [TRPO] iter = 96000 dist_mean = 0.1359 dist_std = 0.5804 vf_loss = 0.1258 grad_norm = 0.7071 nat_grad_norm = 0.2521 cg_residual = 0.0215 step_size = 0.6326 reward = -0.0000 fps = 10 mse_loss = 3.7240 
2022-07-08 10:00:41.141949 - gail/main.py:174 - [TRPO] iter = 97000 dist_mean = 0.1989 dist_std = 0.5727 vf_loss = 0.2408 grad_norm = 0.4831 nat_grad_norm = 0.2755 cg_residual = 0.0169 step_size = 0.6054 reward = 0.0000 fps = 8 mse_loss = 3.8273 
2022-07-08 10:01:06.397961 - gail/main.py:174 - [TRPO] iter = 98000 dist_mean = 0.1872 dist_std = 0.5656 vf_loss = 0.1367 grad_norm = 0.6682 nat_grad_norm = 0.2786 cg_residual = 0.0412 step_size = 0.5231 reward = 0.0000 fps = 6 mse_loss = 3.9773 
2022-07-08 10:01:31.491742 - gail/main.py:174 - [TRPO] iter = 99000 dist_mean = 0.3145 dist_std = 0.5627 vf_loss = 0.3169 grad_norm = 0.3953 nat_grad_norm = 0.3098 cg_residual = 0.0228 step_size = 0.6429 reward = 0.0000 fps = 5 mse_loss = 3.9222 
2022-07-08 10:02:01.010212 - gail/main.py:174 - [TRPO] iter = 100000 dist_mean = 0.3123 dist_std = 0.5605 vf_loss = 0.2987 grad_norm = 0.4590 nat_grad_norm = 0.3709 cg_residual = 0.0450 step_size = 0.5054 reward = 0.0000 fps = 5 mse_loss = 4.0464 
2022-07-08 10:02:02.454675 - gail/main.py:201 - [Discriminator] iter = 100000 loss = -0.7392 grad_norm = 2.8810 grad_penalty = 0.1822 regularization = 0.0000 true_logits = 0.1342 fake_logits = -0.7871 true_prob = 0.5366 fake_prob = 0.3523 
2022-07-08 10:03:24.078728 - gail/main.py:142 - [Evaluate] iter = 100000 episode={ returns = 739.7403 lengths = 226 } discounted_episode={ returns = 615.7264 lengths = 215 } 
2022-07-08 10:03:51.643112 - gail/main.py:174 - [TRPO] iter = 101000 dist_mean = 0.2683 dist_std = 0.5617 vf_loss = 0.0824 grad_norm = 0.6892 nat_grad_norm = 0.2827 cg_residual = 0.0270 step_size = 0.5411 reward = 0.0000 fps = 9 mse_loss = 4.4756 
2022-07-08 10:04:19.251662 - gail/main.py:174 - [TRPO] iter = 102000 dist_mean = 0.2834 dist_std = 0.5568 vf_loss = 0.1451 grad_norm = 0.4862 nat_grad_norm = 0.2146 cg_residual = 0.0244 step_size = 0.6971 reward = -0.0000 fps = 7 mse_loss = 4.2480 
2022-07-08 10:04:46.012969 - gail/main.py:174 - [TRPO] iter = 103000 dist_mean = 0.1903 dist_std = 0.5587 vf_loss = 0.1718 grad_norm = 0.6120 nat_grad_norm = 0.2349 cg_residual = 0.0264 step_size = 0.6310 reward = -0.0000 fps = 6 mse_loss = 4.7658 
2022-07-08 10:05:12.999334 - gail/main.py:174 - [TRPO] iter = 104000 dist_mean = 0.2327 dist_std = 0.5496 vf_loss = 0.3675 grad_norm = 0.8474 nat_grad_norm = 0.3637 cg_residual = 0.0903 step_size = 0.4013 reward = 0.0000 fps = 5 mse_loss = 4.7110 
2022-07-08 10:05:40.127025 - gail/main.py:174 - [TRPO] iter = 105000 dist_mean = 0.2350 dist_std = 0.5498 vf_loss = 0.1053 grad_norm = 0.7795 nat_grad_norm = 0.2988 cg_residual = 0.0184 step_size = 0.4631 reward = 0.0000 fps = 4 mse_loss = 4.7865 
2022-07-08 10:05:40.848870 - gail/main.py:201 - [Discriminator] iter = 105000 loss = -0.7130 grad_norm = 3.0826 grad_penalty = 0.1673 regularization = 0.0000 true_logits = 0.1020 fake_logits = -0.7782 true_prob = 0.5284 fake_prob = 0.3517 
2022-07-08 10:07:13.825780 - gail/main.py:142 - [Evaluate] iter = 105000 episode={ returns = 924.2007 lengths = 268 } discounted_episode={ returns = 795.7664 lengths = 268 } 
2022-07-08 10:07:39.533632 - gail/main.py:174 - [TRPO] iter = 106000 dist_mean = 0.1486 dist_std = 0.5488 vf_loss = 0.1147 grad_norm = 0.6449 nat_grad_norm = 0.2300 cg_residual = 0.0320 step_size = 0.5532 reward = 0.0000 fps = 8 mse_loss = 4.6065 
2022-07-08 10:08:05.075365 - gail/main.py:174 - [TRPO] iter = 107000 dist_mean = 0.2748 dist_std = 0.5500 vf_loss = 0.2161 grad_norm = 0.4615 nat_grad_norm = 0.3021 cg_residual = 0.0430 step_size = 0.5695 reward = -0.0000 fps = 6 mse_loss = 4.5854 
2022-07-08 10:08:31.806559 - gail/main.py:174 - [TRPO] iter = 108000 dist_mean = 0.1962 dist_std = 0.5459 vf_loss = 0.1095 grad_norm = 0.7259 nat_grad_norm = 0.2677 cg_residual = 0.0329 step_size = 0.5702 reward = -0.0000 fps = 5 mse_loss = 4.9248 
2022-07-08 10:08:58.783078 - gail/main.py:174 - [TRPO] iter = 109000 dist_mean = 0.2645 dist_std = 0.5486 vf_loss = 0.1235 grad_norm = 0.8599 nat_grad_norm = 0.2649 cg_residual = 0.0306 step_size = 0.5709 reward = -0.0000 fps = 5 mse_loss = 4.3821 
2022-07-08 10:09:26.867330 - gail/main.py:174 - [TRPO] iter = 110000 dist_mean = 0.1471 dist_std = 0.5515 vf_loss = 0.0860 grad_norm = 0.7058 nat_grad_norm = 0.2244 cg_residual = 0.0246 step_size = 0.6361 reward = -0.0000 fps = 4 mse_loss = 4.8661 
2022-07-08 10:09:27.597777 - gail/main.py:201 - [Discriminator] iter = 110000 loss = -0.6923 grad_norm = 3.2211 grad_penalty = 0.1792 regularization = 0.0000 true_logits = 0.0717 fake_logits = -0.7998 true_prob = 0.5214 fake_prob = 0.3494 
2022-07-08 10:11:09.613970 - gail/main.py:142 - [Evaluate] iter = 110000 episode={ returns = 986.8518 lengths = 290 } discounted_episode={ returns = 840.4546 lengths = 293 } 
2022-07-08 10:11:38.043985 - gail/main.py:174 - [TRPO] iter = 111000 dist_mean = 0.2460 dist_std = 0.5465 vf_loss = 0.0797 grad_norm = 0.8799 nat_grad_norm = 0.2798 cg_residual = 0.0272 step_size = 0.5083 reward = 0.0000 fps = 7 mse_loss = 4.5868 
2022-07-08 10:12:13.126882 - gail/main.py:174 - [TRPO] iter = 112000 dist_mean = 0.2531 dist_std = 0.5455 vf_loss = 0.0723 grad_norm = 0.6710 nat_grad_norm = 0.2658 cg_residual = 0.0403 step_size = 0.5241 reward = -0.0000 fps = 6 mse_loss = 4.5019 
2022-07-08 10:12:39.604295 - gail/main.py:174 - [TRPO] iter = 113000 dist_mean = 0.2000 dist_std = 0.5467 vf_loss = 0.1147 grad_norm = 0.4209 nat_grad_norm = 0.3510 cg_residual = 0.0317 step_size = 0.5504 reward = -0.0000 fps = 5 mse_loss = 4.7589 
2022-07-08 10:13:06.269994 - gail/main.py:174 - [TRPO] iter = 114000 dist_mean = 0.2133 dist_std = 0.5408 vf_loss = 0.1570 grad_norm = 1.0021 nat_grad_norm = 0.5262 cg_residual = 0.1000 step_size = 0.3528 reward = -0.0000 fps = 4 mse_loss = 4.5801 
2022-07-08 10:13:32.220717 - gail/main.py:174 - [TRPO] iter = 115000 dist_mean = 0.1200 dist_std = 0.5406 vf_loss = 0.0476 grad_norm = 1.0090 nat_grad_norm = 0.2276 cg_residual = 0.0277 step_size = 0.4941 reward = -0.0000 fps = 4 mse_loss = 3.9366 
2022-07-08 10:13:33.018276 - gail/main.py:201 - [Discriminator] iter = 115000 loss = -0.6472 grad_norm = 2.5374 grad_penalty = 0.1668 regularization = 0.0000 true_logits = 0.0565 fake_logits = -0.7575 true_prob = 0.5188 fake_prob = 0.3571 
2022-07-08 10:15:05.010064 - gail/main.py:142 - [Evaluate] iter = 115000 episode={ returns = 927.5832 lengths = 268 } discounted_episode={ returns = 778.8448 lengths = 268 } 
2022-07-08 10:15:32.780984 - gail/main.py:174 - [TRPO] iter = 116000 dist_mean = 0.1884 dist_std = 0.5442 vf_loss = 0.1745 grad_norm = 0.4416 nat_grad_norm = 0.2697 cg_residual = 0.0378 step_size = 0.6178 reward = -0.0000 fps = 8 mse_loss = 4.3127 
2022-07-08 10:15:59.710276 - gail/main.py:174 - [TRPO] iter = 117000 dist_mean = 0.2013 dist_std = 0.5455 vf_loss = 0.0559 grad_norm = 0.7094 nat_grad_norm = 0.3757 cg_residual = 0.0487 step_size = 0.4543 reward = 0.0000 fps = 6 mse_loss = 3.8578 
2022-07-08 10:16:25.913551 - gail/main.py:174 - [TRPO] iter = 118000 dist_mean = 0.1908 dist_std = 0.5389 vf_loss = 0.1492 grad_norm = 0.6625 nat_grad_norm = 0.2943 cg_residual = 0.0319 step_size = 0.5276 reward = 0.0000 fps = 5 mse_loss = 4.3739 
2022-07-08 10:16:51.920717 - gail/main.py:174 - [TRPO] iter = 119000 dist_mean = 0.2076 dist_std = 0.5355 vf_loss = 0.1946 grad_norm = 0.5159 nat_grad_norm = 0.3312 cg_residual = 0.0364 step_size = 0.5220 reward = 0.0000 fps = 5 mse_loss = 4.2085 
2022-07-08 10:17:18.760716 - gail/main.py:174 - [TRPO] iter = 120000 dist_mean = 0.1496 dist_std = 0.5412 vf_loss = 0.2213 grad_norm = 0.5531 nat_grad_norm = 0.2256 cg_residual = 0.0428 step_size = 0.5980 reward = -0.0000 fps = 4 mse_loss = 4.3653 
2022-07-08 10:17:19.478464 - gail/main.py:201 - [Discriminator] iter = 120000 loss = -0.7529 grad_norm = 2.9447 grad_penalty = 0.1545 regularization = 0.0000 true_logits = 0.0149 fake_logits = -0.8925 true_prob = 0.5079 fake_prob = 0.3324 
2022-07-08 10:18:52.255148 - gail/main.py:142 - [Evaluate] iter = 120000 episode={ returns = 903.5499 lengths = 272 } discounted_episode={ returns = 773.5100 lengths = 274 } 
2022-07-08 10:19:17.474170 - gail/main.py:174 - [TRPO] iter = 121000 dist_mean = 0.2288 dist_std = 0.5365 vf_loss = 0.1517 grad_norm = 0.5913 nat_grad_norm = 0.2466 cg_residual = 0.0189 step_size = 0.6043 reward = 0.0000 fps = 8 mse_loss = 4.6887 
2022-07-08 10:19:44.195154 - gail/main.py:174 - [TRPO] iter = 122000 dist_mean = 0.2411 dist_std = 0.5284 vf_loss = 0.0926 grad_norm = 0.7287 nat_grad_norm = 0.2420 cg_residual = 0.0114 step_size = 0.6251 reward = -0.0000 fps = 6 mse_loss = 4.6802 
2022-07-08 10:20:11.059225 - gail/main.py:174 - [TRPO] iter = 123000 dist_mean = 0.2211 dist_std = 0.5317 vf_loss = 0.4605 grad_norm = 0.7379 nat_grad_norm = 0.2237 cg_residual = 0.0134 step_size = 0.6518 reward = 0.0000 fps = 5 mse_loss = 4.6504 
2022-07-08 10:20:37.943466 - gail/main.py:174 - [TRPO] iter = 124000 dist_mean = 0.1642 dist_std = 0.5343 vf_loss = 0.1218 grad_norm = 0.6115 nat_grad_norm = 0.2697 cg_residual = 0.0312 step_size = 0.5118 reward = 0.0000 fps = 5 mse_loss = 5.0297 
2022-07-08 10:21:05.426070 - gail/main.py:174 - [TRPO] iter = 125000 dist_mean = 0.1096 dist_std = 0.5221 vf_loss = 0.0879 grad_norm = 0.9248 nat_grad_norm = 0.2457 cg_residual = 0.0354 step_size = 0.5609 reward = 0.0000 fps = 4 mse_loss = 5.1224 
2022-07-08 10:21:06.252049 - gail/main.py:201 - [Discriminator] iter = 125000 loss = -0.7229 grad_norm = 2.4714 grad_penalty = 0.1364 regularization = 0.0000 true_logits = -0.0034 fake_logits = -0.8628 true_prob = 0.5036 fake_prob = 0.3366 
2022-07-08 10:22:35.396659 - gail/main.py:142 - [Evaluate] iter = 125000 episode={ returns = 897.8472 lengths = 258 } discounted_episode={ returns = 728.6023 lengths = 247 } 
2022-07-08 10:23:02.250322 - gail/main.py:174 - [TRPO] iter = 126000 dist_mean = 0.2916 dist_std = 0.5312 vf_loss = 0.2488 grad_norm = 0.8816 nat_grad_norm = 0.2663 cg_residual = 0.0515 step_size = 0.4977 reward = 0.0000 fps = 8 mse_loss = 5.4634 
2022-07-08 10:23:28.572649 - gail/main.py:174 - [TRPO] iter = 127000 dist_mean = 0.1561 dist_std = 0.5246 vf_loss = 0.2720 grad_norm = 0.5917 nat_grad_norm = 0.2045 cg_residual = 0.0289 step_size = 0.6238 reward = 0.0000 fps = 7 mse_loss = 6.2383 
2022-07-08 10:23:54.803989 - gail/main.py:174 - [TRPO] iter = 128000 dist_mean = 0.0783 dist_std = 0.5187 vf_loss = 0.0931 grad_norm = 0.6696 nat_grad_norm = 0.2177 cg_residual = 0.0211 step_size = 0.6643 reward = 0.0000 fps = 5 mse_loss = 6.7128 
2022-07-08 10:24:21.813150 - gail/main.py:174 - [TRPO] iter = 129000 dist_mean = 0.1709 dist_std = 0.5257 vf_loss = 0.8291 grad_norm = 0.8308 nat_grad_norm = 0.2723 cg_residual = 0.0413 step_size = 0.4997 reward = 0.0000 fps = 5 mse_loss = 6.6929 
2022-07-08 10:24:49.127709 - gail/main.py:174 - [TRPO] iter = 130000 dist_mean = 0.1057 dist_std = 0.5228 vf_loss = 0.0743 grad_norm = 0.7878 nat_grad_norm = 0.2294 cg_residual = 0.0369 step_size = 0.5968 reward = -0.0000 fps = 4 mse_loss = 7.1515 
2022-07-08 10:24:49.998254 - gail/main.py:201 - [Discriminator] iter = 130000 loss = -0.6378 grad_norm = 2.2359 grad_penalty = 0.1414 regularization = 0.0000 true_logits = -0.0277 fake_logits = -0.8069 true_prob = 0.4992 fake_prob = 0.3472 
2022-07-08 10:26:21.525695 - gail/main.py:142 - [Evaluate] iter = 130000 episode={ returns = 920.8492 lengths = 260 } discounted_episode={ returns = 792.2397 lengths = 263 } 
2022-07-08 10:26:49.701173 - gail/main.py:174 - [TRPO] iter = 131000 dist_mean = 0.1803 dist_std = 0.5172 vf_loss = 0.1104 grad_norm = 0.7440 nat_grad_norm = 0.2725 cg_residual = 0.0491 step_size = 0.5568 reward = 0.0000 fps = 8 mse_loss = 6.8007 
2022-07-08 10:27:18.005414 - gail/main.py:174 - [TRPO] iter = 132000 dist_mean = 0.1231 dist_std = 0.5146 vf_loss = 0.0772 grad_norm = 0.6568 nat_grad_norm = 0.2710 cg_residual = 0.0568 step_size = 0.5498 reward = 0.0000 fps = 6 mse_loss = 6.8679 
2022-07-08 10:27:45.144492 - gail/main.py:174 - [TRPO] iter = 133000 dist_mean = 0.1741 dist_std = 0.5081 vf_loss = 0.1516 grad_norm = 1.0395 nat_grad_norm = 0.2916 cg_residual = 0.0306 step_size = 0.4863 reward = 0.0000 fps = 5 mse_loss = 6.4543 
2022-07-08 10:28:12.934398 - gail/main.py:174 - [TRPO] iter = 134000 dist_mean = 0.1215 dist_std = 0.5025 vf_loss = 0.0869 grad_norm = 0.9689 nat_grad_norm = 0.2676 cg_residual = 0.0699 step_size = 0.4820 reward = 0.0000 fps = 4 mse_loss = 6.5404 
2022-07-08 10:28:40.388129 - gail/main.py:174 - [TRPO] iter = 135000 dist_mean = 0.1754 dist_std = 0.5014 vf_loss = 0.1241 grad_norm = 0.5894 nat_grad_norm = 0.1886 cg_residual = 0.0192 step_size = 0.6340 reward = 0.0000 fps = 4 mse_loss = 6.7048 
2022-07-08 10:28:41.172184 - gail/main.py:201 - [Discriminator] iter = 135000 loss = -0.7553 grad_norm = 2.4079 grad_penalty = 0.1384 regularization = 0.0000 true_logits = 0.0026 fake_logits = -0.8911 true_prob = 0.5059 fake_prob = 0.3259 
2022-07-08 10:30:22.033461 - gail/main.py:142 - [Evaluate] iter = 135000 episode={ returns = 958.8441 lengths = 273 } discounted_episode={ returns = 825.1908 lengths = 278 } 
2022-07-08 10:30:50.773044 - gail/main.py:174 - [TRPO] iter = 136000 dist_mean = 0.1162 dist_std = 0.5034 vf_loss = 0.0971 grad_norm = 0.5599 nat_grad_norm = 0.2572 cg_residual = 0.0361 step_size = 0.5617 reward = 0.0000 fps = 7 mse_loss = 6.4441 
2022-07-08 10:31:17.938391 - gail/main.py:174 - [TRPO] iter = 137000 dist_mean = 0.1472 dist_std = 0.4956 vf_loss = 0.0768 grad_norm = 0.7732 nat_grad_norm = 0.3219 cg_residual = 0.0410 step_size = 0.4837 reward = -0.0000 fps = 6 mse_loss = 5.8448 
2022-07-08 10:31:52.420305 - gail/main.py:174 - [TRPO] iter = 138000 dist_mean = 0.0883 dist_std = 0.4891 vf_loss = 0.1340 grad_norm = 0.8408 nat_grad_norm = 0.2324 cg_residual = 0.0660 step_size = 0.5017 reward = -0.0000 fps = 5 mse_loss = 5.6633 
