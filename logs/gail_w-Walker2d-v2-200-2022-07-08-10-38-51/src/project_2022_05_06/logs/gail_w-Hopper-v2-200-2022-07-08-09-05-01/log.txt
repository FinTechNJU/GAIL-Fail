2022-07-08 09:05:02.437698 - utils/flags.py:257 - log_dir = logs/gail_w-Hopper-v2-200-2022-07-08-09-05-01
2022-07-08 09:05:22.654702 - gail/utils/replay_buffer.py:86 - Load dataset from /content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/Hopper-v2
2022-07-08 09:05:41.841041 - gail/main.py:80 - Expert Reward 3582.436530
2022-07-08 09:05:42.822079 - gail/main.py:84 - Original dataset size 3000
2022-07-08 09:05:42.931718 - gail/main.py:86 - Subsampled dataset size 3000
2022-07-08 09:05:42.933576 - gail/main.py:87 - np random: 864 random : 786
2022-07-08 09:05:42.945075 - gail/main.py:91 - Sampled obs: 0.4652, acs: 0.0749
2022-07-08 09:05:45.446559 - lunzi/nn/flat_param.py:17 - Enabling flattening... ['GaussianMLPPolicy_1/log_std:0', 'GaussianMLPPolicy_1/Variable:0', 'GaussianMLPPolicy_1/Variable_1:0', 'GaussianMLPPolicy_1/Variable_2:0', 'GaussianMLPPolicy_1/Variable_3:0', 'GaussianMLPPolicy_1/Variable_4:0', 'GaussianMLPPolicy_1/Variable_5:0']
2022-07-08 09:06:07.475884 - gail/discriminator/discriminator.py:30 - Use predefined normalization.
2022-07-08 09:06:07.493056 - gail/discriminator/discriminator.py:33 - Normalizer loc:[[ 1.3966653  -0.06165453 -0.19472611 -0.4584401   0.18350822  2.5732448
   0.00400542 -0.00549955 -0.04755233 -0.02386179  0.00759995]] 
 scale:[[0.16755195 0.05883223 0.15990146 0.34682125 0.5992658  0.6461788
  1.5187451  0.8811966  2.0685835  3.6282625  5.862049  ]]
2022-07-08 09:06:18.365465 - gail/discriminator/discriminator.py:67 - Discriminator uses Wasserstein distance.
2022-07-08 09:06:18.373369 - gail/discriminator/discriminator.py:68 - [<tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable:0' shape=(14, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_1:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_2:0' shape=(100, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_3:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_4:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_5:0' shape=(1,) dtype=float32_ref>]
2022-07-08 09:06:18.375544 - gail/discriminator/discriminator.py:69 - Use gradient penalty regularization (coef = 10.000000)
2022-07-08 09:06:20.903602 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions_mean]
2022-07-08 09:07:00.966891 - gail/main.py:142 - [Evaluate] iter = 0 episode={ returns = 151.7944 lengths = 165 } discounted_episode={ returns = 145.6909 lengths = 173 } 
2022-07-08 09:07:00.976298 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions]
2022-07-08 09:07:23.545163 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [fake_states fake_actions] => [scaled_neural_reward]
2022-07-08 09:07:24.239929 - lunzi/nn/module.py:71 - [MLPVFunction] is making TensorFlow callables, key = [states] => [values]
2022-07-08 09:07:25.463853 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [] => [sync_old]
2022-07-08 09:07:26.044142 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss flat_grad dist_std mean_kl dist_mean]
2022-07-08 09:07:29.767877 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states tangents actions] => [hessian_vec_prod]
2022-07-08 09:07:36.392977 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [] => [get_flat]
2022-07-08 09:07:37.030690 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [feed_flat] => [set_flat]
2022-07-08 09:07:37.671801 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss mean_kl]
2022-07-08 09:07:38.856183 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [train_vf vf_loss]
2022-07-08 09:07:40.726420 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [vf_loss]
2022-07-08 09:07:41.392117 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states actions_] => [mse_loss]
2022-07-08 09:07:42.123968 - gail/main.py:174 - [TRPO] iter = 1000 dist_mean = -0.0000 dist_std = 1.0000 vf_loss = 0.2514 grad_norm = 0.3047 nat_grad_norm = 0.3393 cg_residual = 0.0000 step_size = 0.4765 reward = 0.0000 fps = 12 mse_loss = 0.3253 
2022-07-08 09:07:58.589763 - gail/main.py:174 - [TRPO] iter = 2000 dist_mean = 0.0570 dist_std = 1.0017 vf_loss = 0.3318 grad_norm = 0.3617 nat_grad_norm = 0.4168 cg_residual = 0.0000 step_size = 0.4072 reward = 0.0000 fps = 10 mse_loss = 0.3659 
2022-07-08 09:08:15.537879 - gail/main.py:174 - [TRPO] iter = 3000 dist_mean = 0.1010 dist_std = 0.9916 vf_loss = 0.2525 grad_norm = 0.3418 nat_grad_norm = 0.3390 cg_residual = 0.0000 step_size = 0.4626 reward = 0.0000 fps = 8 mse_loss = 0.3793 
2022-07-08 09:08:33.299011 - gail/main.py:174 - [TRPO] iter = 4000 dist_mean = 0.1515 dist_std = 0.9764 vf_loss = 0.3282 grad_norm = 0.2820 nat_grad_norm = 0.3051 cg_residual = 0.0000 step_size = 0.5470 reward = -0.0000 fps = 7 mse_loss = 0.3545 
2022-07-08 09:08:48.760975 - gail/main.py:174 - [TRPO] iter = 5000 dist_mean = 0.1659 dist_std = 0.9587 vf_loss = 0.3746 grad_norm = 0.2795 nat_grad_norm = 0.3719 cg_residual = 0.0000 step_size = 0.6178 reward = 0.0000 fps = 6 mse_loss = 0.4274 
2022-07-08 09:08:48.762665 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [true_states true_actions fake_states fake_actions true_masks] => [train loss true_logits fake_logits true_prob fake_prob grad_norm grad_penalty regularization]
2022-07-08 09:08:55.087039 - gail/main.py:201 - [Discriminator] iter = 5000 loss = 1.5168 grad_norm = 13.1276 grad_penalty = 1.5373 regularization = 0.0000 true_logits = -0.2267 fake_logits = -0.2472 true_prob = 0.4439 fake_prob = 0.4387 
2022-07-08 09:09:27.805607 - gail/main.py:142 - [Evaluate] iter = 5000 episode={ returns = 212.3677 lengths = 110 } discounted_episode={ returns = 202.3702 lengths = 111 } 
2022-07-08 09:09:49.859975 - gail/main.py:174 - [TRPO] iter = 6000 dist_mean = 0.2075 dist_std = 0.9529 vf_loss = 0.3781 grad_norm = 0.3119 nat_grad_norm = 0.3829 cg_residual = 0.0000 step_size = 0.5587 reward = 0.0000 fps = 18 mse_loss = 0.4183 
2022-07-08 09:10:10.584196 - gail/main.py:174 - [TRPO] iter = 7000 dist_mean = 0.1994 dist_std = 0.9398 vf_loss = 0.4616 grad_norm = 0.3044 nat_grad_norm = 0.4053 cg_residual = 0.0000 step_size = 0.5058 reward = 0.0000 fps = 13 mse_loss = 0.4898 
2022-07-08 09:10:36.648343 - gail/main.py:174 - [TRPO] iter = 8000 dist_mean = 0.2148 dist_std = 0.9394 vf_loss = 0.3368 grad_norm = 0.3283 nat_grad_norm = 0.2875 cg_residual = 0.0000 step_size = 0.6728 reward = -0.0000 fps = 9 mse_loss = 0.4930 
2022-07-08 09:11:04.934221 - gail/main.py:174 - [TRPO] iter = 9000 dist_mean = 0.2162 dist_std = 0.9513 vf_loss = 0.4786 grad_norm = 0.3392 nat_grad_norm = 0.3608 cg_residual = 0.0000 step_size = 0.5537 reward = -0.0000 fps = 7 mse_loss = 0.5917 
2022-07-08 09:11:36.556307 - gail/main.py:174 - [TRPO] iter = 10000 dist_mean = 0.2444 dist_std = 0.9428 vf_loss = 0.5203 grad_norm = 0.3483 nat_grad_norm = 0.3780 cg_residual = 0.0000 step_size = 0.5376 reward = -0.0000 fps = 6 mse_loss = 0.5879 
2022-07-08 09:11:37.284201 - gail/main.py:201 - [Discriminator] iter = 10000 loss = 1.1647 grad_norm = 11.8083 grad_penalty = 1.2152 regularization = 0.0000 true_logits = -0.2148 fake_logits = -0.2652 true_prob = 0.4468 fake_prob = 0.4343 
2022-07-08 09:12:15.039050 - gail/main.py:142 - [Evaluate] iter = 10000 episode={ returns = 212.8109 lengths = 102 } discounted_episode={ returns = 200.6702 lengths = 102 } 
2022-07-08 09:12:45.245023 - gail/main.py:174 - [TRPO] iter = 11000 dist_mean = 0.2034 dist_std = 0.9360 vf_loss = 0.3714 grad_norm = 0.4180 nat_grad_norm = 0.3630 cg_residual = 0.0000 step_size = 0.5608 reward = 0.0000 fps = 14 mse_loss = 0.5669 
2022-07-08 09:13:15.147606 - gail/main.py:174 - [TRPO] iter = 12000 dist_mean = 0.2222 dist_std = 0.9287 vf_loss = 0.3097 grad_norm = 0.3477 nat_grad_norm = 0.3733 cg_residual = 0.0001 step_size = 0.5640 reward = -0.0000 fps = 10 mse_loss = 0.6244 
2022-07-08 09:13:43.950934 - gail/main.py:174 - [TRPO] iter = 13000 dist_mean = 0.2226 dist_std = 0.9231 vf_loss = 0.3194 grad_norm = 0.3439 nat_grad_norm = 0.4140 cg_residual = 0.0001 step_size = 0.4991 reward = -0.0000 fps = 7 mse_loss = 0.6310 
2022-07-08 09:14:11.522022 - gail/main.py:174 - [TRPO] iter = 14000 dist_mean = 0.2330 dist_std = 0.9168 vf_loss = 0.1945 grad_norm = 0.3866 nat_grad_norm = 0.4637 cg_residual = 0.0001 step_size = 0.4829 reward = -0.0000 fps = 6 mse_loss = 0.6749 
2022-07-08 09:14:40.554032 - gail/main.py:174 - [TRPO] iter = 15000 dist_mean = 0.2551 dist_std = 0.8989 vf_loss = 0.1861 grad_norm = 0.3426 nat_grad_norm = 0.3643 cg_residual = 0.0002 step_size = 0.6016 reward = -0.0000 fps = 5 mse_loss = 0.6948 
2022-07-08 09:14:41.350134 - gail/main.py:201 - [Discriminator] iter = 15000 loss = 0.6386 grad_norm = 8.3017 grad_penalty = 0.7459 regularization = 0.0000 true_logits = -0.1843 fake_logits = -0.2916 true_prob = 0.4543 fake_prob = 0.4279 
2022-07-08 09:15:11.167501 - gail/main.py:142 - [Evaluate] iter = 15000 episode={ returns = 155.8603 lengths = 75 } discounted_episode={ returns = 148.5311 lengths = 75 } 
2022-07-08 09:15:40.058725 - gail/main.py:174 - [TRPO] iter = 16000 dist_mean = 0.2386 dist_std = 0.8888 vf_loss = 0.2245 grad_norm = 0.4049 nat_grad_norm = 0.3847 cg_residual = 0.0001 step_size = 0.5365 reward = -0.0000 fps = 17 mse_loss = 0.6923 
2022-07-08 09:16:08.603699 - gail/main.py:174 - [TRPO] iter = 17000 dist_mean = 0.2551 dist_std = 0.8834 vf_loss = 0.3139 grad_norm = 0.3426 nat_grad_norm = 0.4557 cg_residual = 0.0002 step_size = 0.5624 reward = -0.0000 fps = 11 mse_loss = 0.7085 
2022-07-08 09:16:36.145978 - gail/main.py:174 - [TRPO] iter = 18000 dist_mean = 0.2783 dist_std = 0.8784 vf_loss = 0.1582 grad_norm = 0.3786 nat_grad_norm = 0.3808 cg_residual = 0.0002 step_size = 0.5186 reward = -0.0000 fps = 8 mse_loss = 0.7464 
2022-07-08 09:17:05.441068 - gail/main.py:174 - [TRPO] iter = 19000 dist_mean = 0.2793 dist_std = 0.8624 vf_loss = 0.1068 grad_norm = 0.4843 nat_grad_norm = 0.3254 cg_residual = 0.0001 step_size = 0.4530 reward = -0.0000 fps = 6 mse_loss = 0.8593 
2022-07-08 09:17:33.751370 - gail/main.py:174 - [TRPO] iter = 20000 dist_mean = 0.2635 dist_std = 0.8452 vf_loss = 0.1228 grad_norm = 0.5489 nat_grad_norm = 0.3241 cg_residual = 0.0002 step_size = 0.5241 reward = 0.0000 fps = 5 mse_loss = 0.8033 
2022-07-08 09:17:34.449464 - gail/main.py:201 - [Discriminator] iter = 20000 loss = 0.3868 grad_norm = 5.3833 grad_penalty = 0.5608 regularization = 0.0000 true_logits = -0.1438 fake_logits = -0.3178 true_prob = 0.4644 fake_prob = 0.4216 
2022-07-08 09:18:00.294181 - gail/main.py:142 - [Evaluate] iter = 20000 episode={ returns = 140.3872 lengths = 69 } discounted_episode={ returns = 136.0499 lengths = 69 } 
2022-07-08 09:18:26.794957 - gail/main.py:174 - [TRPO] iter = 21000 dist_mean = 0.2476 dist_std = 0.8274 vf_loss = 0.0758 grad_norm = 0.4850 nat_grad_norm = 0.2344 cg_residual = 0.0001 step_size = 0.6442 reward = -0.0000 fps = 19 mse_loss = 0.8782 
2022-07-08 09:18:54.792121 - gail/main.py:174 - [TRPO] iter = 22000 dist_mean = 0.2456 dist_std = 0.8096 vf_loss = 0.0813 grad_norm = 0.4105 nat_grad_norm = 0.2989 cg_residual = 0.0002 step_size = 0.6027 reward = 0.0000 fps = 12 mse_loss = 0.8778 
2022-07-08 09:19:23.523154 - gail/main.py:174 - [TRPO] iter = 23000 dist_mean = 0.2251 dist_std = 0.7965 vf_loss = 0.0423 grad_norm = 0.5567 nat_grad_norm = 0.3364 cg_residual = 0.0007 step_size = 0.4910 reward = -0.0000 fps = 9 mse_loss = 1.0011 
2022-07-08 09:19:52.033497 - gail/main.py:174 - [TRPO] iter = 24000 dist_mean = 0.2032 dist_std = 0.7772 vf_loss = 0.0528 grad_norm = 0.5639 nat_grad_norm = 0.3291 cg_residual = 0.0009 step_size = 0.5261 reward = 0.0000 fps = 7 mse_loss = 0.9172 
2022-07-08 09:20:20.874338 - gail/main.py:174 - [TRPO] iter = 25000 dist_mean = 0.1975 dist_std = 0.7621 vf_loss = 0.0547 grad_norm = 0.7780 nat_grad_norm = 0.3132 cg_residual = 0.0006 step_size = 0.4485 reward = -0.0000 fps = 6 mse_loss = 0.9994 
2022-07-08 09:20:21.814368 - gail/main.py:201 - [Discriminator] iter = 25000 loss = 0.2326 grad_norm = 5.2508 grad_penalty = 0.5157 regularization = 0.0000 true_logits = -0.1168 fake_logits = -0.3999 true_prob = 0.4711 fake_prob = 0.4020 
2022-07-08 09:20:48.147327 - gail/main.py:142 - [Evaluate] iter = 25000 episode={ returns = 151.4039 lengths = 72 } discounted_episode={ returns = 143.7783 lengths = 71 } 
2022-07-08 09:21:14.055348 - gail/main.py:174 - [TRPO] iter = 26000 dist_mean = 0.2152 dist_std = 0.7457 vf_loss = 0.0552 grad_norm = 0.5437 nat_grad_norm = 0.2900 cg_residual = 0.0007 step_size = 0.5696 reward = -0.0000 fps = 19 mse_loss = 1.0861 
2022-07-08 09:21:40.804209 - gail/main.py:174 - [TRPO] iter = 27000 dist_mean = 0.2367 dist_std = 0.7348 vf_loss = 0.0565 grad_norm = 0.5984 nat_grad_norm = 0.2684 cg_residual = 0.0006 step_size = 0.5448 reward = -0.0000 fps = 12 mse_loss = 1.2726 
2022-07-08 09:22:14.284417 - gail/main.py:174 - [TRPO] iter = 28000 dist_mean = 0.1962 dist_std = 0.7214 vf_loss = 0.0336 grad_norm = 0.8070 nat_grad_norm = 0.3504 cg_residual = 0.0023 step_size = 0.4136 reward = 0.0000 fps = 8 mse_loss = 1.1631 
2022-07-08 09:22:41.434033 - gail/main.py:174 - [TRPO] iter = 29000 dist_mean = 0.2077 dist_std = 0.7080 vf_loss = 0.0603 grad_norm = 0.8653 nat_grad_norm = 0.3128 cg_residual = 0.0019 step_size = 0.4225 reward = -0.0000 fps = 7 mse_loss = 1.2068 
2022-07-08 09:23:09.127592 - gail/main.py:174 - [TRPO] iter = 30000 dist_mean = 0.2169 dist_std = 0.7044 vf_loss = 0.0729 grad_norm = 0.7730 nat_grad_norm = 0.3587 cg_residual = 0.0020 step_size = 0.4452 reward = -0.0000 fps = 5 mse_loss = 1.1024 
2022-07-08 09:23:10.005925 - gail/main.py:201 - [Discriminator] iter = 30000 loss = 0.0636 grad_norm = 5.2505 grad_penalty = 0.4787 regularization = 0.0000 true_logits = -0.0929 fake_logits = -0.5080 true_prob = 0.4772 fake_prob = 0.3767 
2022-07-08 09:23:40.284509 - gail/main.py:142 - [Evaluate] iter = 30000 episode={ returns = 181.4794 lengths = 81 } discounted_episode={ returns = 173.9151 lengths = 81 } 
2022-07-08 09:24:07.756404 - gail/main.py:174 - [TRPO] iter = 31000 dist_mean = 0.2297 dist_std = 0.6905 vf_loss = 0.0663 grad_norm = 0.4081 nat_grad_norm = 0.3554 cg_residual = 0.0016 step_size = 0.5270 reward = 0.0000 fps = 17 mse_loss = 1.3083 
2022-07-08 09:24:33.479161 - gail/main.py:174 - [TRPO] iter = 32000 dist_mean = 0.2110 dist_std = 0.6807 vf_loss = 0.0533 grad_norm = 0.6239 nat_grad_norm = 0.3122 cg_residual = 0.0025 step_size = 0.4790 reward = 0.0000 fps = 11 mse_loss = 1.3966 
2022-07-08 09:24:59.263014 - gail/main.py:174 - [TRPO] iter = 33000 dist_mean = 0.2353 dist_std = 0.6679 vf_loss = 0.0466 grad_norm = 0.5990 nat_grad_norm = 0.3800 cg_residual = 0.0039 step_size = 0.5146 reward = 0.0000 fps = 9 mse_loss = 1.5581 
2022-07-08 09:25:25.304595 - gail/main.py:174 - [TRPO] iter = 34000 dist_mean = 0.2349 dist_std = 0.6641 vf_loss = 0.0286 grad_norm = 0.4840 nat_grad_norm = 0.3888 cg_residual = 0.0039 step_size = 0.5091 reward = -0.0000 fps = 7 mse_loss = 1.6106 
2022-07-08 09:25:51.720830 - gail/main.py:174 - [TRPO] iter = 35000 dist_mean = 0.2306 dist_std = 0.6598 vf_loss = 0.0426 grad_norm = 0.4497 nat_grad_norm = 0.3282 cg_residual = 0.0040 step_size = 0.5352 reward = -0.0000 fps = 6 mse_loss = 1.8104 
2022-07-08 09:25:52.449702 - gail/main.py:201 - [Discriminator] iter = 35000 loss = -0.1942 grad_norm = 5.6992 grad_penalty = 0.4096 regularization = 0.0000 true_logits = -0.0502 fake_logits = -0.6541 true_prob = 0.4878 fake_prob = 0.3438 
2022-07-08 09:26:23.797050 - gail/main.py:142 - [Evaluate] iter = 35000 episode={ returns = 187.6796 lengths = 82 } discounted_episode={ returns = 178.5474 lengths = 83 } 
2022-07-08 09:26:51.152563 - gail/main.py:174 - [TRPO] iter = 36000 dist_mean = 0.2137 dist_std = 0.6462 vf_loss = 0.0416 grad_norm = 0.5041 nat_grad_norm = 0.2427 cg_residual = 0.0026 step_size = 0.5977 reward = 0.0000 fps = 17 mse_loss = 1.9085 
2022-07-08 09:27:17.414760 - gail/main.py:174 - [TRPO] iter = 37000 dist_mean = 0.2267 dist_std = 0.6399 vf_loss = 0.0427 grad_norm = 0.8805 nat_grad_norm = 0.3497 cg_residual = 0.0065 step_size = 0.5154 reward = 0.0000 fps = 11 mse_loss = 1.8510 
2022-07-08 09:27:44.192933 - gail/main.py:174 - [TRPO] iter = 38000 dist_mean = 0.2104 dist_std = 0.6291 vf_loss = 0.0204 grad_norm = 0.7024 nat_grad_norm = 0.2511 cg_residual = 0.0030 step_size = 0.5782 reward = 0.0000 fps = 8 mse_loss = 1.7818 
2022-07-08 09:28:10.545455 - gail/main.py:174 - [TRPO] iter = 39000 dist_mean = 0.2242 dist_std = 0.6199 vf_loss = 0.0282 grad_norm = 0.6286 nat_grad_norm = 0.2968 cg_residual = 0.0062 step_size = 0.5880 reward = 0.0000 fps = 7 mse_loss = 1.8711 
2022-07-08 09:28:37.265343 - gail/main.py:174 - [TRPO] iter = 40000 dist_mean = 0.1909 dist_std = 0.6162 vf_loss = 0.0203 grad_norm = 0.5197 nat_grad_norm = 0.2785 cg_residual = 0.0061 step_size = 0.6007 reward = -0.0000 fps = 6 mse_loss = 1.8370 
2022-07-08 09:28:38.018879 - gail/main.py:201 - [Discriminator] iter = 40000 loss = -0.4049 grad_norm = 5.5519 grad_penalty = 0.3653 regularization = 0.0000 true_logits = -0.0584 fake_logits = -0.8287 true_prob = 0.4863 fake_prob = 0.3067 
2022-07-08 09:29:06.963713 - gail/main.py:142 - [Evaluate] iter = 40000 episode={ returns = 185.1044 lengths = 82 } discounted_episode={ returns = 176.4178 lengths = 82 } 
2022-07-08 09:29:33.405629 - gail/main.py:174 - [TRPO] iter = 41000 dist_mean = 0.2270 dist_std = 0.6042 vf_loss = 0.0375 grad_norm = 0.5923 nat_grad_norm = 0.4153 cg_residual = 0.0087 step_size = 0.4570 reward = -0.0000 fps = 18 mse_loss = 2.0923 
2022-07-08 09:29:59.293486 - gail/main.py:174 - [TRPO] iter = 42000 dist_mean = 0.2205 dist_std = 0.5938 vf_loss = 0.0152 grad_norm = 0.9188 nat_grad_norm = 0.3256 cg_residual = 0.0056 step_size = 0.4723 reward = 0.0000 fps = 12 mse_loss = 1.8826 
2022-07-08 09:30:25.461374 - gail/main.py:174 - [TRPO] iter = 43000 dist_mean = 0.2076 dist_std = 0.5898 vf_loss = 0.0196 grad_norm = 0.9524 nat_grad_norm = 0.3844 cg_residual = 0.0219 step_size = 0.4208 reward = -0.0000 fps = 9 mse_loss = 2.0372 
2022-07-08 09:30:51.738574 - gail/main.py:174 - [TRPO] iter = 44000 dist_mean = 0.2036 dist_std = 0.5833 vf_loss = 0.0260 grad_norm = 0.7796 nat_grad_norm = 0.3481 cg_residual = 0.0428 step_size = 0.4877 reward = 0.0000 fps = 7 mse_loss = 1.9859 
2022-07-08 09:31:18.807927 - gail/main.py:174 - [TRPO] iter = 45000 dist_mean = 0.2005 dist_std = 0.5716 vf_loss = 0.0432 grad_norm = 0.8479 nat_grad_norm = 0.3042 cg_residual = 0.0211 step_size = 0.4385 reward = -0.0000 fps = 6 mse_loss = 2.3184 
2022-07-08 09:31:19.537603 - gail/main.py:201 - [Discriminator] iter = 45000 loss = -0.5579 grad_norm = 4.9007 grad_penalty = 0.3805 regularization = 0.0000 true_logits = -0.0340 fake_logits = -0.9724 true_prob = 0.4928 fake_prob = 0.2785 
2022-07-08 09:31:49.112556 - gail/main.py:142 - [Evaluate] iter = 45000 episode={ returns = 192.8000 lengths = 84 } discounted_episode={ returns = 183.5244 lengths = 84 } 
2022-07-08 09:32:21.615038 - gail/main.py:174 - [TRPO] iter = 46000 dist_mean = 0.2004 dist_std = 0.5585 vf_loss = 0.0220 grad_norm = 1.0817 nat_grad_norm = 0.2836 cg_residual = 0.0448 step_size = 0.4759 reward = 0.0000 fps = 16 mse_loss = 2.3520 
2022-07-08 09:32:48.132907 - gail/main.py:174 - [TRPO] iter = 47000 dist_mean = 0.1789 dist_std = 0.5493 vf_loss = 0.0337 grad_norm = 1.0988 nat_grad_norm = 0.2996 cg_residual = 0.0283 step_size = 0.4301 reward = -0.0000 fps = 11 mse_loss = 2.5246 
2022-07-08 09:33:13.976259 - gail/main.py:174 - [TRPO] iter = 48000 dist_mean = 0.1807 dist_std = 0.5389 vf_loss = 0.0622 grad_norm = 1.2491 nat_grad_norm = 0.2004 cg_residual = 0.0407 step_size = 0.4819 reward = 0.0000 fps = 8 mse_loss = 2.5563 
2022-07-08 09:33:39.488805 - gail/main.py:174 - [TRPO] iter = 49000 dist_mean = 0.1994 dist_std = 0.5332 vf_loss = 0.0467 grad_norm = 1.5604 nat_grad_norm = 0.3106 cg_residual = 0.0546 step_size = 0.4091 reward = -0.0000 fps = 7 mse_loss = 2.6147 
2022-07-08 09:34:04.953452 - gail/main.py:174 - [TRPO] iter = 50000 dist_mean = 0.1871 dist_std = 0.5267 vf_loss = 0.0646 grad_norm = 1.1572 nat_grad_norm = 0.1967 cg_residual = 0.0274 step_size = 0.4869 reward = -0.0000 fps = 6 mse_loss = 2.4905 
2022-07-08 09:34:05.718263 - gail/main.py:201 - [Discriminator] iter = 50000 loss = -0.8306 grad_norm = 5.3214 grad_penalty = 0.3350 regularization = 0.0000 true_logits = -0.0386 fake_logits = -1.2042 true_prob = 0.4924 fake_prob = 0.2361 
2022-07-08 09:34:37.177415 - gail/main.py:142 - [Evaluate] iter = 50000 episode={ returns = 236.7419 lengths = 95 } discounted_episode={ returns = 223.0379 lengths = 95 } 
2022-07-08 09:35:03.322167 - gail/main.py:174 - [TRPO] iter = 51000 dist_mean = 0.1683 dist_std = 0.5185 vf_loss = 0.0812 grad_norm = 1.9120 nat_grad_norm = 0.2433 cg_residual = 0.1431 step_size = 0.3944 reward = -0.0000 fps = 17 mse_loss = 2.6689 
2022-07-08 09:35:28.791269 - gail/main.py:174 - [TRPO] iter = 52000 dist_mean = 0.1597 dist_std = 0.5171 vf_loss = 0.0799 grad_norm = 2.0665 nat_grad_norm = 0.2276 cg_residual = 0.0272 step_size = 0.4056 reward = 0.0000 fps = 12 mse_loss = 2.8093 
2022-07-08 09:35:54.118308 - gail/main.py:174 - [TRPO] iter = 53000 dist_mean = 0.1639 dist_std = 0.5221 vf_loss = 0.0402 grad_norm = 1.7922 nat_grad_norm = 0.2170 cg_residual = 0.0245 step_size = 0.4597 reward = 0.0000 fps = 9 mse_loss = 2.6074 
2022-07-08 09:36:19.496972 - gail/main.py:174 - [TRPO] iter = 54000 dist_mean = 0.1356 dist_std = 0.5200 vf_loss = 0.0398 grad_norm = 1.6319 nat_grad_norm = 0.1887 cg_residual = 0.0306 step_size = 0.4781 reward = 0.0000 fps = 7 mse_loss = 2.6353 
2022-07-08 09:36:44.477437 - gail/main.py:174 - [TRPO] iter = 55000 dist_mean = 0.1215 dist_std = 0.5182 vf_loss = 0.0680 grad_norm = 1.5209 nat_grad_norm = 0.2194 cg_residual = 0.0271 step_size = 0.4677 reward = -0.0000 fps = 6 mse_loss = 2.6093 
2022-07-08 09:36:45.182970 - gail/main.py:201 - [Discriminator] iter = 55000 loss = -1.0355 grad_norm = 4.1591 grad_penalty = 0.2803 regularization = 0.0000 true_logits = -0.0452 fake_logits = -1.3610 true_prob = 0.4921 fake_prob = 0.2112 
2022-07-08 09:37:19.499535 - gail/main.py:142 - [Evaluate] iter = 55000 episode={ returns = 258.0774 lengths = 101 } discounted_episode={ returns = 242.3798 lengths = 101 } 
2022-07-08 09:37:44.921994 - gail/main.py:174 - [TRPO] iter = 56000 dist_mean = 0.1102 dist_std = 0.5088 vf_loss = 0.0802 grad_norm = 1.4154 nat_grad_norm = 0.2481 cg_residual = 0.0588 step_size = 0.4327 reward = 0.0000 fps = 16 mse_loss = 2.8115 
2022-07-08 09:38:09.373647 - gail/main.py:174 - [TRPO] iter = 57000 dist_mean = 0.0999 dist_std = 0.5079 vf_loss = 0.0440 grad_norm = 2.0247 nat_grad_norm = 0.2551 cg_residual = 0.0648 step_size = 0.4178 reward = 0.0000 fps = 11 mse_loss = 2.5531 
2022-07-08 09:38:34.267724 - gail/main.py:174 - [TRPO] iter = 58000 dist_mean = 0.0984 dist_std = 0.5034 vf_loss = 0.0622 grad_norm = 2.3810 nat_grad_norm = 0.2053 cg_residual = 0.0349 step_size = 0.4392 reward = 0.0000 fps = 9 mse_loss = 2.6552 
2022-07-08 09:38:59.944561 - gail/main.py:174 - [TRPO] iter = 59000 dist_mean = 0.0929 dist_std = 0.5075 vf_loss = 0.0645 grad_norm = 1.9058 nat_grad_norm = 0.2688 cg_residual = 0.0450 step_size = 0.4045 reward = 0.0000 fps = 7 mse_loss = 2.9088 
2022-07-08 09:39:24.426810 - gail/main.py:174 - [TRPO] iter = 60000 dist_mean = 0.1018 dist_std = 0.4961 vf_loss = 0.1711 grad_norm = 1.9180 nat_grad_norm = 0.2571 cg_residual = 0.0622 step_size = 0.4048 reward = -0.0000 fps = 6 mse_loss = 2.7993 
2022-07-08 09:39:25.173549 - gail/main.py:201 - [Discriminator] iter = 60000 loss = -1.1345 grad_norm = 4.4884 grad_penalty = 0.2386 regularization = 0.0000 true_logits = -0.1144 fake_logits = -1.4876 true_prob = 0.4771 fake_prob = 0.1948 
2022-07-08 09:40:01.215266 - gail/main.py:142 - [Evaluate] iter = 60000 episode={ returns = 288.8926 lengths = 108 } discounted_episode={ returns = 270.0255 lengths = 108 } 
2022-07-08 09:40:28.184863 - gail/main.py:174 - [TRPO] iter = 61000 dist_mean = 0.1050 dist_std = 0.4961 vf_loss = 0.2081 grad_norm = 1.3136 nat_grad_norm = 0.3165 cg_residual = 0.0620 step_size = 0.4077 reward = 0.0000 fps = 15 mse_loss = 2.8284 
2022-07-08 09:40:52.788070 - gail/main.py:174 - [TRPO] iter = 62000 dist_mean = 0.0965 dist_std = 0.4882 vf_loss = 0.0766 grad_norm = 1.3628 nat_grad_norm = 0.2430 cg_residual = 0.0281 step_size = 0.4707 reward = -0.0000 fps = 11 mse_loss = 2.7309 
2022-07-08 09:41:17.226913 - gail/main.py:174 - [TRPO] iter = 63000 dist_mean = 0.0709 dist_std = 0.4839 vf_loss = 0.0930 grad_norm = 2.1558 nat_grad_norm = 0.2255 cg_residual = 0.0435 step_size = 0.3683 reward = 0.0000 fps = 8 mse_loss = 2.8125 
2022-07-08 09:41:41.765539 - gail/main.py:174 - [TRPO] iter = 64000 dist_mean = 0.0852 dist_std = 0.4827 vf_loss = 0.0683 grad_norm = 1.7167 nat_grad_norm = 0.2288 cg_residual = 0.1404 step_size = 0.4399 reward = 0.0000 fps = 7 mse_loss = 2.7384 
2022-07-08 09:42:09.984174 - gail/main.py:174 - [TRPO] iter = 65000 dist_mean = 0.0757 dist_std = 0.4753 vf_loss = 0.0803 grad_norm = 1.5177 nat_grad_norm = 0.2349 cg_residual = 0.1608 step_size = 0.4585 reward = -0.0000 fps = 6 mse_loss = 2.7207 
2022-07-08 09:42:10.918523 - gail/main.py:201 - [Discriminator] iter = 65000 loss = -1.2388 grad_norm = 3.7651 grad_penalty = 0.2464 regularization = 0.0000 true_logits = -0.0966 fake_logits = -1.5818 true_prob = 0.4816 fake_prob = 0.1842 
2022-07-08 09:42:47.959121 - gail/main.py:142 - [Evaluate] iter = 65000 episode={ returns = 315.8386 lengths = 115 } discounted_episode={ returns = 296.2392 lengths = 115 } 
2022-07-08 09:43:11.878401 - gail/main.py:174 - [TRPO] iter = 66000 dist_mean = 0.0715 dist_std = 0.4762 vf_loss = 0.0420 grad_norm = 1.1831 nat_grad_norm = 0.2214 cg_residual = 0.1158 step_size = 0.5430 reward = -0.0000 fps = 16 mse_loss = 2.8607 
2022-07-08 09:43:35.614139 - gail/main.py:174 - [TRPO] iter = 67000 dist_mean = 0.0764 dist_std = 0.4773 vf_loss = 0.1023 grad_norm = 1.8436 nat_grad_norm = 0.2368 cg_residual = 0.2460 step_size = 0.4324 reward = -0.0000 fps = 11 mse_loss = 2.5576 
2022-07-08 09:43:59.878327 - gail/main.py:174 - [TRPO] iter = 68000 dist_mean = 0.0685 dist_std = 0.4727 vf_loss = 0.0632 grad_norm = 1.1280 nat_grad_norm = 0.3254 cg_residual = 0.0576 step_size = 0.4660 reward = 0.0000 fps = 9 mse_loss = 2.8297 
2022-07-08 09:44:24.090092 - gail/main.py:174 - [TRPO] iter = 69000 dist_mean = 0.0377 dist_std = 0.4706 vf_loss = 0.0471 grad_norm = 1.3872 nat_grad_norm = 0.2342 cg_residual = 0.2548 step_size = 0.5525 reward = 0.0000 fps = 7 mse_loss = 3.0923 
2022-07-08 09:44:48.063837 - gail/main.py:174 - [TRPO] iter = 70000 dist_mean = 0.0612 dist_std = 0.4675 vf_loss = 0.0692 grad_norm = 1.1273 nat_grad_norm = 0.2733 cg_residual = 0.0513 step_size = 0.4433 reward = -0.0000 fps = 6 mse_loss = 2.9432 
2022-07-08 09:44:48.812160 - gail/main.py:201 - [Discriminator] iter = 70000 loss = -1.2906 grad_norm = 3.8430 grad_penalty = 0.2551 regularization = 0.0000 true_logits = -0.0855 fake_logits = -1.6312 true_prob = 0.4851 fake_prob = 0.1783 
2022-07-08 09:45:26.263200 - gail/main.py:142 - [Evaluate] iter = 70000 episode={ returns = 341.6816 lengths = 121 } discounted_episode={ returns = 317.1510 lengths = 121 } 
2022-07-08 09:45:50.476042 - gail/main.py:174 - [TRPO] iter = 71000 dist_mean = 0.0402 dist_std = 0.4698 vf_loss = 0.0464 grad_norm = 0.9538 nat_grad_norm = 0.1970 cg_residual = 0.0342 step_size = 0.5477 reward = 0.0000 fps = 16 mse_loss = 2.8640 
2022-07-08 09:46:14.802794 - gail/main.py:174 - [TRPO] iter = 72000 dist_mean = 0.0400 dist_std = 0.4660 vf_loss = 0.0443 grad_norm = 0.9240 nat_grad_norm = 0.2842 cg_residual = 0.0634 step_size = 0.4931 reward = -0.0000 fps = 11 mse_loss = 2.9925 
2022-07-08 09:46:38.828126 - gail/main.py:174 - [TRPO] iter = 73000 dist_mean = 0.0410 dist_std = 0.4631 vf_loss = 0.0581 grad_norm = 1.2830 nat_grad_norm = 0.2381 cg_residual = 0.1171 step_size = 0.5273 reward = 0.0000 fps = 9 mse_loss = 2.9179 
2022-07-08 09:47:02.843604 - gail/main.py:174 - [TRPO] iter = 74000 dist_mean = 0.0332 dist_std = 0.4569 vf_loss = 0.1275 grad_norm = 1.4829 nat_grad_norm = 0.2133 cg_residual = 0.3049 step_size = 0.4616 reward = -0.0000 fps = 7 mse_loss = 2.9053 
2022-07-08 09:47:27.314456 - gail/main.py:174 - [TRPO] iter = 75000 dist_mean = 0.0534 dist_std = 0.4580 vf_loss = 0.1106 grad_norm = 1.5426 nat_grad_norm = 0.2943 cg_residual = 0.0531 step_size = 0.4281 reward = -0.0000 fps = 6 mse_loss = 2.8938 
2022-07-08 09:47:28.027090 - gail/main.py:201 - [Discriminator] iter = 75000 loss = -1.2890 grad_norm = 3.6793 grad_penalty = 0.2328 regularization = 0.0000 true_logits = -0.0928 fake_logits = -1.6145 true_prob = 0.4828 fake_prob = 0.1805 
2022-07-08 09:48:08.546085 - gail/main.py:142 - [Evaluate] iter = 75000 episode={ returns = 368.3868 lengths = 128 } discounted_episode={ returns = 341.4907 lengths = 128 } 
2022-07-08 09:48:31.901778 - gail/main.py:174 - [TRPO] iter = 76000 dist_mean = 0.0553 dist_std = 0.4534 vf_loss = 0.0695 grad_norm = 1.1026 nat_grad_norm = 0.2114 cg_residual = 0.1962 step_size = 0.5978 reward = 0.0000 fps = 15 mse_loss = 3.1662 
2022-07-08 09:48:56.199527 - gail/main.py:174 - [TRPO] iter = 77000 dist_mean = 0.0454 dist_std = 0.4581 vf_loss = 0.0546 grad_norm = 1.2326 nat_grad_norm = 0.3546 cg_residual = 0.1960 step_size = 0.3894 reward = 0.0000 fps = 11 mse_loss = 3.1279 
2022-07-08 09:49:20.466476 - gail/main.py:174 - [TRPO] iter = 78000 dist_mean = 0.0588 dist_std = 0.4567 vf_loss = 0.0783 grad_norm = 1.5479 nat_grad_norm = 0.1890 cg_residual = 0.1228 step_size = 0.5084 reward = -0.0000 fps = 8 mse_loss = 3.1044 
2022-07-08 09:49:45.870057 - gail/main.py:174 - [TRPO] iter = 79000 dist_mean = 0.0483 dist_std = 0.4516 vf_loss = 0.0822 grad_norm = 1.3911 nat_grad_norm = 0.2304 cg_residual = 0.0430 step_size = 0.4694 reward = -0.0000 fps = 7 mse_loss = 3.3125 
2022-07-08 09:50:12.621747 - gail/main.py:174 - [TRPO] iter = 80000 dist_mean = 0.0395 dist_std = 0.4471 vf_loss = 0.0761 grad_norm = 0.6621 nat_grad_norm = 0.1953 cg_residual = 0.0401 step_size = 0.5939 reward = 0.0000 fps = 6 mse_loss = 3.1988 
2022-07-08 09:50:13.357796 - gail/main.py:201 - [Discriminator] iter = 80000 loss = -1.2838 grad_norm = 3.3050 grad_penalty = 0.2048 regularization = 0.0000 true_logits = -0.1021 fake_logits = -1.5908 true_prob = 0.4826 fake_prob = 0.1858 
2022-07-08 09:50:55.222096 - gail/main.py:142 - [Evaluate] iter = 80000 episode={ returns = 388.7339 lengths = 135 } discounted_episode={ returns = 359.7117 lengths = 135 } 
2022-07-08 09:51:18.403453 - gail/main.py:174 - [TRPO] iter = 81000 dist_mean = 0.0423 dist_std = 0.4413 vf_loss = 0.0878 grad_norm = 0.9069 nat_grad_norm = 0.2530 cg_residual = 0.1414 step_size = 0.5440 reward = -0.0000 fps = 15 mse_loss = 3.6082 
2022-07-08 09:51:41.783768 - gail/main.py:174 - [TRPO] iter = 82000 dist_mean = 0.0330 dist_std = 0.4421 vf_loss = 0.1195 grad_norm = 1.5128 nat_grad_norm = 0.3176 cg_residual = 0.1481 step_size = 0.4403 reward = -0.0000 fps = 11 mse_loss = 3.7751 
2022-07-08 09:52:13.533461 - gail/main.py:174 - [TRPO] iter = 83000 dist_mean = 0.0312 dist_std = 0.4379 vf_loss = 0.0756 grad_norm = 1.7340 nat_grad_norm = 0.2237 cg_residual = 0.1190 step_size = 0.4577 reward = -0.0000 fps = 8 mse_loss = 3.7723 
2022-07-08 09:52:37.269266 - gail/main.py:174 - [TRPO] iter = 84000 dist_mean = 0.0505 dist_std = 0.4365 vf_loss = 0.1083 grad_norm = 1.9647 nat_grad_norm = 0.2378 cg_residual = 0.0888 step_size = 0.3876 reward = 0.0000 fps = 6 mse_loss = 3.7012 
2022-07-08 09:53:02.072825 - gail/main.py:174 - [TRPO] iter = 85000 dist_mean = 0.0399 dist_std = 0.4309 vf_loss = 0.0770 grad_norm = 1.2408 nat_grad_norm = 0.2522 cg_residual = 0.0816 step_size = 0.4556 reward = 0.0000 fps = 5 mse_loss = 3.4736 
2022-07-08 09:53:02.843589 - gail/main.py:201 - [Discriminator] iter = 85000 loss = -1.2678 grad_norm = 2.8485 grad_penalty = 0.2134 regularization = 0.0000 true_logits = -0.0339 fake_logits = -1.5151 true_prob = 0.4992 fake_prob = 0.1995 
2022-07-08 09:53:49.030196 - gail/main.py:142 - [Evaluate] iter = 85000 episode={ returns = 411.4572 lengths = 141 } discounted_episode={ returns = 377.6954 lengths = 140 } 
2022-07-08 09:54:14.413079 - gail/main.py:174 - [TRPO] iter = 86000 dist_mean = 0.0401 dist_std = 0.4270 vf_loss = 0.0498 grad_norm = 0.9695 nat_grad_norm = 0.2455 cg_residual = 0.0686 step_size = 0.5191 reward = 0.0000 fps = 13 mse_loss = 3.6952 
2022-07-08 09:54:39.370694 - gail/main.py:174 - [TRPO] iter = 87000 dist_mean = 0.0518 dist_std = 0.4271 vf_loss = 0.0782 grad_norm = 1.5113 nat_grad_norm = 0.1923 cg_residual = 0.0488 step_size = 0.4534 reward = 0.0000 fps = 10 mse_loss = 3.4928 
2022-07-08 09:55:03.359531 - gail/main.py:174 - [TRPO] iter = 88000 dist_mean = 0.0566 dist_std = 0.4247 vf_loss = 0.3052 grad_norm = 1.9154 nat_grad_norm = 0.3382 cg_residual = 0.1632 step_size = 0.3602 reward = -0.0000 fps = 8 mse_loss = 3.7025 
2022-07-08 09:55:26.838161 - gail/main.py:174 - [TRPO] iter = 89000 dist_mean = 0.0565 dist_std = 0.4207 vf_loss = 0.1128 grad_norm = 1.2651 nat_grad_norm = 0.2360 cg_residual = 0.1024 step_size = 0.4890 reward = -0.0000 fps = 6 mse_loss = 3.5770 
2022-07-08 09:55:50.220548 - gail/main.py:174 - [TRPO] iter = 90000 dist_mean = 0.0685 dist_std = 0.4110 vf_loss = 0.0925 grad_norm = 1.7407 nat_grad_norm = 0.1728 cg_residual = 0.0853 step_size = 0.5029 reward = -0.0000 fps = 5 mse_loss = 3.6345 
2022-07-08 09:55:50.959945 - gail/main.py:201 - [Discriminator] iter = 90000 loss = -1.2255 grad_norm = 3.5347 grad_penalty = 0.2334 regularization = 0.0000 true_logits = 0.0246 fake_logits = -1.4343 true_prob = 0.5125 fake_prob = 0.2151 
2022-07-08 09:56:34.410890 - gail/main.py:142 - [Evaluate] iter = 90000 episode={ returns = 439.5732 lengths = 148 } discounted_episode={ returns = 402.3400 lengths = 148 } 
2022-07-08 09:56:57.294795 - gail/main.py:174 - [TRPO] iter = 91000 dist_mean = 0.0811 dist_std = 0.4120 vf_loss = 0.0916 grad_norm = 2.0489 nat_grad_norm = 0.2033 cg_residual = 0.1263 step_size = 0.4154 reward = -0.0000 fps = 15 mse_loss = 3.5312 
2022-07-08 09:57:20.662844 - gail/main.py:174 - [TRPO] iter = 92000 dist_mean = 0.1052 dist_std = 0.4083 vf_loss = 0.0828 grad_norm = 1.2835 nat_grad_norm = 0.1862 cg_residual = 0.0644 step_size = 0.5114 reward = 0.0000 fps = 11 mse_loss = 3.5255 
2022-07-08 09:57:44.385640 - gail/main.py:174 - [TRPO] iter = 93000 dist_mean = 0.1141 dist_std = 0.4071 vf_loss = 0.0599 grad_norm = 1.9158 nat_grad_norm = 0.2450 cg_residual = 0.1974 step_size = 0.4284 reward = -0.0000 fps = 8 mse_loss = 3.9537 
2022-07-08 09:58:07.739120 - gail/main.py:174 - [TRPO] iter = 94000 dist_mean = 0.0999 dist_std = 0.4081 vf_loss = 0.0793 grad_norm = 1.4278 nat_grad_norm = 0.2294 cg_residual = 0.0903 step_size = 0.4413 reward = 0.0000 fps = 7 mse_loss = 3.6160 
2022-07-08 09:58:32.030323 - gail/main.py:174 - [TRPO] iter = 95000 dist_mean = 0.1183 dist_std = 0.4051 vf_loss = 0.2020 grad_norm = 1.5228 nat_grad_norm = 0.2424 cg_residual = 0.1574 step_size = 0.4159 reward = -0.0000 fps = 6 mse_loss = 3.7480 
2022-07-08 09:58:32.784993 - gail/main.py:201 - [Discriminator] iter = 95000 loss = -1.1844 grad_norm = 3.3551 grad_penalty = 0.2330 regularization = 0.0000 true_logits = -0.0089 fake_logits = -1.4262 true_prob = 0.5038 fake_prob = 0.2187 
2022-07-08 09:59:24.485890 - gail/main.py:142 - [Evaluate] iter = 95000 episode={ returns = 506.1870 lengths = 167 } discounted_episode={ returns = 459.0010 lengths = 167 } 
2022-07-08 09:59:49.903759 - gail/main.py:174 - [TRPO] iter = 96000 dist_mean = 0.1490 dist_std = 0.4082 vf_loss = 0.1459 grad_norm = 1.1674 nat_grad_norm = 0.2166 cg_residual = 0.1740 step_size = 0.4750 reward = -0.0000 fps = 12 mse_loss = 3.4625 
2022-07-08 10:00:14.994129 - gail/main.py:174 - [TRPO] iter = 97000 dist_mean = 0.1381 dist_std = 0.4043 vf_loss = 0.0959 grad_norm = 1.1408 nat_grad_norm = 0.2168 cg_residual = 0.0652 step_size = 0.5002 reward = -0.0000 fps = 9 mse_loss = 3.4682 
2022-07-08 10:00:39.617227 - gail/main.py:174 - [TRPO] iter = 98000 dist_mean = 0.1450 dist_std = 0.3997 vf_loss = 0.8988 grad_norm = 1.4455 nat_grad_norm = 0.1575 cg_residual = 0.0836 step_size = 0.5485 reward = -0.0000 fps = 7 mse_loss = 3.4312 
2022-07-08 10:01:05.784327 - gail/main.py:174 - [TRPO] iter = 99000 dist_mean = 0.1712 dist_std = 0.4031 vf_loss = 0.1118 grad_norm = 1.5180 nat_grad_norm = 0.2320 cg_residual = 0.1164 step_size = 0.4156 reward = -0.0000 fps = 6 mse_loss = 3.6180 
2022-07-08 10:01:31.182827 - gail/main.py:174 - [TRPO] iter = 100000 dist_mean = 0.1863 dist_std = 0.3994 vf_loss = 1.1615 grad_norm = 0.9819 nat_grad_norm = 0.3064 cg_residual = 0.2654 step_size = 0.4064 reward = 0.0000 fps = 5 mse_loss = 3.6667 
2022-07-08 10:01:31.936067 - gail/main.py:201 - [Discriminator] iter = 100000 loss = -1.2713 grad_norm = 3.3957 grad_penalty = 0.2103 regularization = 0.0000 true_logits = 0.0539 fake_logits = -1.4277 true_prob = 0.5175 fake_prob = 0.2206 
2022-07-08 10:02:40.246817 - gail/main.py:142 - [Evaluate] iter = 100000 episode={ returns = 524.7527 lengths = 175 } discounted_episode={ returns = 475.2330 lengths = 175 } 
2022-07-08 10:03:07.124519 - gail/main.py:174 - [TRPO] iter = 101000 dist_mean = 0.2041 dist_std = 0.3985 vf_loss = 0.8901 grad_norm = 1.2679 nat_grad_norm = 0.2124 cg_residual = 0.0828 step_size = 0.5077 reward = 0.0000 fps = 10 mse_loss = 3.6057 
2022-07-08 10:03:34.758302 - gail/main.py:174 - [TRPO] iter = 102000 dist_mean = 0.2234 dist_std = 0.3983 vf_loss = 0.0907 grad_norm = 1.5035 nat_grad_norm = 0.1806 cg_residual = 0.1533 step_size = 0.4765 reward = 0.0000 fps = 8 mse_loss = 3.6670 
2022-07-08 10:04:03.272635 - gail/main.py:174 - [TRPO] iter = 103000 dist_mean = 0.2425 dist_std = 0.3953 vf_loss = 0.2632 grad_norm = 1.6037 nat_grad_norm = 0.2268 cg_residual = 0.1125 step_size = 0.3940 reward = 0.0000 fps = 6 mse_loss = 3.4946 
2022-07-08 10:04:30.675964 - gail/main.py:174 - [TRPO] iter = 104000 dist_mean = 0.2167 dist_std = 0.3997 vf_loss = 0.1114 grad_norm = 1.2151 nat_grad_norm = 0.2592 cg_residual = 0.1916 step_size = 0.3992 reward = -0.0000 fps = 5 mse_loss = 3.4841 
2022-07-08 10:04:57.714661 - gail/main.py:174 - [TRPO] iter = 105000 dist_mean = 0.2236 dist_std = 0.4015 vf_loss = 0.1225 grad_norm = 1.9894 nat_grad_norm = 0.1856 cg_residual = 0.0786 step_size = 0.4074 reward = 0.0000 fps = 4 mse_loss = 3.4272 
2022-07-08 10:04:58.473949 - gail/main.py:201 - [Discriminator] iter = 105000 loss = -1.2889 grad_norm = 3.0073 grad_penalty = 0.1953 regularization = 0.0000 true_logits = 0.0470 fake_logits = -1.4372 true_prob = 0.5153 fake_prob = 0.2179 
2022-07-08 10:06:04.252752 - gail/main.py:142 - [Evaluate] iter = 105000 episode={ returns = 551.8683 lengths = 181 } discounted_episode={ returns = 497.2430 lengths = 181 } 
2022-07-08 10:06:32.078301 - gail/main.py:174 - [TRPO] iter = 106000 dist_mean = 0.2710 dist_std = 0.3990 vf_loss = 0.1318 grad_norm = 1.1065 nat_grad_norm = 0.3114 cg_residual = 0.1327 step_size = 0.4126 reward = 0.0000 fps = 10 mse_loss = 3.4142 
2022-07-08 10:07:00.222607 - gail/main.py:174 - [TRPO] iter = 107000 dist_mean = 0.2569 dist_std = 0.3973 vf_loss = 0.1166 grad_norm = 1.0888 nat_grad_norm = 0.1977 cg_residual = 0.0996 step_size = 0.4868 reward = 0.0000 fps = 8 mse_loss = 3.3709 
2022-07-08 10:07:28.338357 - gail/main.py:174 - [TRPO] iter = 108000 dist_mean = 0.2647 dist_std = 0.3984 vf_loss = 0.8572 grad_norm = 1.6475 nat_grad_norm = 0.2812 cg_residual = 0.3161 step_size = 0.3804 reward = -0.0000 fps = 6 mse_loss = 3.4708 
2022-07-08 10:07:55.354762 - gail/main.py:174 - [TRPO] iter = 109000 dist_mean = 0.2227 dist_std = 0.3961 vf_loss = 0.3386 grad_norm = 2.1382 nat_grad_norm = 0.3171 cg_residual = 0.2337 step_size = 0.2880 reward = 0.0000 fps = 5 mse_loss = 3.1321 
2022-07-08 10:08:22.495028 - gail/main.py:174 - [TRPO] iter = 110000 dist_mean = 0.2502 dist_std = 0.3965 vf_loss = 0.1513 grad_norm = 1.1853 nat_grad_norm = 0.2747 cg_residual = 0.2065 step_size = 0.3700 reward = -0.0000 fps = 4 mse_loss = 3.2409 
2022-07-08 10:08:23.280946 - gail/main.py:201 - [Discriminator] iter = 110000 loss = -1.2612 grad_norm = 3.1251 grad_penalty = 0.1818 regularization = 0.0000 true_logits = -0.0005 fake_logits = -1.4435 true_prob = 0.5070 fake_prob = 0.2165 
2022-07-08 10:09:31.626174 - gail/main.py:142 - [Evaluate] iter = 110000 episode={ returns = 579.5104 lengths = 186 } discounted_episode={ returns = 518.0452 lengths = 186 } 
2022-07-08 10:09:59.250098 - gail/main.py:174 - [TRPO] iter = 111000 dist_mean = 0.2970 dist_std = 0.3974 vf_loss = 0.1395 grad_norm = 1.4569 nat_grad_norm = 0.2051 cg_residual = 0.1330 step_size = 0.4842 reward = 0.0000 fps = 10 mse_loss = 3.1945 
2022-07-08 10:10:26.146553 - gail/main.py:174 - [TRPO] iter = 112000 dist_mean = 0.2815 dist_std = 0.3946 vf_loss = 0.1070 grad_norm = 1.5769 nat_grad_norm = 0.1653 cg_residual = 0.0788 step_size = 0.5004 reward = -0.0000 fps = 8 mse_loss = 3.1864 
2022-07-08 10:10:53.303500 - gail/main.py:174 - [TRPO] iter = 113000 dist_mean = 0.2260 dist_std = 0.3966 vf_loss = 0.1652 grad_norm = 1.5362 nat_grad_norm = 0.2193 cg_residual = 0.1124 step_size = 0.4283 reward = -0.0000 fps = 6 mse_loss = 3.4142 
2022-07-08 10:11:21.082326 - gail/main.py:174 - [TRPO] iter = 114000 dist_mean = 0.2117 dist_std = 0.3987 vf_loss = 0.6352 grad_norm = 1.1333 nat_grad_norm = 0.2293 cg_residual = 0.0811 step_size = 0.4915 reward = -0.0000 fps = 5 mse_loss = 3.1417 
2022-07-08 10:11:49.554361 - gail/main.py:174 - [TRPO] iter = 115000 dist_mean = 0.2152 dist_std = 0.3972 vf_loss = 0.2558 grad_norm = 1.2342 nat_grad_norm = 0.3218 cg_residual = 0.5000 step_size = 0.3367 reward = 0.0000 fps = 4 mse_loss = 3.2336 
2022-07-08 10:11:50.454825 - gail/main.py:201 - [Discriminator] iter = 115000 loss = -1.3030 grad_norm = 3.2879 grad_penalty = 0.1813 regularization = 0.0000 true_logits = -0.0024 fake_logits = -1.4867 true_prob = 0.5038 fake_prob = 0.2126 
2022-07-08 10:13:04.557603 - gail/main.py:142 - [Evaluate] iter = 115000 episode={ returns = 634.4294 lengths = 198 } discounted_episode={ returns = 559.4973 lengths = 197 } 
2022-07-08 10:13:32.316781 - gail/main.py:174 - [TRPO] iter = 116000 dist_mean = 0.1658 dist_std = 0.3967 vf_loss = 1.3176 grad_norm = 1.4275 nat_grad_norm = 0.2367 cg_residual = 0.0955 step_size = 0.4370 reward = 0.0000 fps = 9 mse_loss = 3.1764 
2022-07-08 10:13:58.814450 - gail/main.py:174 - [TRPO] iter = 117000 dist_mean = 0.2594 dist_std = 0.3928 vf_loss = 0.5438 grad_norm = 0.9268 nat_grad_norm = 0.1985 cg_residual = 0.0809 step_size = 0.5314 reward = 0.0000 fps = 7 mse_loss = 3.2469 
2022-07-08 10:14:26.015266 - gail/main.py:174 - [TRPO] iter = 118000 dist_mean = 0.1876 dist_std = 0.3920 vf_loss = 0.6932 grad_norm = 1.0059 nat_grad_norm = 0.1949 cg_residual = 0.0745 step_size = 0.5464 reward = 0.0000 fps = 6 mse_loss = 3.0145 
2022-07-08 10:14:52.928951 - gail/main.py:174 - [TRPO] iter = 119000 dist_mean = 0.2484 dist_std = 0.3961 vf_loss = 0.4688 grad_norm = 1.1534 nat_grad_norm = 0.2064 cg_residual = 0.2092 step_size = 0.5018 reward = -0.0000 fps = 5 mse_loss = 3.2217 
2022-07-08 10:15:20.231203 - gail/main.py:174 - [TRPO] iter = 120000 dist_mean = 0.2344 dist_std = 0.3918 vf_loss = 0.0752 grad_norm = 1.2884 nat_grad_norm = 0.2471 cg_residual = 0.1969 step_size = 0.4096 reward = -0.0000 fps = 4 mse_loss = 3.0264 
2022-07-08 10:15:21.053194 - gail/main.py:201 - [Discriminator] iter = 120000 loss = -1.4717 grad_norm = 2.8676 grad_penalty = 0.1711 regularization = 0.0000 true_logits = 0.1216 fake_logits = -1.5212 true_prob = 0.5299 fake_prob = 0.2090 
2022-07-08 10:16:34.203685 - gail/main.py:142 - [Evaluate] iter = 120000 episode={ returns = 670.0493 lengths = 208 } discounted_episode={ returns = 592.3361 lengths = 208 } 
2022-07-08 10:16:59.931682 - gail/main.py:174 - [TRPO] iter = 121000 dist_mean = 0.1997 dist_std = 0.3919 vf_loss = 0.2914 grad_norm = 1.2676 nat_grad_norm = 0.2928 cg_residual = 0.1869 step_size = 0.3656 reward = 0.0000 fps = 10 mse_loss = 3.1594 
2022-07-08 10:17:26.209576 - gail/main.py:174 - [TRPO] iter = 122000 dist_mean = 0.2412 dist_std = 0.3929 vf_loss = 0.0866 grad_norm = 1.7838 nat_grad_norm = 0.2360 cg_residual = 0.1874 step_size = 0.4221 reward = 0.0000 fps = 7 mse_loss = 3.1025 
2022-07-08 10:17:52.798865 - gail/main.py:174 - [TRPO] iter = 123000 dist_mean = 0.2455 dist_std = 0.3970 vf_loss = 0.1011 grad_norm = 1.4211 nat_grad_norm = 0.2959 cg_residual = 0.1625 step_size = 0.3706 reward = -0.0000 fps = 6 mse_loss = 3.2058 
2022-07-08 10:18:18.842864 - gail/main.py:174 - [TRPO] iter = 124000 dist_mean = 0.1651 dist_std = 0.3964 vf_loss = 0.5938 grad_norm = 1.3631 nat_grad_norm = 0.2875 cg_residual = 0.1379 step_size = 0.3959 reward = 0.0000 fps = 5 mse_loss = 3.1740 
2022-07-08 10:18:45.115390 - gail/main.py:174 - [TRPO] iter = 125000 dist_mean = 0.2072 dist_std = 0.3965 vf_loss = 0.2093 grad_norm = 0.8094 nat_grad_norm = 0.2331 cg_residual = 0.1307 step_size = 0.4906 reward = 0.0000 fps = 4 mse_loss = 3.1363 
2022-07-08 10:18:45.928104 - gail/main.py:201 - [Discriminator] iter = 125000 loss = -1.3277 grad_norm = 3.1283 grad_penalty = 0.1581 regularization = 0.0000 true_logits = 0.0394 fake_logits = -1.4464 true_prob = 0.5108 fake_prob = 0.2220 
2022-07-08 10:19:59.545214 - gail/main.py:142 - [Evaluate] iter = 125000 episode={ returns = 705.1440 lengths = 219 } discounted_episode={ returns = 619.9434 lengths = 219 } 
2022-07-08 10:20:27.157871 - gail/main.py:174 - [TRPO] iter = 126000 dist_mean = 0.2110 dist_std = 0.3880 vf_loss = 0.1428 grad_norm = 1.0374 nat_grad_norm = 0.2599 cg_residual = 0.1605 step_size = 0.4786 reward = 0.0000 fps = 9 mse_loss = 3.0321 
2022-07-08 10:20:54.641782 - gail/main.py:174 - [TRPO] iter = 127000 dist_mean = 0.2474 dist_std = 0.3847 vf_loss = 0.5018 grad_norm = 0.9730 nat_grad_norm = 0.2026 cg_residual = 0.0757 step_size = 0.5471 reward = -0.0000 fps = 7 mse_loss = 3.0717 
2022-07-08 10:21:21.858110 - gail/main.py:174 - [TRPO] iter = 128000 dist_mean = 0.2414 dist_std = 0.3814 vf_loss = 0.2090 grad_norm = 1.1482 nat_grad_norm = 0.1770 cg_residual = 0.2000 step_size = 0.5616 reward = -0.0000 fps = 6 mse_loss = 3.3821 
2022-07-08 10:21:53.663381 - gail/main.py:174 - [TRPO] iter = 129000 dist_mean = 0.2341 dist_std = 0.3829 vf_loss = 0.2001 grad_norm = 1.5354 nat_grad_norm = 0.2398 cg_residual = 0.1553 step_size = 0.4188 reward = 0.0000 fps = 5 mse_loss = 3.1442 
2022-07-08 10:22:19.365681 - gail/main.py:174 - [TRPO] iter = 130000 dist_mean = 0.2087 dist_std = 0.3850 vf_loss = 0.0824 grad_norm = 0.9923 nat_grad_norm = 0.2399 cg_residual = 0.1738 step_size = 0.4826 reward = 0.0000 fps = 4 mse_loss = 3.1457 
2022-07-08 10:22:20.178467 - gail/main.py:201 - [Discriminator] iter = 130000 loss = -1.2916 grad_norm = 2.5748 grad_penalty = 0.1603 regularization = 0.0000 true_logits = 0.0490 fake_logits = -1.4029 true_prob = 0.5141 fake_prob = 0.2304 
2022-07-08 10:23:48.005828 - gail/main.py:142 - [Evaluate] iter = 130000 episode={ returns = 854.6264 lengths = 257 } discounted_episode={ returns = 736.5277 lengths = 258 } 
2022-07-08 10:24:15.204583 - gail/main.py:174 - [TRPO] iter = 131000 dist_mean = 0.1936 dist_std = 0.3850 vf_loss = 0.1252 grad_norm = 1.4671 nat_grad_norm = 0.2565 cg_residual = 0.2149 step_size = 0.4039 reward = -0.0000 fps = 8 mse_loss = 3.2059 
2022-07-08 10:24:42.879126 - gail/main.py:174 - [TRPO] iter = 132000 dist_mean = 0.2105 dist_std = 0.3863 vf_loss = 0.1953 grad_norm = 1.2491 nat_grad_norm = 0.2094 cg_residual = 0.1240 step_size = 0.4791 reward = -0.0000 fps = 7 mse_loss = 3.0665 
2022-07-08 10:25:09.992210 - gail/main.py:174 - [TRPO] iter = 133000 dist_mean = 0.2471 dist_std = 0.3813 vf_loss = 0.0590 grad_norm = 1.2716 nat_grad_norm = 0.2724 cg_residual = 0.1466 step_size = 0.4051 reward = -0.0000 fps = 5 mse_loss = 2.9845 
2022-07-08 10:25:36.301667 - gail/main.py:174 - [TRPO] iter = 134000 dist_mean = 0.2125 dist_std = 0.3767 vf_loss = 0.0865 grad_norm = 1.0278 nat_grad_norm = 0.2067 cg_residual = 0.1319 step_size = 0.4473 reward = -0.0000 fps = 5 mse_loss = 2.9073 
2022-07-08 10:26:02.353065 - gail/main.py:174 - [TRPO] iter = 135000 dist_mean = 0.2199 dist_std = 0.3771 vf_loss = 0.1137 grad_norm = 1.0668 nat_grad_norm = 0.2538 cg_residual = 0.0942 step_size = 0.4772 reward = 0.0000 fps = 4 mse_loss = 3.0757 
2022-07-08 10:26:03.174352 - gail/main.py:201 - [Discriminator] iter = 135000 loss = -1.3327 grad_norm = 2.7502 grad_penalty = 0.1555 regularization = 0.0000 true_logits = 0.0749 fake_logits = -1.4132 true_prob = 0.5201 fake_prob = 0.2295 
2022-07-08 10:27:43.481304 - gail/main.py:142 - [Evaluate] iter = 135000 episode={ returns = 931.6046 lengths = 277 } discounted_episode={ returns = 792.0731 lengths = 277 } 
2022-07-08 10:28:11.708375 - gail/main.py:174 - [TRPO] iter = 136000 dist_mean = 0.2628 dist_std = 0.3783 vf_loss = 0.2250 grad_norm = 0.6556 nat_grad_norm = 0.1890 cg_residual = 0.0748 step_size = 0.6141 reward = 0.0000 fps = 7 mse_loss = 3.0308 
2022-07-08 10:28:39.051245 - gail/main.py:174 - [TRPO] iter = 137000 dist_mean = 0.2432 dist_std = 0.3810 vf_loss = 0.1191 grad_norm = 1.3099 nat_grad_norm = 0.2838 cg_residual = 0.2046 step_size = 0.4261 reward = -0.0000 fps = 6 mse_loss = 3.1111 
2022-07-08 10:29:07.811726 - gail/main.py:174 - [TRPO] iter = 138000 dist_mean = 0.2360 dist_std = 0.3830 vf_loss = 0.2628 grad_norm = 1.1254 nat_grad_norm = 0.2970 cg_residual = 0.1988 step_size = 0.4199 reward = -0.0000 fps = 5 mse_loss = 3.2604 
2022-07-08 10:29:36.251285 - gail/main.py:174 - [TRPO] iter = 139000 dist_mean = 0.2561 dist_std = 0.3833 vf_loss = 0.0988 grad_norm = 1.2697 nat_grad_norm = 0.2393 cg_residual = 0.0947 step_size = 0.4696 reward = 0.0000 fps = 4 mse_loss = 3.1882 
2022-07-08 10:30:04.273114 - gail/main.py:174 - [TRPO] iter = 140000 dist_mean = 0.2652 dist_std = 0.3854 vf_loss = 0.0788 grad_norm = 1.1660 nat_grad_norm = 0.1973 cg_residual = 0.1012 step_size = 0.5181 reward = 0.0000 fps = 4 mse_loss = 2.8779 
2022-07-08 10:30:05.060135 - gail/main.py:201 - [Discriminator] iter = 140000 loss = -1.2072 grad_norm = 2.4555 grad_penalty = 0.1221 regularization = 0.0000 true_logits = 0.0491 fake_logits = -1.2803 true_prob = 0.5139 fake_prob = 0.2526 
2022-07-08 10:31:33.097572 - gail/main.py:142 - [Evaluate] iter = 140000 episode={ returns = 669.5957 lengths = 225 } discounted_episode={ returns = 588.6885 lengths = 225 } 
