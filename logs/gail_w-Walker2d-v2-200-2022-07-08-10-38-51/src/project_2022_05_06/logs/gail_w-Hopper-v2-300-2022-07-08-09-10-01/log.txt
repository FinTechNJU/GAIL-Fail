2022-07-08 09:10:02.250146 - utils/flags.py:257 - log_dir = logs/gail_w-Hopper-v2-300-2022-07-08-09-10-01
2022-07-08 09:10:40.066966 - gail/utils/replay_buffer.py:86 - Load dataset from /content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/Hopper-v2
2022-07-08 09:11:12.648054 - gail/main.py:80 - Expert Reward 3582.436530
2022-07-08 09:11:14.600373 - gail/main.py:84 - Original dataset size 3000
2022-07-08 09:11:14.841185 - gail/main.py:86 - Subsampled dataset size 3000
2022-07-08 09:11:14.851835 - gail/main.py:87 - np random: 864 random : 786
2022-07-08 09:11:14.940882 - gail/main.py:91 - Sampled obs: 0.4652, acs: 0.0749
2022-07-08 09:11:19.437881 - lunzi/nn/flat_param.py:17 - Enabling flattening... ['GaussianMLPPolicy_1/log_std:0', 'GaussianMLPPolicy_1/Variable:0', 'GaussianMLPPolicy_1/Variable_1:0', 'GaussianMLPPolicy_1/Variable_2:0', 'GaussianMLPPolicy_1/Variable_3:0', 'GaussianMLPPolicy_1/Variable_4:0', 'GaussianMLPPolicy_1/Variable_5:0']
2022-07-08 09:11:56.416059 - gail/discriminator/discriminator.py:30 - Use predefined normalization.
2022-07-08 09:11:56.446643 - gail/discriminator/discriminator.py:33 - Normalizer loc:[[ 1.3966653  -0.06165453 -0.19472611 -0.4584401   0.18350822  2.5732448
   0.00400542 -0.00549955 -0.04755233 -0.02386179  0.00759995]] 
 scale:[[0.16755195 0.05883223 0.15990146 0.34682125 0.5992658  0.6461788
  1.5187451  0.8811966  2.0685835  3.6282625  5.862049  ]]
2022-07-08 09:12:14.806197 - gail/discriminator/discriminator.py:67 - Discriminator uses Wasserstein distance.
2022-07-08 09:12:14.819479 - gail/discriminator/discriminator.py:68 - [<tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable:0' shape=(14, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_1:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_2:0' shape=(100, 100) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_3:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_4:0' shape=(100, 1) dtype=float32_ref>, <tf.Variable 'Discriminator_1/BinaryClassifier_1/Variable_5:0' shape=(1,) dtype=float32_ref>]
2022-07-08 09:12:14.847182 - gail/discriminator/discriminator.py:69 - Use gradient penalty regularization (coef = 10.000000)
2022-07-08 09:12:19.093037 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions_mean]
2022-07-08 09:13:36.963265 - gail/main.py:142 - [Evaluate] iter = 0 episode={ returns = 176.4389 lengths = 193 } discounted_episode={ returns = 145.2316 lengths = 186 } 
2022-07-08 09:13:36.997483 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states] => [actions]
2022-07-08 09:14:14.052091 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [fake_states fake_actions] => [scaled_neural_reward]
2022-07-08 09:14:15.222095 - lunzi/nn/module.py:71 - [MLPVFunction] is making TensorFlow callables, key = [states] => [values]
2022-07-08 09:14:17.251213 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [] => [sync_old]
2022-07-08 09:14:18.281175 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss flat_grad dist_std mean_kl dist_mean]
2022-07-08 09:14:24.364059 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states tangents actions] => [hessian_vec_prod]
2022-07-08 09:14:35.900388 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [] => [get_flat]
2022-07-08 09:14:36.943273 - lunzi/nn/module.py:71 - [FlatParam] is making TensorFlow callables, key = [feed_flat] => [set_flat]
2022-07-08 09:14:38.058047 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states actions advantages ent_coef] => [loss mean_kl]
2022-07-08 09:14:40.063754 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [train_vf vf_loss]
2022-07-08 09:14:43.556815 - lunzi/nn/module.py:71 - [TRPO] is making TensorFlow callables, key = [states returns] => [vf_loss]
2022-07-08 09:14:44.770395 - lunzi/nn/module.py:71 - [GaussianMLPPolicy] is making TensorFlow callables, key = [states actions_] => [mse_loss]
2022-07-08 09:14:45.967574 - gail/main.py:174 - [TRPO] iter = 1000 dist_mean = 0.0000 dist_std = 1.0000 vf_loss = 0.0893 grad_norm = 0.2735 nat_grad_norm = 0.3080 cg_residual = 0.0000 step_size = 0.5282 reward = -0.0000 fps = 6 mse_loss = 0.3020 
2022-07-08 09:15:15.856116 - gail/main.py:174 - [TRPO] iter = 2000 dist_mean = 0.0518 dist_std = 0.9998 vf_loss = 0.1021 grad_norm = 0.1874 nat_grad_norm = 0.2988 cg_residual = 0.0000 step_size = 0.7212 reward = -0.0000 fps = 5 mse_loss = 0.2801 
2022-07-08 09:15:45.253063 - gail/main.py:174 - [TRPO] iter = 3000 dist_mean = 0.0605 dist_std = 1.0094 vf_loss = 0.0958 grad_norm = 0.2970 nat_grad_norm = 0.3500 cg_residual = 0.0000 step_size = 0.4870 reward = 0.0000 fps = 4 mse_loss = 0.3062 
2022-07-08 09:16:14.093334 - gail/main.py:174 - [TRPO] iter = 4000 dist_mean = 0.0843 dist_std = 1.0021 vf_loss = 0.0983 grad_norm = 0.2958 nat_grad_norm = 0.4074 cg_residual = 0.0000 step_size = 0.4906 reward = 0.0000 fps = 4 mse_loss = 0.3226 
2022-07-08 09:16:42.143022 - gail/main.py:174 - [TRPO] iter = 5000 dist_mean = 0.1118 dist_std = 0.9981 vf_loss = 0.1753 grad_norm = 0.3245 nat_grad_norm = 0.3193 cg_residual = 0.0000 step_size = 0.5108 reward = -0.0000 fps = 3 mse_loss = 0.3361 
2022-07-08 09:16:42.144404 - lunzi/nn/module.py:71 - [Discriminator] is making TensorFlow callables, key = [true_states true_actions fake_states fake_actions true_masks] => [train loss true_logits fake_logits true_prob fake_prob grad_norm grad_penalty regularization]
2022-07-08 09:16:51.778604 - gail/main.py:201 - [Discriminator] iter = 5000 loss = 1.6857 grad_norm = 13.9240 grad_penalty = 1.7181 regularization = 0.0000 true_logits = -0.0537 fake_logits = -0.0862 true_prob = 0.4866 fake_prob = 0.4785 
2022-07-08 09:17:00.622055 - gail/main.py:142 - [Evaluate] iter = 5000 episode={ returns = 24.3797 lengths = 23 } discounted_episode={ returns = 24.1963 lengths = 23 } 
2022-07-08 09:17:28.710522 - gail/main.py:174 - [TRPO] iter = 6000 dist_mean = 0.1356 dist_std = 0.9838 vf_loss = 0.1885 grad_norm = 0.3739 nat_grad_norm = 0.3805 cg_residual = 0.0000 step_size = 0.4288 reward = -0.0000 fps = 27 mse_loss = 0.3522 
2022-07-08 09:17:57.002331 - gail/main.py:174 - [TRPO] iter = 7000 dist_mean = 0.1719 dist_std = 0.9871 vf_loss = 0.3947 grad_norm = 0.4376 nat_grad_norm = 0.3273 cg_residual = 0.0000 step_size = 0.4055 reward = -0.0000 fps = 15 mse_loss = 0.4048 
2022-07-08 09:18:23.678469 - gail/main.py:174 - [TRPO] iter = 8000 dist_mean = 0.2410 dist_std = 0.9820 vf_loss = 0.5669 grad_norm = 0.4189 nat_grad_norm = 0.4182 cg_residual = 0.0000 step_size = 0.4188 reward = 0.0000 fps = 10 mse_loss = 0.3722 
2022-07-08 09:18:51.804343 - gail/main.py:174 - [TRPO] iter = 9000 dist_mean = 0.2971 dist_std = 0.9815 vf_loss = 0.4890 grad_norm = 0.5694 nat_grad_norm = 0.3796 cg_residual = 0.0000 step_size = 0.3700 reward = 0.0000 fps = 8 mse_loss = 0.3895 
2022-07-08 09:19:19.804967 - gail/main.py:174 - [TRPO] iter = 10000 dist_mean = 0.3015 dist_std = 0.9738 vf_loss = 0.5037 grad_norm = 0.4065 nat_grad_norm = 0.4599 cg_residual = 0.0001 step_size = 0.4226 reward = -0.0000 fps = 6 mse_loss = 0.4640 
2022-07-08 09:19:20.586199 - gail/main.py:201 - [Discriminator] iter = 10000 loss = 0.6828 grad_norm = 9.0137 grad_penalty = 0.8898 regularization = 0.0000 true_logits = -0.0209 fake_logits = -0.2278 true_prob = 0.4948 fake_prob = 0.4435 
2022-07-08 09:19:43.315681 - gail/main.py:142 - [Evaluate] iter = 10000 episode={ returns = 88.7811 lengths = 60 } discounted_episode={ returns = 85.9995 lengths = 60 } 
2022-07-08 09:20:11.795413 - gail/main.py:174 - [TRPO] iter = 11000 dist_mean = 0.3543 dist_std = 0.9616 vf_loss = 0.5588 grad_norm = 0.4426 nat_grad_norm = 0.3434 cg_residual = 0.0000 step_size = 0.4756 reward = -0.0000 fps = 19 mse_loss = 0.4854 
2022-07-08 09:20:39.480228 - gail/main.py:174 - [TRPO] iter = 12000 dist_mean = 0.3449 dist_std = 0.9428 vf_loss = 0.6071 grad_norm = 0.3303 nat_grad_norm = 0.3769 cg_residual = 0.0002 step_size = 0.4952 reward = 0.0000 fps = 12 mse_loss = 0.4574 
2022-07-08 09:21:06.885443 - gail/main.py:174 - [TRPO] iter = 13000 dist_mean = 0.3460 dist_std = 0.9248 vf_loss = 0.4828 grad_norm = 0.4647 nat_grad_norm = 0.3377 cg_residual = 0.0003 step_size = 0.4662 reward = 0.0000 fps = 9 mse_loss = 0.5401 
2022-07-08 09:21:33.472981 - gail/main.py:174 - [TRPO] iter = 14000 dist_mean = 0.3678 dist_std = 0.9177 vf_loss = 0.3280 grad_norm = 0.4834 nat_grad_norm = 0.3665 cg_residual = 0.0005 step_size = 0.4692 reward = -0.0000 fps = 7 mse_loss = 0.6540 
2022-07-08 09:22:06.653776 - gail/main.py:174 - [TRPO] iter = 15000 dist_mean = 0.4183 dist_std = 0.9068 vf_loss = 0.2204 grad_norm = 0.5655 nat_grad_norm = 0.3795 cg_residual = 0.0003 step_size = 0.4271 reward = 0.0000 fps = 6 mse_loss = 0.7990 
2022-07-08 09:22:07.423610 - gail/main.py:201 - [Discriminator] iter = 15000 loss = 0.3881 grad_norm = 8.2585 grad_penalty = 0.7118 regularization = 0.0000 true_logits = -0.0021 fake_logits = -0.3258 true_prob = 0.4996 fake_prob = 0.4197 
2022-07-08 09:22:29.778707 - gail/main.py:142 - [Evaluate] iter = 15000 episode={ returns = 93.8004 lengths = 61 } discounted_episode={ returns = 91.3223 lengths = 61 } 
2022-07-08 09:22:57.369507 - gail/main.py:174 - [TRPO] iter = 16000 dist_mean = 0.4660 dist_std = 0.8948 vf_loss = 0.1118 grad_norm = 0.3669 nat_grad_norm = 0.3298 cg_residual = 0.0003 step_size = 0.5510 reward = -0.0000 fps = 20 mse_loss = 0.8771 
2022-07-08 09:23:25.204943 - gail/main.py:174 - [TRPO] iter = 17000 dist_mean = 0.4318 dist_std = 0.8815 vf_loss = 0.0665 grad_norm = 0.4872 nat_grad_norm = 0.3167 cg_residual = 0.0002 step_size = 0.5254 reward = -0.0000 fps = 12 mse_loss = 0.8386 
2022-07-08 09:23:52.030199 - gail/main.py:174 - [TRPO] iter = 18000 dist_mean = 0.4945 dist_std = 0.8704 vf_loss = 0.0552 grad_norm = 0.4926 nat_grad_norm = 0.3294 cg_residual = 0.0003 step_size = 0.4539 reward = -0.0000 fps = 9 mse_loss = 0.9216 
2022-07-08 09:24:19.040270 - gail/main.py:174 - [TRPO] iter = 19000 dist_mean = 0.4956 dist_std = 0.8568 vf_loss = 0.0353 grad_norm = 0.5008 nat_grad_norm = 0.2871 cg_residual = 0.0003 step_size = 0.5071 reward = -0.0000 fps = 7 mse_loss = 0.9007 
2022-07-08 09:24:44.369424 - gail/main.py:174 - [TRPO] iter = 20000 dist_mean = 0.5401 dist_std = 0.8394 vf_loss = 0.0499 grad_norm = 0.5223 nat_grad_norm = 0.3039 cg_residual = 0.0005 step_size = 0.4590 reward = 0.0000 fps = 6 mse_loss = 0.9233 
2022-07-08 09:24:45.092059 - gail/main.py:201 - [Discriminator] iter = 20000 loss = 0.0875 grad_norm = 6.1094 grad_penalty = 0.5830 regularization = 0.0000 true_logits = 0.0155 fake_logits = -0.4799 true_prob = 0.5041 fake_prob = 0.3830 
2022-07-08 09:25:02.982107 - gail/main.py:142 - [Evaluate] iter = 20000 episode={ returns = 87.1648 lengths = 55 } discounted_episode={ returns = 85.2060 lengths = 55 } 
2022-07-08 09:25:28.997024 - gail/main.py:174 - [TRPO] iter = 21000 dist_mean = 0.5526 dist_std = 0.8208 vf_loss = 0.0402 grad_norm = 0.3878 nat_grad_norm = 0.3228 cg_residual = 0.0005 step_size = 0.5359 reward = 0.0000 fps = 22 mse_loss = 0.9190 
2022-07-08 09:25:56.369824 - gail/main.py:174 - [TRPO] iter = 22000 dist_mean = 0.5684 dist_std = 0.8097 vf_loss = 0.0241 grad_norm = 0.5189 nat_grad_norm = 0.2876 cg_residual = 0.0007 step_size = 0.5116 reward = -0.0000 fps = 14 mse_loss = 1.0207 
2022-07-08 09:26:24.360517 - gail/main.py:174 - [TRPO] iter = 23000 dist_mean = 0.5918 dist_std = 0.8014 vf_loss = 0.0287 grad_norm = 0.4687 nat_grad_norm = 0.2930 cg_residual = 0.0006 step_size = 0.5499 reward = -0.0000 fps = 10 mse_loss = 1.1172 
2022-07-08 09:26:50.600506 - gail/main.py:174 - [TRPO] iter = 24000 dist_mean = 0.6220 dist_std = 0.7893 vf_loss = 0.0269 grad_norm = 0.7337 nat_grad_norm = 0.3734 cg_residual = 0.0019 step_size = 0.4126 reward = 0.0000 fps = 7 mse_loss = 1.0835 
2022-07-08 09:27:16.376811 - gail/main.py:174 - [TRPO] iter = 25000 dist_mean = 0.5717 dist_std = 0.7863 vf_loss = 0.0373 grad_norm = 0.6201 nat_grad_norm = 0.2633 cg_residual = 0.0004 step_size = 0.5332 reward = 0.0000 fps = 6 mse_loss = 1.0986 
2022-07-08 09:27:17.056333 - gail/main.py:201 - [Discriminator] iter = 25000 loss = -0.1417 grad_norm = 6.0387 grad_penalty = 0.5568 regularization = 0.0000 true_logits = 0.0139 fake_logits = -0.6845 true_prob = 0.5040 fake_prob = 0.3369 
2022-07-08 09:27:36.520474 - gail/main.py:142 - [Evaluate] iter = 25000 episode={ returns = 95.2370 lengths = 58 } discounted_episode={ returns = 92.0204 lengths = 58 } 
2022-07-08 09:28:01.506957 - gail/main.py:174 - [TRPO] iter = 26000 dist_mean = 0.6131 dist_std = 0.7816 vf_loss = 0.0365 grad_norm = 0.6994 nat_grad_norm = 0.2951 cg_residual = 0.0005 step_size = 0.5080 reward = 0.0000 fps = 22 mse_loss = 1.1452 
2022-07-08 09:28:27.478127 - gail/main.py:174 - [TRPO] iter = 27000 dist_mean = 0.6012 dist_std = 0.7709 vf_loss = 0.0465 grad_norm = 0.8729 nat_grad_norm = 0.3613 cg_residual = 0.0012 step_size = 0.3990 reward = 0.0000 fps = 14 mse_loss = 1.1476 
2022-07-08 09:28:54.325226 - gail/main.py:174 - [TRPO] iter = 28000 dist_mean = 0.5869 dist_std = 0.7633 vf_loss = 0.0283 grad_norm = 0.7586 nat_grad_norm = 0.2943 cg_residual = 0.0012 step_size = 0.5294 reward = 0.0000 fps = 10 mse_loss = 1.0216 
2022-07-08 09:29:20.812035 - gail/main.py:174 - [TRPO] iter = 29000 dist_mean = 0.5655 dist_std = 0.7598 vf_loss = 0.0498 grad_norm = 0.7813 nat_grad_norm = 0.3359 cg_residual = 0.0013 step_size = 0.5000 reward = 0.0000 fps = 8 mse_loss = 1.0321 
2022-07-08 09:29:46.729689 - gail/main.py:174 - [TRPO] iter = 30000 dist_mean = 0.5502 dist_std = 0.7489 vf_loss = 0.0319 grad_norm = 0.4918 nat_grad_norm = 0.3317 cg_residual = 0.0021 step_size = 0.5777 reward = 0.0000 fps = 6 mse_loss = 1.1751 
2022-07-08 09:29:47.571502 - gail/main.py:201 - [Discriminator] iter = 30000 loss = -0.5012 grad_norm = 5.6909 grad_penalty = 0.4362 regularization = 0.0000 true_logits = 0.0422 fake_logits = -0.8952 true_prob = 0.5113 fake_prob = 0.2933 
2022-07-08 09:30:08.516753 - gail/main.py:142 - [Evaluate] iter = 30000 episode={ returns = 107.6240 lengths = 62 } discounted_episode={ returns = 104.2008 lengths = 62 } 
2022-07-08 09:30:34.101825 - gail/main.py:174 - [TRPO] iter = 31000 dist_mean = 0.5943 dist_std = 0.7443 vf_loss = 0.0365 grad_norm = 0.8799 nat_grad_norm = 0.3025 cg_residual = 0.0021 step_size = 0.5014 reward = -0.0000 fps = 21 mse_loss = 1.1433 
2022-07-08 09:30:59.788988 - gail/main.py:174 - [TRPO] iter = 32000 dist_mean = 0.5600 dist_std = 0.7389 vf_loss = 0.0435 grad_norm = 0.7430 nat_grad_norm = 0.2922 cg_residual = 0.0016 step_size = 0.5696 reward = 0.0000 fps = 13 mse_loss = 1.1096 
2022-07-08 09:31:25.863370 - gail/main.py:174 - [TRPO] iter = 33000 dist_mean = 0.5521 dist_std = 0.7374 vf_loss = 0.0165 grad_norm = 0.5302 nat_grad_norm = 0.3699 cg_residual = 0.0039 step_size = 0.5218 reward = 0.0000 fps = 10 mse_loss = 1.1412 
2022-07-08 09:31:51.589808 - gail/main.py:174 - [TRPO] iter = 34000 dist_mean = 0.5530 dist_std = 0.7338 vf_loss = 0.1000 grad_norm = 0.6982 nat_grad_norm = 0.6139 cg_residual = 0.0052 step_size = 0.3122 reward = -0.0000 fps = 8 mse_loss = 1.2430 
2022-07-08 09:32:24.351071 - gail/main.py:174 - [TRPO] iter = 35000 dist_mean = 0.5610 dist_std = 0.7302 vf_loss = 0.3655 grad_norm = 0.4167 nat_grad_norm = 0.4167 cg_residual = 0.0068 step_size = 0.5692 reward = -0.0000 fps = 6 mse_loss = 1.2588 
2022-07-08 09:32:25.100039 - gail/main.py:201 - [Discriminator] iter = 35000 loss = -0.7409 grad_norm = 5.1965 grad_penalty = 0.4136 regularization = 0.0000 true_logits = 0.0581 fake_logits = -1.0964 true_prob = 0.5161 fake_prob = 0.2584 
2022-07-08 09:32:47.074517 - gail/main.py:142 - [Evaluate] iter = 35000 episode={ returns = 116.9039 lengths = 66 } discounted_episode={ returns = 112.8270 lengths = 66 } 
2022-07-08 09:33:12.078233 - gail/main.py:174 - [TRPO] iter = 36000 dist_mean = 0.5538 dist_std = 0.7278 vf_loss = 0.2866 grad_norm = 0.8970 nat_grad_norm = 0.4513 cg_residual = 0.0095 step_size = 0.4177 reward = 0.0000 fps = 21 mse_loss = 1.2210 
2022-07-08 09:33:37.600136 - gail/main.py:174 - [TRPO] iter = 37000 dist_mean = 0.4665 dist_std = 0.7255 vf_loss = 0.6317 grad_norm = 0.6327 nat_grad_norm = 0.4792 cg_residual = 0.0077 step_size = 0.4790 reward = -0.0000 fps = 13 mse_loss = 1.2945 
2022-07-08 09:34:02.675505 - gail/main.py:174 - [TRPO] iter = 38000 dist_mean = 0.4092 dist_std = 0.7269 vf_loss = 0.8376 grad_norm = 0.8387 nat_grad_norm = 0.3284 cg_residual = 0.0093 step_size = 0.4964 reward = -0.0000 fps = 10 mse_loss = 1.2596 
2022-07-08 09:34:27.209868 - gail/main.py:174 - [TRPO] iter = 39000 dist_mean = 0.3617 dist_std = 0.7214 vf_loss = 0.3769 grad_norm = 0.5800 nat_grad_norm = 0.3820 cg_residual = 0.0201 step_size = 0.5211 reward = 0.0000 fps = 8 mse_loss = 1.2779 
2022-07-08 09:34:52.192673 - gail/main.py:174 - [TRPO] iter = 40000 dist_mean = 0.3399 dist_std = 0.7221 vf_loss = 0.1649 grad_norm = 0.8089 nat_grad_norm = 0.3147 cg_residual = 0.0079 step_size = 0.4996 reward = 0.0000 fps = 6 mse_loss = 1.3948 
2022-07-08 09:34:52.873924 - gail/main.py:201 - [Discriminator] iter = 40000 loss = -0.4835 grad_norm = 5.8566 grad_penalty = 0.5082 regularization = 0.0000 true_logits = 0.0599 fake_logits = -0.9317 true_prob = 0.5170 fake_prob = 0.3001 
2022-07-08 09:35:26.904344 - gail/main.py:142 - [Evaluate] iter = 40000 episode={ returns = 249.5141 lengths = 101 } discounted_episode={ returns = 234.0070 lengths = 101 } 
2022-07-08 09:35:52.402738 - gail/main.py:174 - [TRPO] iter = 41000 dist_mean = 0.2923 dist_std = 0.7262 vf_loss = 0.4023 grad_norm = 1.4171 nat_grad_norm = 0.8402 cg_residual = 0.1683 step_size = 0.2417 reward = 0.0000 fps = 16 mse_loss = 1.4647 
2022-07-08 09:36:17.109694 - gail/main.py:174 - [TRPO] iter = 42000 dist_mean = 0.3061 dist_std = 0.7236 vf_loss = 0.1458 grad_norm = 0.6272 nat_grad_norm = 0.3380 cg_residual = 0.0050 step_size = 0.5074 reward = -0.0000 fps = 11 mse_loss = 1.4303 
2022-07-08 09:36:42.112845 - gail/main.py:174 - [TRPO] iter = 43000 dist_mean = 0.3266 dist_std = 0.7178 vf_loss = 0.0887 grad_norm = 0.9288 nat_grad_norm = 0.4896 cg_residual = 0.0109 step_size = 0.3895 reward = 0.0000 fps = 9 mse_loss = 1.3716 
2022-07-08 09:37:07.042511 - gail/main.py:174 - [TRPO] iter = 44000 dist_mean = 0.3295 dist_std = 0.7144 vf_loss = 0.3454 grad_norm = 1.0177 nat_grad_norm = 0.3538 cg_residual = 0.0333 step_size = 0.4502 reward = 0.0000 fps = 7 mse_loss = 1.2712 
2022-07-08 09:37:31.288654 - gail/main.py:174 - [TRPO] iter = 45000 dist_mean = 0.2710 dist_std = 0.7122 vf_loss = 0.1543 grad_norm = 0.5951 nat_grad_norm = 0.4132 cg_residual = 0.0664 step_size = 0.4624 reward = -0.0000 fps = 6 mse_loss = 1.1029 
2022-07-08 09:37:32.042584 - gail/main.py:201 - [Discriminator] iter = 45000 loss = -0.8251 grad_norm = 5.5698 grad_penalty = 0.3405 regularization = 0.0000 true_logits = 0.0698 fake_logits = -1.0957 true_prob = 0.5198 fake_prob = 0.2675 
2022-07-08 09:38:05.856204 - gail/main.py:142 - [Evaluate] iter = 45000 episode={ returns = 269.6948 lengths = 104 } discounted_episode={ returns = 253.5484 lengths = 104 } 
2022-07-08 09:38:30.725831 - gail/main.py:174 - [TRPO] iter = 46000 dist_mean = 0.3070 dist_std = 0.7042 vf_loss = 0.1506 grad_norm = 0.3894 nat_grad_norm = 0.3312 cg_residual = 0.0066 step_size = 0.6270 reward = 0.0000 fps = 17 mse_loss = 1.0870 
2022-07-08 09:38:55.478294 - gail/main.py:174 - [TRPO] iter = 47000 dist_mean = 0.2813 dist_std = 0.7014 vf_loss = 0.3584 grad_norm = 0.6880 nat_grad_norm = 0.4733 cg_residual = 0.0272 step_size = 0.4337 reward = 0.0000 fps = 11 mse_loss = 1.0862 
2022-07-08 09:39:19.855403 - gail/main.py:174 - [TRPO] iter = 48000 dist_mean = 0.3196 dist_std = 0.6943 vf_loss = 0.3517 grad_norm = 1.1561 nat_grad_norm = 0.4025 cg_residual = 0.0160 step_size = 0.4302 reward = -0.0000 fps = 9 mse_loss = 1.1177 
2022-07-08 09:39:44.271625 - gail/main.py:174 - [TRPO] iter = 49000 dist_mean = 0.2884 dist_std = 0.6933 vf_loss = 0.3162 grad_norm = 0.7013 nat_grad_norm = 0.4456 cg_residual = 0.0137 step_size = 0.4587 reward = -0.0000 fps = 7 mse_loss = 1.2251 
2022-07-08 09:40:10.548177 - gail/main.py:174 - [TRPO] iter = 50000 dist_mean = 0.2963 dist_std = 0.6892 vf_loss = 0.3213 grad_norm = 1.0030 nat_grad_norm = 0.4625 cg_residual = 0.0397 step_size = 0.3907 reward = 0.0000 fps = 6 mse_loss = 1.1746 
2022-07-08 09:40:11.394096 - gail/main.py:201 - [Discriminator] iter = 50000 loss = -1.0419 grad_norm = 4.5450 grad_penalty = 0.2627 regularization = 0.0000 true_logits = 0.1010 fake_logits = -1.2035 true_prob = 0.5278 fake_prob = 0.2501 
2022-07-08 09:40:54.559299 - gail/main.py:142 - [Evaluate] iter = 50000 episode={ returns = 387.6634 lengths = 132 } discounted_episode={ returns = 357.4410 lengths = 132 } 
2022-07-08 09:41:18.579071 - gail/main.py:174 - [TRPO] iter = 51000 dist_mean = 0.2658 dist_std = 0.6819 vf_loss = 0.2332 grad_norm = 0.7937 nat_grad_norm = 0.3126 cg_residual = 0.0060 step_size = 0.4730 reward = 0.0000 fps = 14 mse_loss = 1.1847 
2022-07-08 09:41:43.969211 - gail/main.py:174 - [TRPO] iter = 52000 dist_mean = 0.3177 dist_std = 0.6791 vf_loss = 0.2309 grad_norm = 0.8187 nat_grad_norm = 0.2923 cg_residual = 0.0094 step_size = 0.5217 reward = -0.0000 fps = 10 mse_loss = 1.1384 
2022-07-08 09:42:13.997663 - gail/main.py:174 - [TRPO] iter = 53000 dist_mean = 0.2621 dist_std = 0.6780 vf_loss = 0.2392 grad_norm = 0.4685 nat_grad_norm = 0.3811 cg_residual = 0.0072 step_size = 0.5821 reward = 0.0000 fps = 8 mse_loss = 1.3298 
2022-07-08 09:42:39.006828 - gail/main.py:174 - [TRPO] iter = 54000 dist_mean = 0.2744 dist_std = 0.6775 vf_loss = 0.3044 grad_norm = 0.8753 nat_grad_norm = 0.3416 cg_residual = 0.0341 step_size = 0.4986 reward = 0.0000 fps = 6 mse_loss = 1.1911 
2022-07-08 09:43:02.787517 - gail/main.py:174 - [TRPO] iter = 55000 dist_mean = 0.2847 dist_std = 0.6777 vf_loss = 0.4324 grad_norm = 0.6144 nat_grad_norm = 0.5253 cg_residual = 0.0173 step_size = 0.4705 reward = -0.0000 fps = 5 mse_loss = 1.2620 
2022-07-08 09:43:03.456842 - gail/main.py:201 - [Discriminator] iter = 55000 loss = -1.0384 grad_norm = 3.6043 grad_penalty = 0.2384 regularization = 0.0000 true_logits = 0.1242 fake_logits = -1.1526 true_prob = 0.5349 fake_prob = 0.2675 
2022-07-08 09:43:44.984822 - gail/main.py:142 - [Evaluate] iter = 55000 episode={ returns = 396.4137 lengths = 135 } discounted_episode={ returns = 359.1257 lengths = 134 } 
2022-07-08 09:44:09.711205 - gail/main.py:174 - [TRPO] iter = 56000 dist_mean = 0.2668 dist_std = 0.6764 vf_loss = 0.3809 grad_norm = 0.6164 nat_grad_norm = 0.4197 cg_residual = 0.0164 step_size = 0.5125 reward = -0.0000 fps = 15 mse_loss = 1.2899 
2022-07-08 09:44:33.582931 - gail/main.py:174 - [TRPO] iter = 57000 dist_mean = 0.2946 dist_std = 0.6732 vf_loss = 0.3610 grad_norm = 0.9290 nat_grad_norm = 0.4262 cg_residual = 0.0167 step_size = 0.4593 reward = -0.0000 fps = 11 mse_loss = 1.5308 
2022-07-08 09:44:57.302344 - gail/main.py:174 - [TRPO] iter = 58000 dist_mean = 0.2372 dist_std = 0.6683 vf_loss = 0.2076 grad_norm = 1.0669 nat_grad_norm = 0.2910 cg_residual = 0.0328 step_size = 0.4353 reward = 0.0000 fps = 8 mse_loss = 1.4054 
2022-07-08 09:45:21.228468 - gail/main.py:174 - [TRPO] iter = 59000 dist_mean = 0.2172 dist_std = 0.6674 vf_loss = 0.2869 grad_norm = 0.5749 nat_grad_norm = 0.4563 cg_residual = 0.0187 step_size = 0.5088 reward = -0.0000 fps = 7 mse_loss = 1.5511 
2022-07-08 09:45:44.876430 - gail/main.py:174 - [TRPO] iter = 60000 dist_mean = 0.2388 dist_std = 0.6667 vf_loss = 0.3182 grad_norm = 0.6016 nat_grad_norm = 0.2856 cg_residual = 0.0099 step_size = 0.5681 reward = -0.0000 fps = 6 mse_loss = 1.4936 
2022-07-08 09:45:45.578739 - gail/main.py:201 - [Discriminator] iter = 60000 loss = -0.9528 grad_norm = 3.4734 grad_penalty = 0.2124 regularization = 0.0000 true_logits = 0.1531 fake_logits = -1.0121 true_prob = 0.5410 fake_prob = 0.3001 
2022-07-08 09:46:27.473937 - gail/main.py:142 - [Evaluate] iter = 60000 episode={ returns = 392.2479 lengths = 135 } discounted_episode={ returns = 382.1903 lengths = 140 } 
2022-07-08 09:46:52.334096 - gail/main.py:174 - [TRPO] iter = 61000 dist_mean = 0.2680 dist_std = 0.6571 vf_loss = 0.1754 grad_norm = 0.6547 nat_grad_norm = 0.3907 cg_residual = 0.0125 step_size = 0.5438 reward = -0.0000 fps = 14 mse_loss = 1.6730 
2022-07-08 09:47:16.308773 - gail/main.py:174 - [TRPO] iter = 62000 dist_mean = 0.2536 dist_std = 0.6505 vf_loss = 0.4094 grad_norm = 0.7286 nat_grad_norm = 0.3580 cg_residual = 0.0190 step_size = 0.4788 reward = -0.0000 fps = 11 mse_loss = 1.5713 
2022-07-08 09:47:40.158572 - gail/main.py:174 - [TRPO] iter = 63000 dist_mean = 0.2429 dist_std = 0.6531 vf_loss = 0.2746 grad_norm = 0.8377 nat_grad_norm = 0.3759 cg_residual = 0.0109 step_size = 0.5153 reward = -0.0000 fps = 8 mse_loss = 1.6926 
2022-07-08 09:48:04.538518 - gail/main.py:174 - [TRPO] iter = 64000 dist_mean = 0.2243 dist_std = 0.6552 vf_loss = 0.4089 grad_norm = 1.0576 nat_grad_norm = 0.2894 cg_residual = 0.0174 step_size = 0.4766 reward = -0.0000 fps = 7 mse_loss = 1.5435 
2022-07-08 09:48:27.225131 - gail/main.py:174 - [TRPO] iter = 65000 dist_mean = 0.1959 dist_std = 0.6523 vf_loss = 0.4015 grad_norm = 0.7647 nat_grad_norm = 0.3571 cg_residual = 0.0105 step_size = 0.4357 reward = -0.0000 fps = 6 mse_loss = 1.3887 
2022-07-08 09:48:28.005646 - gail/main.py:201 - [Discriminator] iter = 65000 loss = -0.7897 grad_norm = 3.5485 grad_penalty = 0.2271 regularization = 0.0000 true_logits = 0.1642 fake_logits = -0.8526 true_prob = 0.5439 fake_prob = 0.3357 
2022-07-08 09:49:16.369366 - gail/main.py:142 - [Evaluate] iter = 65000 episode={ returns = 515.5308 lengths = 158 } discounted_episode={ returns = 457.1497 lengths = 155 } 
2022-07-08 09:49:41.654097 - gail/main.py:174 - [TRPO] iter = 66000 dist_mean = 0.2158 dist_std = 0.6405 vf_loss = 0.9562 grad_norm = 0.6836 nat_grad_norm = 0.2952 cg_residual = 0.0076 step_size = 0.5658 reward = -0.0000 fps = 13 mse_loss = 1.2911 
2022-07-08 09:50:09.633066 - gail/main.py:174 - [TRPO] iter = 67000 dist_mean = 0.2273 dist_std = 0.6362 vf_loss = 0.4548 grad_norm = 0.4732 nat_grad_norm = 0.3273 cg_residual = 0.0079 step_size = 0.5918 reward = -0.0000 fps = 9 mse_loss = 1.5626 
2022-07-08 09:50:34.783009 - gail/main.py:174 - [TRPO] iter = 68000 dist_mean = 0.1866 dist_std = 0.6344 vf_loss = 1.2105 grad_norm = 1.2432 nat_grad_norm = 0.3999 cg_residual = 0.0289 step_size = 0.3746 reward = -0.0000 fps = 7 mse_loss = 1.4545 
2022-07-08 09:50:57.647598 - gail/main.py:174 - [TRPO] iter = 69000 dist_mean = 0.2020 dist_std = 0.6337 vf_loss = 0.8162 grad_norm = 0.3731 nat_grad_norm = 0.3020 cg_residual = 0.0088 step_size = 0.6585 reward = 0.0000 fps = 6 mse_loss = 1.4920 
2022-07-08 09:51:20.518380 - gail/main.py:174 - [TRPO] iter = 70000 dist_mean = 0.1698 dist_std = 0.6235 vf_loss = 0.5951 grad_norm = 0.6481 nat_grad_norm = 0.2858 cg_residual = 0.0118 step_size = 0.6211 reward = -0.0000 fps = 5 mse_loss = 1.4871 
2022-07-08 09:51:21.249698 - gail/main.py:201 - [Discriminator] iter = 70000 loss = -0.8747 grad_norm = 3.7693 grad_penalty = 0.2161 regularization = 0.0000 true_logits = 0.2275 fake_logits = -0.8633 true_prob = 0.5583 fake_prob = 0.3361 
2022-07-08 09:52:18.676623 - gail/main.py:142 - [Evaluate] iter = 70000 episode={ returns = 530.3091 lengths = 161 } discounted_episode={ returns = 480.1704 lengths = 161 } 
2022-07-08 09:52:42.641540 - gail/main.py:174 - [TRPO] iter = 71000 dist_mean = 0.2518 dist_std = 0.6233 vf_loss = 1.7859 grad_norm = 0.7846 nat_grad_norm = 0.3659 cg_residual = 0.0153 step_size = 0.4971 reward = -0.0000 fps = 12 mse_loss = 1.5907 
2022-07-08 09:53:07.490688 - gail/main.py:174 - [TRPO] iter = 72000 dist_mean = 0.2284 dist_std = 0.6180 vf_loss = 1.0741 grad_norm = 0.8811 nat_grad_norm = 0.3857 cg_residual = 0.0165 step_size = 0.4237 reward = -0.0000 fps = 9 mse_loss = 1.6074 
2022-07-08 09:53:32.423009 - gail/main.py:174 - [TRPO] iter = 73000 dist_mean = 0.1904 dist_std = 0.6184 vf_loss = 1.0293 grad_norm = 0.7808 nat_grad_norm = 0.3698 cg_residual = 0.0205 step_size = 0.5196 reward = 0.0000 fps = 7 mse_loss = 1.7902 
2022-07-08 09:53:57.626905 - gail/main.py:174 - [TRPO] iter = 74000 dist_mean = 0.1302 dist_std = 0.6190 vf_loss = 0.6780 grad_norm = 0.5940 nat_grad_norm = 0.3355 cg_residual = 0.0122 step_size = 0.5519 reward = -0.0000 fps = 6 mse_loss = 1.6435 
2022-07-08 09:54:22.672078 - gail/main.py:174 - [TRPO] iter = 75000 dist_mean = 0.1957 dist_std = 0.6159 vf_loss = 0.4423 grad_norm = 0.5200 nat_grad_norm = 0.3614 cg_residual = 0.0141 step_size = 0.5680 reward = -0.0000 fps = 5 mse_loss = 1.8368 
2022-07-08 09:54:23.357738 - gail/main.py:201 - [Discriminator] iter = 75000 loss = -0.9750 grad_norm = 3.4175 grad_penalty = 0.2223 regularization = 0.0000 true_logits = 0.2446 fake_logits = -0.9527 true_prob = 0.5614 fake_prob = 0.3210 
2022-07-08 09:55:18.269577 - gail/main.py:142 - [Evaluate] iter = 75000 episode={ returns = 592.7360 lengths = 176 } discounted_episode={ returns = 526.1679 lengths = 175 } 
2022-07-08 09:55:40.790027 - gail/main.py:174 - [TRPO] iter = 76000 dist_mean = 0.1706 dist_std = 0.6200 vf_loss = 0.7921 grad_norm = 0.6606 nat_grad_norm = 0.4371 cg_residual = 0.0533 step_size = 0.4576 reward = 0.0000 fps = 12 mse_loss = 2.0233 
2022-07-08 09:56:03.469652 - gail/main.py:174 - [TRPO] iter = 77000 dist_mean = 0.1607 dist_std = 0.6184 vf_loss = 0.3531 grad_norm = 0.6384 nat_grad_norm = 0.3114 cg_residual = 0.0096 step_size = 0.5815 reward = 0.0000 fps = 9 mse_loss = 1.8667 
2022-07-08 09:56:26.352477 - gail/main.py:174 - [TRPO] iter = 78000 dist_mean = 0.1755 dist_std = 0.6198 vf_loss = 1.1082 grad_norm = 0.8225 nat_grad_norm = 0.3934 cg_residual = 0.0137 step_size = 0.4686 reward = 0.0000 fps = 8 mse_loss = 2.0921 
2022-07-08 09:56:48.745499 - gail/main.py:174 - [TRPO] iter = 79000 dist_mean = 0.1128 dist_std = 0.6205 vf_loss = 0.3544 grad_norm = 0.5292 nat_grad_norm = 0.2727 cg_residual = 0.0143 step_size = 0.5932 reward = 0.0000 fps = 6 mse_loss = 2.2972 
2022-07-08 09:57:11.440081 - gail/main.py:174 - [TRPO] iter = 80000 dist_mean = 0.2042 dist_std = 0.6195 vf_loss = 0.7380 grad_norm = 0.4512 nat_grad_norm = 0.2323 cg_residual = 0.0106 step_size = 0.7216 reward = 0.0000 fps = 5 mse_loss = 1.7376 
2022-07-08 09:57:12.161292 - gail/main.py:201 - [Discriminator] iter = 80000 loss = -0.9242 grad_norm = 3.5417 grad_penalty = 0.2173 regularization = 0.0000 true_logits = 0.2386 fake_logits = -0.9029 true_prob = 0.5595 fake_prob = 0.3294 
2022-07-08 09:58:06.042803 - gail/main.py:142 - [Evaluate] iter = 80000 episode={ returns = 606.3843 lengths = 180 } discounted_episode={ returns = 545.7226 lengths = 181 } 
2022-07-08 09:58:30.770797 - gail/main.py:174 - [TRPO] iter = 81000 dist_mean = 0.1670 dist_std = 0.6217 vf_loss = 0.5135 grad_norm = 0.4847 nat_grad_norm = 0.2759 cg_residual = 0.0083 step_size = 0.6225 reward = -0.0000 fps = 12 mse_loss = 1.9001 
2022-07-08 09:58:54.979157 - gail/main.py:174 - [TRPO] iter = 82000 dist_mean = 0.1598 dist_std = 0.6160 vf_loss = 0.4844 grad_norm = 0.6480 nat_grad_norm = 0.3220 cg_residual = 0.0101 step_size = 0.6054 reward = 0.0000 fps = 9 mse_loss = 1.7989 
2022-07-08 09:59:18.343145 - gail/main.py:174 - [TRPO] iter = 83000 dist_mean = 0.1368 dist_std = 0.6140 vf_loss = 0.2602 grad_norm = 0.4419 nat_grad_norm = 0.2878 cg_residual = 0.0156 step_size = 0.6553 reward = 0.0000 fps = 7 mse_loss = 2.0075 
2022-07-08 09:59:43.394224 - gail/main.py:174 - [TRPO] iter = 84000 dist_mean = 0.1682 dist_std = 0.6144 vf_loss = 0.2409 grad_norm = 0.5275 nat_grad_norm = 0.2615 cg_residual = 0.0153 step_size = 0.6342 reward = 0.0000 fps = 6 mse_loss = 2.0500 
2022-07-08 10:00:08.154937 - gail/main.py:174 - [TRPO] iter = 85000 dist_mean = 0.3498 dist_std = 0.6138 vf_loss = 0.4670 grad_norm = 0.7836 nat_grad_norm = 0.2718 cg_residual = 0.0116 step_size = 0.5364 reward = 0.0000 fps = 5 mse_loss = 2.1565 
2022-07-08 10:00:08.926019 - gail/main.py:201 - [Discriminator] iter = 85000 loss = -1.0146 grad_norm = 3.3861 grad_penalty = 0.1987 regularization = 0.0000 true_logits = 0.1692 fake_logits = -1.0441 true_prob = 0.5458 fake_prob = 0.3108 
2022-07-08 10:01:11.012209 - gail/main.py:142 - [Evaluate] iter = 85000 episode={ returns = 642.9132 lengths = 193 } discounted_episode={ returns = 571.8689 lengths = 192 } 
2022-07-08 10:01:37.012019 - gail/main.py:174 - [TRPO] iter = 86000 dist_mean = 0.1211 dist_std = 0.6152 vf_loss = 0.4634 grad_norm = 0.8053 nat_grad_norm = 0.2973 cg_residual = 0.0162 step_size = 0.4836 reward = 0.0000 fps = 11 mse_loss = 2.3049 
2022-07-08 10:02:10.171617 - gail/main.py:174 - [TRPO] iter = 87000 dist_mean = 0.1341 dist_std = 0.6144 vf_loss = 0.2202 grad_norm = 0.4913 nat_grad_norm = 0.3395 cg_residual = 0.0196 step_size = 0.6013 reward = 0.0000 fps = 8 mse_loss = 2.3806 
2022-07-08 10:02:37.211698 - gail/main.py:174 - [TRPO] iter = 88000 dist_mean = 0.1771 dist_std = 0.6133 vf_loss = 1.0854 grad_norm = 0.7807 nat_grad_norm = 0.3839 cg_residual = 0.0324 step_size = 0.4378 reward = -0.0000 fps = 6 mse_loss = 2.6745 
2022-07-08 10:03:04.595232 - gail/main.py:174 - [TRPO] iter = 89000 dist_mean = 0.2055 dist_std = 0.6087 vf_loss = 0.1331 grad_norm = 0.6574 nat_grad_norm = 0.3458 cg_residual = 0.0183 step_size = 0.4961 reward = -0.0000 fps = 5 mse_loss = 2.7527 
2022-07-08 10:03:31.230768 - gail/main.py:174 - [TRPO] iter = 90000 dist_mean = 0.2252 dist_std = 0.6057 vf_loss = 0.9175 grad_norm = 1.0470 nat_grad_norm = 0.3823 cg_residual = 0.0242 step_size = 0.3914 reward = -0.0000 fps = 4 mse_loss = 2.8989 
2022-07-08 10:03:32.069542 - gail/main.py:201 - [Discriminator] iter = 90000 loss = -0.9164 grad_norm = 2.7552 grad_penalty = 0.1841 regularization = 0.0000 true_logits = 0.1874 fake_logits = -0.9131 true_prob = 0.5496 fake_prob = 0.3356 
2022-07-08 10:04:47.450564 - gail/main.py:142 - [Evaluate] iter = 90000 episode={ returns = 685.4149 lengths = 211 } discounted_episode={ returns = 605.6049 lengths = 210 } 
2022-07-08 10:05:15.989425 - gail/main.py:174 - [TRPO] iter = 91000 dist_mean = 0.1368 dist_std = 0.5998 vf_loss = 0.2232 grad_norm = 0.5688 nat_grad_norm = 0.3029 cg_residual = 0.0159 step_size = 0.5654 reward = 0.0000 fps = 9 mse_loss = 3.1119 
2022-07-08 10:05:44.441423 - gail/main.py:174 - [TRPO] iter = 92000 dist_mean = 0.1744 dist_std = 0.5969 vf_loss = 0.2805 grad_norm = 0.8325 nat_grad_norm = 0.2923 cg_residual = 0.0226 step_size = 0.4851 reward = 0.0000 fps = 7 mse_loss = 3.3532 
2022-07-08 10:06:11.546793 - gail/main.py:174 - [TRPO] iter = 93000 dist_mean = 0.1825 dist_std = 0.5895 vf_loss = 0.1488 grad_norm = 0.6732 nat_grad_norm = 0.3030 cg_residual = 0.0274 step_size = 0.5031 reward = 0.0000 fps = 6 mse_loss = 3.5367 
2022-07-08 10:06:38.719266 - gail/main.py:174 - [TRPO] iter = 94000 dist_mean = 0.2193 dist_std = 0.5856 vf_loss = 0.1219 grad_norm = 0.4764 nat_grad_norm = 0.3381 cg_residual = 0.0275 step_size = 0.5627 reward = 0.0000 fps = 5 mse_loss = 3.3459 
2022-07-08 10:07:05.989996 - gail/main.py:174 - [TRPO] iter = 95000 dist_mean = 0.1586 dist_std = 0.5800 vf_loss = 0.1246 grad_norm = 0.3911 nat_grad_norm = 0.2864 cg_residual = 0.0162 step_size = 0.6915 reward = 0.0000 fps = 4 mse_loss = 3.3344 
2022-07-08 10:07:06.832741 - gail/main.py:201 - [Discriminator] iter = 95000 loss = -0.7343 grad_norm = 2.6365 grad_penalty = 0.1834 regularization = 0.0000 true_logits = 0.1685 fake_logits = -0.7492 true_prob = 0.5453 fake_prob = 0.3645 
2022-07-08 10:08:24.875293 - gail/main.py:142 - [Evaluate] iter = 95000 episode={ returns = 793.1862 lengths = 236 } discounted_episode={ returns = 608.5683 lengths = 213 } 
2022-07-08 10:08:51.716045 - gail/main.py:174 - [TRPO] iter = 96000 dist_mean = 0.1359 dist_std = 0.5804 vf_loss = 0.1258 grad_norm = 0.7071 nat_grad_norm = 0.2521 cg_residual = 0.0215 step_size = 0.6326 reward = -0.0000 fps = 9 mse_loss = 3.7240 
2022-07-08 10:09:19.096911 - gail/main.py:174 - [TRPO] iter = 97000 dist_mean = 0.1989 dist_std = 0.5727 vf_loss = 0.2408 grad_norm = 0.4831 nat_grad_norm = 0.2755 cg_residual = 0.0169 step_size = 0.6054 reward = 0.0000 fps = 7 mse_loss = 3.8273 
2022-07-08 10:09:47.920449 - gail/main.py:174 - [TRPO] iter = 98000 dist_mean = 0.1872 dist_std = 0.5656 vf_loss = 0.1367 grad_norm = 0.6682 nat_grad_norm = 0.2786 cg_residual = 0.0412 step_size = 0.5231 reward = 0.0000 fps = 6 mse_loss = 3.9773 
2022-07-08 10:10:14.526735 - gail/main.py:174 - [TRPO] iter = 99000 dist_mean = 0.3145 dist_std = 0.5627 vf_loss = 0.3169 grad_norm = 0.3953 nat_grad_norm = 0.3098 cg_residual = 0.0228 step_size = 0.6429 reward = 0.0000 fps = 5 mse_loss = 3.9222 
2022-07-08 10:10:41.180610 - gail/main.py:174 - [TRPO] iter = 100000 dist_mean = 0.3123 dist_std = 0.5605 vf_loss = 0.2987 grad_norm = 0.4590 nat_grad_norm = 0.3709 cg_residual = 0.0450 step_size = 0.5054 reward = 0.0000 fps = 4 mse_loss = 4.0464 
2022-07-08 10:10:42.030564 - gail/main.py:201 - [Discriminator] iter = 100000 loss = -0.7392 grad_norm = 2.8810 grad_penalty = 0.1822 regularization = 0.0000 true_logits = 0.1342 fake_logits = -0.7871 true_prob = 0.5366 fake_prob = 0.3523 
2022-07-08 10:12:08.542360 - gail/main.py:142 - [Evaluate] iter = 100000 episode={ returns = 739.7403 lengths = 226 } discounted_episode={ returns = 615.7264 lengths = 215 } 
2022-07-08 10:12:36.836384 - gail/main.py:174 - [TRPO] iter = 101000 dist_mean = 0.2683 dist_std = 0.5617 vf_loss = 0.0824 grad_norm = 0.6892 nat_grad_norm = 0.2827 cg_residual = 0.0270 step_size = 0.5411 reward = 0.0000 fps = 8 mse_loss = 4.4756 
2022-07-08 10:13:04.341770 - gail/main.py:174 - [TRPO] iter = 102000 dist_mean = 0.2834 dist_std = 0.5568 vf_loss = 0.1451 grad_norm = 0.4862 nat_grad_norm = 0.2146 cg_residual = 0.0244 step_size = 0.6971 reward = -0.0000 fps = 7 mse_loss = 4.2480 
2022-07-08 10:13:30.893740 - gail/main.py:174 - [TRPO] iter = 103000 dist_mean = 0.1903 dist_std = 0.5587 vf_loss = 0.1718 grad_norm = 0.6120 nat_grad_norm = 0.2349 cg_residual = 0.0264 step_size = 0.6310 reward = -0.0000 fps = 5 mse_loss = 4.7658 
2022-07-08 10:13:58.056279 - gail/main.py:174 - [TRPO] iter = 104000 dist_mean = 0.2327 dist_std = 0.5496 vf_loss = 0.3675 grad_norm = 0.8474 nat_grad_norm = 0.3637 cg_residual = 0.0903 step_size = 0.4013 reward = 0.0000 fps = 5 mse_loss = 4.7110 
2022-07-08 10:14:25.657217 - gail/main.py:174 - [TRPO] iter = 105000 dist_mean = 0.2350 dist_std = 0.5498 vf_loss = 0.1053 grad_norm = 0.7795 nat_grad_norm = 0.2988 cg_residual = 0.0184 step_size = 0.4631 reward = 0.0000 fps = 4 mse_loss = 4.7865 
2022-07-08 10:14:26.478272 - gail/main.py:201 - [Discriminator] iter = 105000 loss = -0.7130 grad_norm = 3.0826 grad_penalty = 0.1673 regularization = 0.0000 true_logits = 0.1020 fake_logits = -0.7782 true_prob = 0.5284 fake_prob = 0.3517 
2022-07-08 10:16:00.659948 - gail/main.py:142 - [Evaluate] iter = 105000 episode={ returns = 924.2007 lengths = 268 } discounted_episode={ returns = 795.7664 lengths = 268 } 
2022-07-08 10:16:27.328426 - gail/main.py:174 - [TRPO] iter = 106000 dist_mean = 0.1486 dist_std = 0.5488 vf_loss = 0.1147 grad_norm = 0.6449 nat_grad_norm = 0.2300 cg_residual = 0.0320 step_size = 0.5532 reward = 0.0000 fps = 8 mse_loss = 4.6065 
2022-07-08 10:16:53.338194 - gail/main.py:174 - [TRPO] iter = 107000 dist_mean = 0.2748 dist_std = 0.5500 vf_loss = 0.2161 grad_norm = 0.4615 nat_grad_norm = 0.3021 cg_residual = 0.0430 step_size = 0.5695 reward = -0.0000 fps = 6 mse_loss = 4.5854 
2022-07-08 10:17:19.754796 - gail/main.py:174 - [TRPO] iter = 108000 dist_mean = 0.1962 dist_std = 0.5459 vf_loss = 0.1095 grad_norm = 0.7259 nat_grad_norm = 0.2677 cg_residual = 0.0329 step_size = 0.5702 reward = -0.0000 fps = 5 mse_loss = 4.9248 
2022-07-08 10:17:47.030411 - gail/main.py:174 - [TRPO] iter = 109000 dist_mean = 0.2645 dist_std = 0.5486 vf_loss = 0.1235 grad_norm = 0.8599 nat_grad_norm = 0.2649 cg_residual = 0.0306 step_size = 0.5709 reward = -0.0000 fps = 4 mse_loss = 4.3821 
2022-07-08 10:18:13.351124 - gail/main.py:174 - [TRPO] iter = 110000 dist_mean = 0.1471 dist_std = 0.5515 vf_loss = 0.0860 grad_norm = 0.7058 nat_grad_norm = 0.2244 cg_residual = 0.0246 step_size = 0.6361 reward = -0.0000 fps = 4 mse_loss = 4.8661 
2022-07-08 10:18:14.097510 - gail/main.py:201 - [Discriminator] iter = 110000 loss = -0.6923 grad_norm = 3.2211 grad_penalty = 0.1792 regularization = 0.0000 true_logits = 0.0717 fake_logits = -0.7998 true_prob = 0.5214 fake_prob = 0.3494 
2022-07-08 10:19:54.441841 - gail/main.py:142 - [Evaluate] iter = 110000 episode={ returns = 986.8518 lengths = 290 } discounted_episode={ returns = 840.4546 lengths = 293 } 
2022-07-08 10:20:21.428028 - gail/main.py:174 - [TRPO] iter = 111000 dist_mean = 0.2460 dist_std = 0.5465 vf_loss = 0.0797 grad_norm = 0.8799 nat_grad_norm = 0.2798 cg_residual = 0.0272 step_size = 0.5083 reward = 0.0000 fps = 7 mse_loss = 4.5868 
2022-07-08 10:20:48.909255 - gail/main.py:174 - [TRPO] iter = 112000 dist_mean = 0.2531 dist_std = 0.5455 vf_loss = 0.0723 grad_norm = 0.6710 nat_grad_norm = 0.2658 cg_residual = 0.0403 step_size = 0.5241 reward = -0.0000 fps = 6 mse_loss = 4.5019 
2022-07-08 10:21:15.917365 - gail/main.py:174 - [TRPO] iter = 113000 dist_mean = 0.2000 dist_std = 0.5467 vf_loss = 0.1147 grad_norm = 0.4209 nat_grad_norm = 0.3510 cg_residual = 0.0317 step_size = 0.5504 reward = -0.0000 fps = 5 mse_loss = 4.7589 
2022-07-08 10:21:47.734365 - gail/main.py:174 - [TRPO] iter = 114000 dist_mean = 0.2133 dist_std = 0.5408 vf_loss = 0.1570 grad_norm = 1.0021 nat_grad_norm = 0.5262 cg_residual = 0.1000 step_size = 0.3528 reward = -0.0000 fps = 4 mse_loss = 4.5801 
2022-07-08 10:22:13.922837 - gail/main.py:174 - [TRPO] iter = 115000 dist_mean = 0.1200 dist_std = 0.5406 vf_loss = 0.0476 grad_norm = 1.0090 nat_grad_norm = 0.2276 cg_residual = 0.0277 step_size = 0.4941 reward = -0.0000 fps = 4 mse_loss = 3.9366 
2022-07-08 10:22:14.650133 - gail/main.py:201 - [Discriminator] iter = 115000 loss = -0.6472 grad_norm = 2.5374 grad_penalty = 0.1668 regularization = 0.0000 true_logits = 0.0565 fake_logits = -0.7575 true_prob = 0.5188 fake_prob = 0.3571 
2022-07-08 10:23:48.639213 - gail/main.py:142 - [Evaluate] iter = 115000 episode={ returns = 927.5832 lengths = 268 } discounted_episode={ returns = 778.8448 lengths = 268 } 
2022-07-08 10:24:15.782572 - gail/main.py:174 - [TRPO] iter = 116000 dist_mean = 0.1884 dist_std = 0.5442 vf_loss = 0.1745 grad_norm = 0.4416 nat_grad_norm = 0.2697 cg_residual = 0.0378 step_size = 0.6178 reward = -0.0000 fps = 8 mse_loss = 4.3127 
2022-07-08 10:24:43.386860 - gail/main.py:174 - [TRPO] iter = 117000 dist_mean = 0.2013 dist_std = 0.5455 vf_loss = 0.0559 grad_norm = 0.7094 nat_grad_norm = 0.3757 cg_residual = 0.0487 step_size = 0.4543 reward = 0.0000 fps = 6 mse_loss = 3.8578 
2022-07-08 10:25:09.917253 - gail/main.py:174 - [TRPO] iter = 118000 dist_mean = 0.1908 dist_std = 0.5389 vf_loss = 0.1492 grad_norm = 0.6625 nat_grad_norm = 0.2943 cg_residual = 0.0319 step_size = 0.5276 reward = 0.0000 fps = 5 mse_loss = 4.3739 
2022-07-08 10:25:36.697528 - gail/main.py:174 - [TRPO] iter = 119000 dist_mean = 0.2076 dist_std = 0.5355 vf_loss = 0.1946 grad_norm = 0.5159 nat_grad_norm = 0.3312 cg_residual = 0.0364 step_size = 0.5220 reward = 0.0000 fps = 4 mse_loss = 4.2085 
2022-07-08 10:26:02.353739 - gail/main.py:174 - [TRPO] iter = 120000 dist_mean = 0.1496 dist_std = 0.5412 vf_loss = 0.2213 grad_norm = 0.5531 nat_grad_norm = 0.2256 cg_residual = 0.0428 step_size = 0.5980 reward = -0.0000 fps = 4 mse_loss = 4.3653 
2022-07-08 10:26:03.126331 - gail/main.py:201 - [Discriminator] iter = 120000 loss = -0.7529 grad_norm = 2.9447 grad_penalty = 0.1545 regularization = 0.0000 true_logits = 0.0149 fake_logits = -0.8925 true_prob = 0.5079 fake_prob = 0.3324 
2022-07-08 10:27:45.134064 - gail/main.py:142 - [Evaluate] iter = 120000 episode={ returns = 903.5499 lengths = 272 } discounted_episode={ returns = 773.5100 lengths = 274 } 
2022-07-08 10:28:13.266491 - gail/main.py:174 - [TRPO] iter = 121000 dist_mean = 0.2288 dist_std = 0.5365 vf_loss = 0.1517 grad_norm = 0.5913 nat_grad_norm = 0.2466 cg_residual = 0.0189 step_size = 0.6043 reward = 0.0000 fps = 7 mse_loss = 4.6887 
2022-07-08 10:28:40.738731 - gail/main.py:174 - [TRPO] iter = 122000 dist_mean = 0.2411 dist_std = 0.5284 vf_loss = 0.0926 grad_norm = 0.7287 nat_grad_norm = 0.2420 cg_residual = 0.0114 step_size = 0.6251 reward = -0.0000 fps = 6 mse_loss = 4.6802 
2022-07-08 10:29:08.770947 - gail/main.py:174 - [TRPO] iter = 123000 dist_mean = 0.2211 dist_std = 0.5317 vf_loss = 0.4605 grad_norm = 0.7379 nat_grad_norm = 0.2237 cg_residual = 0.0134 step_size = 0.6518 reward = 0.0000 fps = 5 mse_loss = 4.6504 
2022-07-08 10:29:37.426174 - gail/main.py:174 - [TRPO] iter = 124000 dist_mean = 0.1642 dist_std = 0.5343 vf_loss = 0.1218 grad_norm = 0.6115 nat_grad_norm = 0.2697 cg_residual = 0.0312 step_size = 0.5118 reward = 0.0000 fps = 4 mse_loss = 5.0297 
2022-07-08 10:30:05.838327 - gail/main.py:174 - [TRPO] iter = 125000 dist_mean = 0.1096 dist_std = 0.5221 vf_loss = 0.0879 grad_norm = 0.9248 nat_grad_norm = 0.2457 cg_residual = 0.0354 step_size = 0.5609 reward = 0.0000 fps = 4 mse_loss = 5.1224 
2022-07-08 10:30:06.685071 - gail/main.py:201 - [Discriminator] iter = 125000 loss = -0.7229 grad_norm = 2.4714 grad_penalty = 0.1364 regularization = 0.0000 true_logits = -0.0034 fake_logits = -0.8628 true_prob = 0.5036 fake_prob = 0.3366 
2022-07-08 10:31:49.940101 - gail/main.py:142 - [Evaluate] iter = 125000 episode={ returns = 897.8472 lengths = 258 } discounted_episode={ returns = 728.6023 lengths = 247 } 
